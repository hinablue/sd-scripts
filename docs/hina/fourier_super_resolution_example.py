#!/usr/bin/env python3
"""
HinaAdaptive å„ªåŒ–å™¨æ­£å‰‡åŒ–æŠ€è¡“ç¤ºä¾‹è…³æœ¬

æœ¬è…³æœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨ HinaAdaptive å„ªåŒ–å™¨çš„å„ç¨®æ­£å‰‡åŒ–æŠ€è¡“ï¼Œ
åŒ…æ‹¬é‚Šç·£æ„ŸçŸ¥ã€ç©ºé–“æ„ŸçŸ¥ã€èƒŒæ™¯æ­£å‰‡åŒ–å’Œ LoRA ä½ç§©æ­£å‰‡åŒ–ã€‚

æ³¨æ„ï¼šå‚…ç«‹è‘‰ç‰¹å¾µæå¤±åŠŸèƒ½å·²è¢«ç§»é™¤ï¼Œå› ç‚ºå®ƒä¸é©ç”¨æ–¼ SD-Scripts
çš„ latent space è¨“ç·´ç’°å¢ƒã€‚

ä½œè€…: Hina
æ—¥æœŸ: 2024
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, Tuple, Optional
import sys
import os

# æ·»åŠ åº«è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from library.hina_adaptive import HinaAdaptive

class SuperResolutionModel(nn.Module):
    """
    ç°¡å–®çš„è¶…è§£æåº¦æ¨¡å‹ç¤ºä¾‹
    """
    def __init__(self, scale_factor: int = 4, num_channels: int = 3):
        super().__init__()
        self.scale_factor = scale_factor

        # ç‰¹å¾µæå–å±¤
        self.feature_extraction = nn.Sequential(
            nn.Conv2d(num_channels, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(inplace=True),
        )

        # æ®˜å·®å¡Š
        self.residual_blocks = nn.Sequential(*[
            ResidualBlock(64) for _ in range(6)
        ])

        # ä¸Šæ¡æ¨£å±¤
        self.upsampling = nn.Sequential(
            nn.Conv2d(64, 64 * (scale_factor ** 2), 3, padding=1),
            nn.PixelShuffle(scale_factor),
            nn.Conv2d(64, num_channels, 3, padding=1),
        )

    def forward(self, x):
        # ç‰¹å¾µæå–
        features = self.feature_extraction(x)

        # æ®˜å·®è™•ç†
        residual = self.residual_blocks(features)

        # ä¸Šæ¡æ¨£
        sr_image = self.upsampling(residual)

        return sr_image

class ResidualBlock(nn.Module):
    """æ®˜å·®å¡Š"""
    def __init__(self, channels: int):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.relu(out)
        out = self.conv2(out)
        return out + residual

def generate_synthetic_data(batch_size: int = 8, image_size: int = 64,
                          scale_factor: int = 4, device: str = 'cpu') -> Tuple[torch.Tensor, torch.Tensor]:
    """
    ç”Ÿæˆç”¨æ–¼æ¸¬è©¦çš„åˆæˆæ•¸æ“š
    """
    # ç”Ÿæˆé«˜è§£æåº¦åœ–åƒ
    hr_size = image_size * scale_factor
    hr_images = torch.randn(batch_size, 3, hr_size, hr_size, device=device)

    # é€šéä¸‹æ¡æ¨£å‰µå»ºä½è§£æåº¦åœ–åƒ
    lr_images = F.interpolate(hr_images, size=(image_size, image_size), mode='bilinear', align_corners=False)

    # æ·»åŠ ä¸€äº›ç´°ç¯€çµæ§‹
    for i in range(batch_size):
        # æ·»åŠ é‚Šç·£çµæ§‹
        hr_images[i, 0, 50:150, 50:150] = 1.0
        hr_images[i, 1, 100:200, 100:200] = 1.0
        hr_images[i, 2, 150:250, 150:250] = 1.0

        # é‡æ–°ç”Ÿæˆå°æ‡‰çš„ä½è§£æåº¦ç‰ˆæœ¬
        lr_images[i] = F.interpolate(hr_images[i:i+1], size=(image_size, image_size), mode='bilinear', align_corners=False)[0]

    return lr_images, hr_images

def train_with_edge_suppression(scale_factor: int = 4):
    """
    ä½¿ç”¨é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–è¨“ç·´ç¤ºä¾‹
    """
    print("=" * 60)
    print(f"ğŸ” é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–è¨“ç·´ç¤ºä¾‹ (Scale: {scale_factor}x)")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")

    # å‰µå»ºæ¨¡å‹
    model = SuperResolutionModel(scale_factor=scale_factor).to(device)
    print(f"æ¨¡å‹åƒæ•¸æ•¸é‡: {sum(p.numel() for p in model.parameters())}")

    # å‰µå»ºå„ªåŒ–å™¨ - ä½¿ç”¨é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=1e-4,
        # é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–
        edge_suppression=True,
        edge_penalty=0.1,
        edge_threshold=0.6,
        # çµåˆå…¶ä»–åŠŸèƒ½
        use_tam=True,
        use_cautious=True,
        memory_efficient=True,
        vram_budget_gb=8.0
    )

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info = optimizer.get_optimization_info()
    print(f"å„ªåŒ–å™¨é¡å‹: {info['optimizer_type']}")
    print(f"é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–: {info['features']['edge_suppression']}")
    print(f"TAM é˜»å°¼: {info['features']['tam']}")

    # è¨“ç·´å¾ªç’°
    model.train()
    num_epochs = 5

    for epoch in range(num_epochs):
        print(f"\n--- Epoch {epoch + 1}/{num_epochs} ---")

        # ç”Ÿæˆè¨“ç·´æ•¸æ“š
        lr_images, hr_images = generate_synthetic_data(
            batch_size=4,
            image_size=32,
            scale_factor=scale_factor,
            device=device
        )

        # å‰å‘å‚³æ’­
        optimizer.zero_grad()
        sr_images = model(lr_images)

        # è¨ˆç®—æå¤±
        loss = F.mse_loss(sr_images, hr_images)

        # åå‘å‚³æ’­
        loss.backward()
        optimizer.step()

        # è¼¸å‡ºè¨“ç·´ä¿¡æ¯
        with torch.no_grad():
            psnr = 20 * torch.log10(2.0 / torch.sqrt(F.mse_loss(sr_images, hr_images)))
            print(f"Loss: {loss.item():.4f}, PSNR: {psnr.item():.2f}dB")

    print("âœ… é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–è¨“ç·´å®Œæˆ!")
    return model, optimizer

def train_with_spatial_awareness(scale_factor: int = 4):
    """
    ä½¿ç”¨ç©ºé–“æ„ŸçŸ¥æ­£å‰‡åŒ–è¨“ç·´ç¤ºä¾‹
    """
    print("=" * 60)
    print(f"ğŸ” ç©ºé–“æ„ŸçŸ¥æ­£å‰‡åŒ–è¨“ç·´ç¤ºä¾‹ (Scale: {scale_factor}x)")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")

    # å‰µå»ºæ¨¡å‹
    model = SuperResolutionModel(scale_factor=scale_factor).to(device)

    # å‰µå»ºå„ªåŒ–å™¨ - ä½¿ç”¨ç©ºé–“æ„ŸçŸ¥æ­£å‰‡åŒ–
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=1e-4,
        # ç©ºé–“æ„ŸçŸ¥æ­£å‰‡åŒ–
        spatial_awareness=True,
        frequency_penalty=0.05,
        detail_preservation=0.8,
        # çµåˆå…¶ä»–åŠŸèƒ½
        use_dynamic_adaptation=True,
        memory_efficient=True,
        vram_budget_gb=8.0
    )

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info = optimizer.get_optimization_info()
    print(f"ç©ºé–“æ„ŸçŸ¥æ­£å‰‡åŒ–: {info['features']['spatial_awareness']}")
    print(f"å‹•æ…‹è‡ªé©æ‡‰: {info['features']['dynamic_adaptation']}")

    # è¨“ç·´å¾ªç’°
    model.train()
    num_epochs = 5

    for epoch in range(num_epochs):
        print(f"\n--- Epoch {epoch + 1}/{num_epochs} ---")

        # ç”Ÿæˆè¨“ç·´æ•¸æ“š
        lr_images, hr_images = generate_synthetic_data(
            batch_size=4,
            image_size=32,
            scale_factor=scale_factor,
            device=device
        )

        # å‰å‘å‚³æ’­
        optimizer.zero_grad()
        sr_images = model(lr_images)

        # è¨ˆç®—æå¤±
        loss = F.mse_loss(sr_images, hr_images)

        # åå‘å‚³æ’­
        loss.backward()
        optimizer.step()

        # è¼¸å‡ºè¨“ç·´ä¿¡æ¯
        with torch.no_grad():
            psnr = 20 * torch.log10(2.0 / torch.sqrt(F.mse_loss(sr_images, hr_images)))
            print(f"Loss: {loss.item():.4f}, PSNR: {psnr.item():.2f}dB")

    print("âœ… ç©ºé–“æ„ŸçŸ¥æ­£å‰‡åŒ–è¨“ç·´å®Œæˆ!")
    return model, optimizer

def train_with_background_regularization(scale_factor: int = 4):
    """
    ä½¿ç”¨èƒŒæ™¯æ­£å‰‡åŒ–è¨“ç·´ç¤ºä¾‹
    """
    print("=" * 60)
    print(f"ğŸ” èƒŒæ™¯æ­£å‰‡åŒ–è¨“ç·´ç¤ºä¾‹ (Scale: {scale_factor}x)")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")

    # å‰µå»ºæ¨¡å‹
    model = SuperResolutionModel(scale_factor=scale_factor).to(device)

    # å‰µå»ºå„ªåŒ–å™¨ - ä½¿ç”¨èƒŒæ™¯æ­£å‰‡åŒ–
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=1e-4,
        # èƒŒæ™¯æ­£å‰‡åŒ–
        background_regularization=True,
        # çµåˆå…¶ä»–åŠŸèƒ½
        use_spd=True,
        use_orthogonal_grad=True,
        memory_efficient=True,
        vram_budget_gb=8.0
    )

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info = optimizer.get_optimization_info()
    print(f"èƒŒæ™¯æ­£å‰‡åŒ–: {info['features']['background_regularization']}")
    print(f"SPD æ­£å‰‡åŒ–: {info['features']['spd']}")
    print(f"æ­£äº¤æ¢¯åº¦æŠ•å½±: {info['features']['orthogonal_grad']}")

    # è¨“ç·´å¾ªç’°
    model.train()
    num_epochs = 5

    for epoch in range(num_epochs):
        print(f"\n--- Epoch {epoch + 1}/{num_epochs} ---")

        # ç”Ÿæˆè¨“ç·´æ•¸æ“š
        lr_images, hr_images = generate_synthetic_data(
            batch_size=4,
            image_size=32,
            scale_factor=scale_factor,
            device=device
        )

        # å‰å‘å‚³æ’­
        optimizer.zero_grad()
        sr_images = model(lr_images)

        # è¨ˆç®—æå¤±
        loss = F.mse_loss(sr_images, hr_images)

        # åå‘å‚³æ’­
        loss.backward()
        optimizer.step()

        # è¼¸å‡ºè¨“ç·´ä¿¡æ¯
        with torch.no_grad():
            psnr = 20 * torch.log10(2.0 / torch.sqrt(F.mse_loss(sr_images, hr_images)))
            print(f"Loss: {loss.item():.4f}, PSNR: {psnr.item():.2f}dB")

    print("âœ… èƒŒæ™¯æ­£å‰‡åŒ–è¨“ç·´å®Œæˆ!")
    return model, optimizer

def train_with_lora_regularization():
    """
    ä½¿ç”¨ LoRA ä½ç§©æ­£å‰‡åŒ–è¨“ç·´ç¤ºä¾‹
    """
    print("=" * 60)
    print("ğŸ” LoRA ä½ç§©æ­£å‰‡åŒ–è¨“ç·´ç¤ºä¾‹")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")

    # å‰µå»ºç°¡å–®çš„ç·šæ€§æ¨¡å‹ï¼ˆé©åˆ LoRAï¼‰
    model = nn.Sequential(
        nn.Linear(128, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 64),
    ).to(device)

    # å‰µå»ºå„ªåŒ–å™¨ - ä½¿ç”¨ LoRA ä½ç§©æ­£å‰‡åŒ–
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=1e-4,
        # LoRA ä½ç§©æ­£å‰‡åŒ–
        lora_rank_penalty=True,
        rank_penalty_strength=0.01,
        low_rank_emphasis=1.2,
        # çµåˆå…¶ä»–åŠŸèƒ½
        use_lr_mask=True,
        dynamic_weight_decay=True,
        memory_efficient=True,
        vram_budget_gb=8.0
    )

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info = optimizer.get_optimization_info()
    print(f"LoRA ä½ç§©æ­£å‰‡åŒ–: {info['features']['lora_rank_penalty']}")
    print(f"å­¸ç¿’ç‡é®ç½©: {info['features']['lr_mask']}")
    print(f"å‹•æ…‹æ¬Šé‡è¡°æ¸›: {info['features']['dynamic_weight_decay']}")

    # è¨“ç·´å¾ªç’°
    model.train()
    num_epochs = 5

    for epoch in range(num_epochs):
        print(f"\n--- Epoch {epoch + 1}/{num_epochs} ---")

        # ç”Ÿæˆè¨“ç·´æ•¸æ“š
        batch_size = 32
        x = torch.randn(batch_size, 128, device=device)
        y = torch.randn(batch_size, 64, device=device)

        # å‰å‘å‚³æ’­
        optimizer.zero_grad()
        pred = model(x)

        # è¨ˆç®—æå¤±
        loss = F.mse_loss(pred, y)

        # åå‘å‚³æ’­
        loss.backward()
        optimizer.step()

        # è¼¸å‡ºè¨“ç·´ä¿¡æ¯
        print(f"Loss: {loss.item():.4f}")

    print("âœ… LoRA ä½ç§©æ­£å‰‡åŒ–è¨“ç·´å®Œæˆ!")
    return model, optimizer

def train_with_combined_regularization(scale_factor: int = 4):
    """
    ä½¿ç”¨çµ„åˆæ­£å‰‡åŒ–æŠ€è¡“è¨“ç·´ç¤ºä¾‹
    """
    print("=" * 60)
    print(f"ğŸ” çµ„åˆæ­£å‰‡åŒ–æŠ€è¡“è¨“ç·´ç¤ºä¾‹ (Scale: {scale_factor}x)")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")

    # å‰µå»ºæ¨¡å‹
    model = SuperResolutionModel(scale_factor=scale_factor).to(device)

    # å‰µå»ºå„ªåŒ–å™¨ - ä½¿ç”¨çµ„åˆæ­£å‰‡åŒ–æŠ€è¡“
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=1e-4,
        # çµ„åˆå¤šç¨®æ­£å‰‡åŒ–æŠ€è¡“
        edge_suppression=True,
        edge_penalty=0.1,
        spatial_awareness=True,
        frequency_penalty=0.05,
        background_regularization=True,
        lora_rank_penalty=True,
        rank_penalty_strength=0.01,
        # å…¶ä»–åŠŸèƒ½
        use_dynamic_adaptation=True,
        use_tam=True,
        use_cautious=True,
        use_spd=True,
        memory_efficient=True,
        vram_budget_gb=8.0
    )

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info = optimizer.get_optimization_info()
    print("å•Ÿç”¨çš„æ­£å‰‡åŒ–æŠ€è¡“:")
    print(f"  - é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–: {info['features']['edge_suppression']}")
    print(f"  - ç©ºé–“æ„ŸçŸ¥æ­£å‰‡åŒ–: {info['features']['spatial_awareness']}")
    print(f"  - èƒŒæ™¯æ­£å‰‡åŒ–: {info['features']['background_regularization']}")
    print(f"  - LoRA ä½ç§©æ­£å‰‡åŒ–: {info['features']['lora_rank_penalty']}")
    print(f"  - å‹•æ…‹è‡ªé©æ‡‰: {info['features']['dynamic_adaptation']}")

    # è¨“ç·´å¾ªç’°
    model.train()
    num_epochs = 5

    for epoch in range(num_epochs):
        print(f"\n--- Epoch {epoch + 1}/{num_epochs} ---")

        # ç”Ÿæˆè¨“ç·´æ•¸æ“š
        lr_images, hr_images = generate_synthetic_data(
            batch_size=4,
            image_size=32,
            scale_factor=scale_factor,
            device=device
        )

        # å‰å‘å‚³æ’­
        optimizer.zero_grad()
        sr_images = model(lr_images)

        # è¨ˆç®—æå¤±
        loss = F.mse_loss(sr_images, hr_images)

        # åå‘å‚³æ’­
        loss.backward()
        optimizer.step()

        # è¼¸å‡ºè¨“ç·´ä¿¡æ¯
        with torch.no_grad():
            psnr = 20 * torch.log10(2.0 / torch.sqrt(F.mse_loss(sr_images, hr_images)))
            print(f"Loss: {loss.item():.4f}, PSNR: {psnr.item():.2f}dB")

        # é¡¯ç¤ºè¨˜æ†¶é«”çµ±è¨ˆ
        memory_stats = optimizer.get_memory_stats()
        print(f"è¨˜æ†¶é«”å£“åŠ›: {memory_stats['memory_pressure']:.2%}")

    print("âœ… çµ„åˆæ­£å‰‡åŒ–æŠ€è¡“è¨“ç·´å®Œæˆ!")
    return model, optimizer

def compare_regularization_techniques():
    """
    æ¯”è¼ƒä¸åŒæ­£å‰‡åŒ–æŠ€è¡“çš„æ•ˆæœ
    """
    print("=" * 60)
    print("ğŸ” æ­£å‰‡åŒ–æŠ€è¡“æ¯”è¼ƒ")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")

    # æ¸¬è©¦æ•¸æ“š
    lr_images, hr_images = generate_synthetic_data(
        batch_size=4,
        image_size=32,
        scale_factor=4,
        device=device
    )

    techniques = [
        ("åŸºç¤å„ªåŒ–å™¨", {}),
        ("é‚Šç·£æ„ŸçŸ¥", {"edge_suppression": True, "edge_penalty": 0.1}),
        ("ç©ºé–“æ„ŸçŸ¥", {"spatial_awareness": True, "frequency_penalty": 0.05}),
        ("èƒŒæ™¯æ­£å‰‡åŒ–", {"background_regularization": True}),
        ("çµ„åˆæŠ€è¡“", {
            "edge_suppression": True,
            "edge_penalty": 0.1,
            "spatial_awareness": True,
            "frequency_penalty": 0.05,
            "background_regularization": True,
        }),
    ]

    results = []

    for name, config in techniques:
        print(f"\n--- æ¸¬è©¦ {name} ---")

        # å‰µå»ºæ¨¡å‹
        model = SuperResolutionModel(scale_factor=4).to(device)

        # å‰µå»ºå„ªåŒ–å™¨
        optimizer_config = {
            "lr": 1e-4,
            "memory_efficient": True,
            "vram_budget_gb": 8.0
        }
        optimizer_config.update(config)

        optimizer = HinaAdaptive(model.parameters(), **optimizer_config)

        # å¿«é€Ÿè¨“ç·´
        model.train()
        initial_loss = None
        final_loss = None

        for step in range(10):
            optimizer.zero_grad()
            sr_images = model(lr_images)
            loss = F.mse_loss(sr_images, hr_images)

            if step == 0:
                initial_loss = loss.item()

            loss.backward()
            optimizer.step()

            if step == 9:
                final_loss = loss.item()

        # è¨ˆç®— PSNR
        with torch.no_grad():
            sr_images = model(lr_images)
            psnr = 20 * torch.log10(2.0 / torch.sqrt(F.mse_loss(sr_images, hr_images)))

        improvement = ((initial_loss - final_loss) / initial_loss) * 100
        results.append((name, improvement, psnr.item()))

        print(f"æ”¹å–„ç‡: {improvement:.1f}%, PSNR: {psnr.item():.2f}dB")

    # é¡¯ç¤ºæ¯”è¼ƒçµæœ
    print("\n" + "=" * 60)
    print("ğŸ“Š çµæœæ¯”è¼ƒ")
    print("=" * 60)

    for name, improvement, psnr in results:
        print(f"{name:<12}: æ”¹å–„ç‡ {improvement:>6.1f}%, PSNR {psnr:>5.2f}dB")

    return results

def main():
    """
    ä¸»å‡½æ•¸ - é‹è¡Œæ‰€æœ‰ç¤ºä¾‹
    """
    print("ğŸš€ HinaAdaptive æ­£å‰‡åŒ–æŠ€è¡“ç¤ºä¾‹")
    print("=" * 60)

    try:
        # é‹è¡Œå„ç¨®ç¤ºä¾‹
        print("\n1. é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–ç¤ºä¾‹")
        train_with_edge_suppression(scale_factor=4)

        print("\n2. ç©ºé–“æ„ŸçŸ¥æ­£å‰‡åŒ–ç¤ºä¾‹")
        train_with_spatial_awareness(scale_factor=4)

        print("\n3. èƒŒæ™¯æ­£å‰‡åŒ–ç¤ºä¾‹")
        train_with_background_regularization(scale_factor=4)

        print("\n4. LoRA ä½ç§©æ­£å‰‡åŒ–ç¤ºä¾‹")
        train_with_lora_regularization()

        print("\n5. çµ„åˆæ­£å‰‡åŒ–æŠ€è¡“ç¤ºä¾‹")
        train_with_combined_regularization(scale_factor=4)

        print("\n6. æ­£å‰‡åŒ–æŠ€è¡“æ¯”è¼ƒ")
        compare_regularization_techniques()

    except Exception as e:
        print(f"âŒ åŸ·è¡Œéç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

    print("\nâœ… æ‰€æœ‰ç¤ºä¾‹åŸ·è¡Œå®Œæˆ!")

if __name__ == "__main__":
    main()