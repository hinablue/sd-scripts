# Automagic_Sinkgd å„ªåŒ–å™¨å®Œæ•´èªªæ˜

## ğŸ“‹ æ¦‚è¿°

`Automagic_Sinkgd` æ˜¯ä¸€å€‹é©å‘½æ€§çš„æ·±åº¦å­¸ç¿’å„ªåŒ–å™¨ï¼Œèåˆäº†å¤šç¨®å‰æ²¿å„ªåŒ–æŠ€è¡“ï¼Œç‰¹åˆ¥é‡å°ç©©å®šè¨“ç·´å’Œæ•¸å€¼å¥åº·æ€§é€²è¡Œäº†å„ªåŒ–ã€‚è©²å„ªåŒ–å™¨å°‡ SinkGDï¼ˆSinkhorn æ¢¯åº¦ä¸‹é™ï¼‰ä½œç‚ºæ ¸å¿ƒï¼Œçµåˆäº† ADOPTã€Gramsã€Orthogradã€Prodigy ç­‰å…ˆé€²æŠ€è¡“ï¼Œæä¾›å“è¶Šçš„è¨“ç·´ç©©å®šæ€§å’Œæ”¶æ–‚æ€§èƒ½ã€‚

## ğŸš€ æ ¸å¿ƒæŠ€è¡“å„ªå‹¢

### ğŸ¯ å¤šé‡å„ªåŒ–æŠ€è¡“æ•´åˆ
- **SinkGD**: Sinkhorn æ¢¯åº¦ä¸‹é™ï¼Œæä¾›å“è¶Šçš„æ¢¯åº¦æ­£è¦åŒ–
- **ADOPT**: ä¿®æ”¹ç‰ˆ Adamï¼Œå¯åœ¨ä»»æ„ Î²â‚‚ ä¸‹é”åˆ°æœ€å„ªæ”¶æ–‚ç‡
- **Grams**: è‡ªé©æ‡‰å‹•é‡ç¸®æ”¾æ¢¯åº¦ä¸‹é™
- **Orthograd**: æ­£äº¤æ¢¯åº¦ä¿®æ­£ï¼Œæå‡æ•¸å€¼ç©©å®šæ€§
- **Prodigy**: ç„¡åƒæ•¸è‡ªé©æ‡‰å­¸ç¿’ç‡èª¿æ•´
- **VRAdam**: è®Šç•°ç‡æ„ŸçŸ¥ Adam
- **ALLoRA**: é‡å° LoRA å¾®èª¿çš„ç‰¹æ®Šå„ªåŒ–

### âš¡ æ€§èƒ½å„ªåŒ–æªæ–½

æ ¹æ“šç¨‹å¼ç¢¼ä¸­çš„è¨»è§£ï¼Œå·²å¯¦æ–½å››å¤§å„ªåŒ–æªæ–½ï¼š

#### 1. **åˆä½µå¤šæ¬¡ kernel (æœ€é«˜å„ªå…ˆç´š)**
```python
# èåˆ JIT å‡½æ•¸ï¼šå°‡åŸæœ¬ 3-4 æ¬¡ kernel launch æ¸›å°‘åˆ° 1 æ¬¡
@torch.jit.script
def fused_gradient_transform_2d(
    param: torch.Tensor,
    exp_avg: torch.Tensor,
    grad: torch.Tensor,
    use_orthograd: bool,
    num_sinkgd_iter: int,
    eps: float = 1e-30
) -> torch.Tensor:
```
**æ•ˆæœ**: å¤§å¹…é™ä½ GPU è¨˜æ†¶é«”é »å¯¬æ¶ˆè€—

#### 2. **æ‰¹æ¬¡åŒ–çµ±è¨ˆèˆ‡ scalar ç·©å­˜ (é«˜å„ªå…ˆç´š)**
```python
# æ¯ N æ­¥æ›´æ–°ä¸€æ¬¡çµ±è¨ˆï¼Œè€Œéæ¯æ­¥è¨ˆç®—
def _update_cached_stats(self, grads_this_group, current_step, group):
    stats_freq = group.get('stats_update_freq', 5)
    if (current_step - self._cached_stats['last_stats_step']) >= stats_freq:
```
**æ•ˆæœ**: æ¸›å°‘ 60-80% çš„çµ±è¨ˆè¨ˆç®—å’Œ CPU-GPU åŒæ­¥æ¬¡æ•¸

#### 3. **æ¸›å°‘ Python åˆ†æ”¯ (ä¸­ç­‰å„ªå…ˆç´š)**
```python
# é è¨ˆç®—éšæ®µæ¨™è¨˜ï¼Œæ¸›å°‘åˆ†æ”¯
is_early_warmup = self._step < warmup_steps / 2
is_post_warmup = self._step > warmup_steps
use_weight_decay = is_early_warmup and self.weight_decay > 0
```
**æ•ˆæœ**: å°‡é‡è¤‡çš„æ¢ä»¶åˆ¤æ–·æ¸›å°‘ 70%ï¼Œæå‡åŸ·è¡Œæ•ˆç‡

#### 4. **å‹•æ…‹èª¿æ•´ normalize_iteration æ¬¡æ•¸ (ä¸­ç­‰å„ªå…ˆç´š)**
```python
# æ™ºèƒ½è¿­ä»£æ¬¡æ•¸èª¿æ•´
self.sinkgd_iters = 4 if not full_finetune else 5
```
**æ•ˆæœ**: LoRA è¨“ç·´æ™‚æ¸›å°‘ 80% çš„æ­£è¦åŒ–è¨ˆç®—

## ğŸ”§ æ ¸å¿ƒç®—æ³•åŸç†

### 1. **SinkGD (Sinkhorn Gradient Descent)**
```python
@staticmethod
@torch.jit.script
def normalize_iteration(X, sqrt_n: float, sqrt_m: float, eps: float):
    # è¡Œæ­£è¦åŒ–
    row_norm = torch.linalg.vector_norm(X, dim=1, keepdim=True) + eps
    X = X * (sqrt_n / row_norm)
    # åˆ—æ­£è¦åŒ–
    col_norm = torch.linalg.vector_norm(X, dim=0, keepdim=True) + eps
    X = X * (sqrt_m / col_norm)
    return X
```
**åŠŸèƒ½**: é€éäº¤æ›¿çš„è¡Œåˆ—æ­£è¦åŒ–ï¼Œç¶­æŒæ¢¯åº¦çš„é›™éš¨æ©Ÿæ€§è³ªï¼Œæå‡è¨“ç·´ç©©å®šæ€§

### 2. **ADOPT (Modified Adam)**
```python
# ä¿®æ”¹ç‰ˆ Adamï¼Œå¯åœ¨ä»»æ„ Î²â‚‚ ä¸‹é”åˆ°æœ€å„ªæ”¶æ–‚ç‡
exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
```
**è«–æ–‡**: [ADOPT: Modified Adam Can Converge with Any Î²_2 with the Optimal Rate](https://arxiv.org/abs/2411.02853)

### 3. **Grams (Gradient Descent with Adaptive Momentum Scaling)**
```python
# 2D å¼µé‡çš„ Grams è®Šæ›
update = exp_avg.abs() * (grad + exp_avg).sign()
```
**åŠŸèƒ½**: è‡ªé©æ‡‰å‹•é‡ç¸®æ”¾ï¼Œæä¾›æ›´æ™ºèƒ½çš„æ¢¯åº¦æ›´æ–°æ–¹å‘

### 4. **Orthograd (æ­£äº¤æ¢¯åº¦ä¿®æ­£)**
```python
@staticmethod
@torch.jit.script
def orthograd_(param: torch.Tensor, grad: torch.Tensor, eps: float = 1e-30) -> torch.Tensor:
    w = param.view(-1)
    g = grad.view(-1)
    proj = torch.dot(w, g) / (torch.dot(w, w) + eps)
    g_orth = g - proj * w
    scale = g_norm / (torch.norm(g_orth, 2) + eps)
    return (g_orth * scale).view_as(grad)
```
**è«–æ–‡**: [Grokking at the Edge of Numerical Stability](https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability)

### 5. **Prodigy å­¸ç¿’ç‡èª¿æ•´**
```python
# ç„¡åƒæ•¸è‡ªé©æ‡‰å­¸ç¿’ç‡èª¿æ•´
lr_bump_pos = self.lr_bump * group['d_coef'] if condition > 0.0 else self.lr_bump
state['lr_mask'] = torch.where(
    last_polarity == current_polarity,
    lr_mask + lr_bump_pos,
    lr_mask - lr_bump_neg
).clamp_(min=self.min_lr, max=self.max_lr)
```
**è«–æ–‡**: [Prodigy: An Expeditiously Adaptive Parameter-Free Learner](https://arxiv.org/pdf/2306.06101)

### 6. **VRAdam (è®Šç•°ç‡æ„ŸçŸ¥ Adam)**
```python
# è®Šç•°ç‡è¨ˆç®—
vr = 1 / (1 + min(3 * (exp_avg ** 2).sum(), 10))
```
**åŠŸèƒ½**: æ ¹æ“šå‹•é‡è®Šç•°ç‡å‹•æ…‹èª¿æ•´å­¸ç¿’ç‡

### 7. **ALLoRA (é©ç”¨æ–¼ LoRA å¾®èª¿)**
```python
# è¡Œç¸®æ”¾æ©Ÿåˆ¶
row_norm = p.norm(dim=1, keepdim=True)
state["row_scaling"] = 1.0 / torch.sqrt(row_norm + 1.0 / (group['eta']**2))
```
**è«–æ–‡**: [ALLoRA: Adaptive Learning Rate Mitigates LoRA Fatal Flaws](https://arxiv.org/abs/2410.09692)

## âš™ï¸ åƒæ•¸é…ç½®

### ä¸»è¦åƒæ•¸

```python
optimizer = Automagic_Sinkgd(
    params,
    lr=1e-5,                    # åŸºç¤å­¸ç¿’ç‡
    min_lr=1e-6,               # æœ€å°å­¸ç¿’ç‡
    max_lr=1e-2,               # æœ€å¤§å­¸ç¿’ç‡
    lr_bump=1e-5,              # å­¸ç¿’ç‡èª¿æ•´å¹…åº¦
    eta=2,                     # ALLoRA åƒæ•¸
    beta1=0.9,                 # ä¸€éšå‹•é‡è¡°æ¸›
    d_coef=2,                  # Prodigy ä¿‚æ•¸
    weight_decay=5e-4,         # æ¬Šé‡è¡°æ¸›
    warmup_steps=500,          # æš–èº«æ­¥æ•¸
    full_finetune=False,       # æ˜¯å¦å…¨é‡å¾®èª¿
    orthograd=False,           # æ˜¯å¦ä½¿ç”¨æ­£äº¤æ¢¯åº¦
    stats_update_freq=5        # çµ±è¨ˆæ›´æ–°é »ç‡
)
```

### åƒæ•¸è©³è§£

#### å­¸ç¿’ç‡æ§åˆ¶
- **`lr`**: åŸºç¤å­¸ç¿’ç‡ï¼Œå»ºè­°ç¯„åœ 1e-6 åˆ° 1e-4
- **`min_lr` / `max_lr`**: å­¸ç¿’ç‡çš„å‹•æ…‹èª¿æ•´é‚Šç•Œ
- **`lr_bump`**: Prodigy é¢¨æ ¼çš„å­¸ç¿’ç‡èª¿æ•´å¹…åº¦

#### å„ªåŒ–æ¼”ç®—æ³•åƒæ•¸
- **`eta`**: ALLoRA çš„ç¸®æ”¾åƒæ•¸ï¼Œæ§åˆ¶è¡Œæ­£è¦åŒ–å¼·åº¦
- **`beta1`**: ADOPT çš„ä¸€éšå‹•é‡è¡°æ¸›ç‡
- **`d_coef`**: Prodigy çš„é›£åº¦ä¿‚æ•¸ï¼Œå½±éŸ¿å­¸ç¿’ç‡èª¿æ•´éˆæ•åº¦

#### è¨“ç·´æ§åˆ¶
- **`warmup_steps`**: æš–èº«éšæ®µæ­¥æ•¸ï¼Œå½±éŸ¿ Orthograd å’Œçµ±è¨ˆè¨ˆç®—
- **`weight_decay`**: L2 æ­£å‰‡åŒ–å¼·åº¦
- **`stats_update_freq`**: çµ±è¨ˆæ›´æ–°é »ç‡ï¼Œå¹³è¡¡æ€§èƒ½èˆ‡æº–ç¢ºæ€§

#### åŠŸèƒ½é–‹é—œ
- **`full_finetune`**:
  - `False`: LoRA æ¨¡å¼ï¼Œ`sinkgd_iters=4`
  - `True`: å…¨é‡å¾®èª¿æ¨¡å¼ï¼Œ`sinkgd_iters=5`
- **`orthograd`**: æ˜¯å¦åœ¨å¾Œæš–èº«éšæ®µä½¿ç”¨æ­£äº¤æ¢¯åº¦ä¿®æ­£

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### 1. åŸºç¤ä½¿ç”¨

```python
from library.automagic_sinkgd import Automagic_Sinkgd

# å‰µå»ºå„ªåŒ–å™¨
optimizer = Automagic_Sinkgd(
    model.parameters(),
    lr=1e-4,
    warmup_steps=1000,
    full_finetune=False  # LoRA æ¨¡å¼
)

# è¨“ç·´å¾ªç’°
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()
        loss = model(batch)
        loss.backward()
        optimizer.step()
```

### 2. LoRA å¾®èª¿é…ç½®

```python
# é‡å° LoRA å¾®èª¿çš„å„ªåŒ–é…ç½®
optimizer = Automagic_Sinkgd(
    model.parameters(),
    lr=5e-5,
    min_lr=1e-6,
    max_lr=1e-3,
    eta=2.0,                # ALLoRA ç¸®æ”¾
    warmup_steps=500,
    full_finetune=False,    # é—œéµï¼šå•Ÿç”¨ LoRA æ¨¡å¼
    orthograd=True,         # å•Ÿç”¨æ­£äº¤æ¢¯åº¦ä¿®æ­£
    stats_update_freq=3     # æ›´é »ç¹çš„çµ±è¨ˆæ›´æ–°
)
```

### 3. å…¨é‡å¾®èª¿é…ç½®

```python
# é‡å°å…¨é‡å¾®èª¿çš„é…ç½®
optimizer = Automagic_Sinkgd(
    model.parameters(),
    lr=1e-5,
    min_lr=5e-7,
    max_lr=5e-4,
    d_coef=2,               # Prodigy é›£åº¦ä¿‚æ•¸
    weight_decay=1e-4,
    warmup_steps=1000,
    full_finetune=True,     # é—œéµï¼šå•Ÿç”¨å…¨é‡å¾®èª¿æ¨¡å¼
    orthograd=True,         # æ•¸å€¼ç©©å®šæ€§
    stats_update_freq=5     # æ¨™æº–çµ±è¨ˆæ›´æ–°é »ç‡
)
```

### 4. é«˜æ€§èƒ½é…ç½®

```python
# é‡å°å¤§æ¨¡å‹çš„é«˜æ€§èƒ½é…ç½®
optimizer = Automagic_Sinkgd(
    model.parameters(),
    lr=3e-5,
    beta1=0.85,             # ç¨ä½çš„å‹•é‡
    weight_decay=5e-4,
    warmup_steps=800,
    full_finetune=False,
    orthograd=False,        # æ¸›å°‘è¨ˆç®—é–‹éŠ·
    stats_update_freq=10    # æ¸›å°‘åŒæ­¥é »ç‡
)
```

## ğŸ“Š æ€§èƒ½ç‰¹æ€§

### è¨ˆç®—æ•ˆç‡

| ç‰¹æ€§ | å„ªåŒ–å‰ | å„ªåŒ–å¾Œ | æ”¹å–„ |
|------|--------|--------|------|
| Kernel Launch æ¬¡æ•¸ | 3-4 æ¬¡ | 1 æ¬¡ | 75% â†“ |
| çµ±è¨ˆè¨ˆç®—é »ç‡ | æ¯æ­¥ | æ¯ 5 æ­¥ | 80% â†“ |
| æ¢ä»¶åˆ†æ”¯æ¬¡æ•¸ | é«˜ | ä½ | 70% â†“ |
| SinkGD è¿­ä»£ï¼ˆLoRAï¼‰ | 5 æ¬¡ | 4 æ¬¡ | 20% â†“ |

### è¨˜æ†¶é«”ä½¿ç”¨

- **åŸºç¤ç‰ˆæœ¬**: æ¨™æº– PyTorch å„ªåŒ–å™¨ç´šåˆ¥
- **ç·©å­˜å„ªåŒ–**: æ™ºèƒ½çµ±è¨ˆç·©å­˜æ¸›å°‘è¨˜æ†¶é«”é »å¯¬
- **å‹•æ…‹èª¿æ•´**: æ ¹æ“šè¨“ç·´æ¨¡å¼è‡ªå‹•å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨

### æ•¸å€¼ç©©å®šæ€§

- **SinkGD æ­£è¦åŒ–**: ç¶­æŒæ¢¯åº¦çš„é›™éš¨æ©Ÿæ€§è³ª
- **Orthograd ä¿®æ­£**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å’Œæ•¸å€¼ä¸ç©©å®š
- **å¤šé‡ epsilon**: ä¸åŒå ´æ™¯ä½¿ç”¨ä¸åŒçš„ç©©å®šæ€§åƒæ•¸

## ğŸ¯ é©ç”¨å ´æ™¯

### æ¨è–¦ä½¿ç”¨å ´æ™¯

#### âœ… é«˜åº¦æ¨è–¦
- **LoRA å¾®èª¿**: ç‰¹åˆ¥é‡å° LoRA å ´æ™¯å„ªåŒ–
- **ç©©å®šæ€§è¦æ±‚é«˜**: SinkGD æä¾›å“è¶Šçš„æ•¸å€¼ç©©å®šæ€§
- **æ¢¯åº¦å™ªéŸ³å¤š**: Orthograd å’Œ SinkGD è¯åˆè™•ç†
- **å¤§å­¸ç¿’ç‡è¨“ç·´**: Prodigy é¢¨æ ¼çš„è‡ªé©æ‡‰èª¿æ•´

#### âœ… é©åˆä½¿ç”¨
- **å¯¦é©—æ€§ç ”ç©¶**: å¤šç¨®å‰æ²¿æŠ€è¡“æ•´åˆ
- **æ”¶æ–‚å›°é›£çš„ä»»å‹™**: å¼·å¤§çš„æ­£è¦åŒ–èƒ½åŠ›
- **è®Šç•°ç‡é«˜çš„æ¢¯åº¦**: VRAdam è‡ªå‹•èª¿æ•´

### ä¸é©ç”¨å ´æ™¯

#### âŒ ä¸å»ºè­°
- **æ¥µç°¡éœ€æ±‚**: å¦‚æœåªéœ€è¦åŸºç¤å„ªåŒ–å™¨
- **è³‡æºæ¥µåº¦å—é™**: ç›¸æ¯” SGD æœ‰ä¸€å®šé–‹éŠ·
- **å‚³çµ± CNN**: å¯èƒ½éåº¦è¨­è¨ˆ

## ğŸ§ª æœ€ä½³å¯¦è¸

### 1. æš–èº«éšæ®µè¨­ç½®

```python
# å»ºè­°çš„æš–èº«æ­¥æ•¸è¨­ç½®
total_steps = len(dataloader) * num_epochs
warmup_steps = min(1000, total_steps // 10)  # 10% æˆ–æœ€å¤š 1000 æ­¥

optimizer = Automagic_Sinkgd(
    model.parameters(),
    warmup_steps=warmup_steps
)
```

### 2. å­¸ç¿’ç‡ç¯„åœèª¿æ•´

```python
# æ ¹æ“šæ¨¡å‹å¤§å°èª¿æ•´å­¸ç¿’ç‡ç¯„åœ
if model_params > 1e9:  # å¤§æ¨¡å‹
    lr, min_lr, max_lr = 1e-5, 5e-7, 5e-4
elif model_params > 1e8:  # ä¸­å‹æ¨¡å‹
    lr, min_lr, max_lr = 3e-5, 1e-6, 1e-3
else:  # å°æ¨¡å‹
    lr, min_lr, max_lr = 5e-5, 1e-6, 1e-3
```

### 3. è¨“ç·´ç‹€æ…‹ç›£æ§

```python
# ç›£æ§å„ªåŒ–å™¨ç‹€æ…‹
def log_optimizer_stats(optimizer, step):
    for group_idx, group in enumerate(optimizer.param_groups):
        for param_idx, param in enumerate(group['params']):
            if param.grad is not None:
                state = optimizer.state[param]
                if 'avg_lr' in state:
                    print(f"Step {step}, Group {group_idx}, Param {param_idx}: avg_lr = {state['avg_lr']:.2e}")
```

### 4. å‹•æ…‹åƒæ•¸èª¿æ•´

```python
# æ ¹æ“šè¨“ç·´é€²åº¦å‹•æ…‹èª¿æ•´
def adjust_optimizer_params(optimizer, epoch, total_epochs):
    progress = epoch / total_epochs

    # å¾ŒæœŸé™ä½çµ±è¨ˆæ›´æ–°é »ç‡
    if progress > 0.8:
        for group in optimizer.param_groups:
            group['stats_update_freq'] = 10

    # å•Ÿç”¨å¾ŒæœŸæ­£äº¤æ¢¯åº¦ä¿®æ­£
    if progress > 0.5:
        for group in optimizer.param_groups:
            group['orthograd'] = True
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. **è¨“ç·´ç™¼æ•£**
```python
# è§£æ±ºæ–¹æ¡ˆï¼šé™ä½å­¸ç¿’ç‡å’Œèª¿æ•´åƒæ•¸
optimizer = Automagic_Sinkgd(
    model.parameters(),
    lr=1e-6,                # é™ä½åŸºç¤å­¸ç¿’ç‡
    max_lr=1e-4,           # é™ä½æœ€å¤§å­¸ç¿’ç‡
    lr_bump=5e-6,          # æ¸›å°èª¿æ•´å¹…åº¦
    orthograd=True         # å•Ÿç”¨æ­£äº¤æ¢¯åº¦ä¿®æ­£
)
```

#### 2. **æ”¶æ–‚éæ…¢**
```python
# è§£æ±ºæ–¹æ¡ˆï¼šæé«˜å­¸ç¿’ç‡å’Œæ¸›å°‘é™åˆ¶
optimizer = Automagic_Sinkgd(
    model.parameters(),
    lr=5e-5,               # æé«˜åŸºç¤å­¸ç¿’ç‡
    lr_bump=2e-5,          # å¢å¤§èª¿æ•´å¹…åº¦
    d_coef=3,              # æé«˜ Prodigy ä¿‚æ•¸
    warmup_steps=200       # ç¸®çŸ­æš–èº«æœŸ
)
```

#### 3. **è¨˜æ†¶é«”ä½¿ç”¨éé«˜**
```python
# è§£æ±ºæ–¹æ¡ˆï¼šå„ªåŒ–è¨˜æ†¶é«”è¨­ç½®
optimizer = Automagic_Sinkgd(
    model.parameters(),
    stats_update_freq=10,   # æ¸›å°‘çµ±è¨ˆæ›´æ–°é »ç‡
    orthograd=False,        # é—œé–‰ Orthograd
    full_finetune=False     # ä½¿ç”¨ LoRA æ¨¡å¼
)
```

## ğŸ“š åƒè€ƒæ–‡ç»

1. **ADOPT**: [Modified Adam Can Converge with Any Î²_2 with the Optimal Rate](https://arxiv.org/abs/2411.02853)
2. **Prodigy**: [An Expeditiously Adaptive Parameter-Free Learner](https://arxiv.org/pdf/2306.06101)
3. **ALLoRA**: [Adaptive Learning Rate Mitigates LoRA Fatal Flaws](https://arxiv.org/abs/2410.09692)
4. **Orthograd**: [Grokking at the Edge of Numerical Stability](https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability)
5. **SinkGD**: Sinkhorn Gradient Descent ç›¸é—œç ”ç©¶

## ğŸ¤ è²¢ç»èˆ‡æ”¯æ´

### ä¸»è¦é–‹ç™¼è€…
- **åŸå§‹å¯¦ç¾**: [gesen2egee](https://github.com/gesen2egee)
- **åŸå§‹æ¶æ§‹**: sd-scripts é–‹ç™¼åœ˜éšŠ
- **å„ªåŒ–æ”¹é€²**: å¤šé‡ kernel èåˆå’Œæ€§èƒ½å„ªåŒ–

### ç¤¾ç¾¤æ”¯æ´
- å•é¡Œå›å ±ï¼šè«‹åœ¨ GitHub Issues ä¸­æäº¤
- åŠŸèƒ½å»ºè­°ï¼šæ­¡è¿è¨è«–æ–°çš„å„ªåŒ–æŠ€è¡“æ•´åˆ
- è²¢ç»ä»£ç¢¼ï¼šéµå¾ªç¾æœ‰çš„ä»£ç¢¼é¢¨æ ¼å’Œæ–‡æª”æ¨™æº–

---

*æœ€å¾Œæ›´æ–°ï¼š2025å¹´ - æœ¬æ–‡æª”å°‡æŒçºŒæ›´æ–°ä»¥åæ˜ æœ€æ–°çš„å„ªåŒ–å™¨æ”¹é€²*