#!/usr/bin/env python3
"""
å¤šç¶­å¼µé‡æ­£å‰‡åŒ–è™•ç†æ¸¬è©¦è…³æœ¬

æ¸¬è©¦ HinaAdaptive å„ªåŒ–å™¨å°å¤šç¶­å¼µé‡çš„æ­£å‰‡åŒ–è™•ç†æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

æ³¨æ„ï¼šå‚…ç«‹è‘‰ç‰¹å¾µæå¤±åŠŸèƒ½å·²è¢«ç§»é™¤ï¼Œå› ç‚ºå®ƒä¸é©ç”¨æ–¼ SD-Scripts
çš„ latent space è¨“ç·´ç’°å¢ƒã€‚
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ åº«è·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from library.hina_adaptive import HinaAdaptive

def test_multidim_regularization():
    """æ¸¬è©¦å¤šç¶­å¼µé‡çš„æ­£å‰‡åŒ–è™•ç†"""
    print("=" * 60)
    print("å¤šç¶­å¼µé‡æ­£å‰‡åŒ–è™•ç†æ¸¬è©¦")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")

    # æ¸¬è©¦ä¸åŒç¶­åº¦çš„å¼µé‡
    test_cases = [
        ("1D å¼µé‡", torch.randn(128, device=device, requires_grad=True)),
        ("2D å¼µé‡", torch.randn(64, 32, device=device, requires_grad=True)),
        ("3D å¼µé‡", torch.randn(16, 8, 8, device=device, requires_grad=True)),
        ("4D å¼µé‡", torch.randn(32, 16, 3, 3, device=device, requires_grad=True)),
        ("5D å¼µé‡", torch.randn(8, 4, 2, 2, 2, device=device, requires_grad=True)),
    ]

    all_tests_passed = True

    for test_name, tensor in test_cases:
        print(f"\næ¸¬è©¦ {test_name} (å½¢ç‹€: {tensor.shape}):")
        print("-" * 40)

        try:
            # å‰µå»ºå„ªåŒ–å™¨ï¼Œå•Ÿç”¨å¤šç¨®æ­£å‰‡åŒ–æŠ€è¡“
            optimizer = HinaAdaptive(
                [tensor],
                lr=1e-3,
                # å„ç¨®æ­£å‰‡åŒ–æŠ€è¡“
                edge_suppression=True,
                edge_penalty=0.1,
                spatial_awareness=True,
                frequency_penalty=0.05,
                background_regularization=True,
                lora_rank_penalty=True,
                rank_penalty_strength=0.01,
                # è¨˜æ†¶é«”å„ªåŒ–
                memory_efficient=True,
                vram_budget_gb=8.0
            )

            # å‰µå»ºç›®æ¨™å¼µé‡
            target = torch.randn_like(tensor)

            # åŸ·è¡Œå‰å‘å’Œåå‘å‚³æ’­
            optimizer.zero_grad()
            output = tensor * 2.0
            loss = nn.functional.mse_loss(output, target)
            loss.backward()
            optimizer.step()

            print(f"âœ… {test_name} è™•ç†æˆåŠŸ")
            print(f"   - å¼µé‡å½¢ç‹€: {tensor.shape}")
            print(f"   - å…ƒç´ æ•¸é‡: {tensor.numel()}")
            print(f"   - æ¢¯åº¦ç¯„æ•¸: {torch.norm(tensor.grad).item():.6f}")
            print(f"   - æå¤±å€¼: {loss.item():.6f}")

        except Exception as e:
            print(f"âŒ {test_name} è™•ç†å¤±æ•—: {e}")
            all_tests_passed = False

    return all_tests_passed

def test_conv_layers_regularization():
    """æ¸¬è©¦å·ç©å±¤çš„æ­£å‰‡åŒ–è™•ç†"""
    print("\n" + "=" * 60)
    print("å·ç©å±¤æ­£å‰‡åŒ–è™•ç†æ¸¬è©¦")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # æ¸¬è©¦ä¸åŒé¡å‹çš„å·ç©å±¤
    test_layers = [
        ("Conv1d", nn.Conv1d(16, 32, 3, padding=1)),
        ("Conv2d", nn.Conv2d(16, 32, 3, padding=1)),
        ("Conv3d", nn.Conv3d(16, 32, 3, padding=1)),
        ("ConvTranspose2d", nn.ConvTranspose2d(32, 16, 3, padding=1)),
        ("DepthwiseConv2d", nn.Conv2d(16, 16, 3, padding=1, groups=16)),
    ]

    all_tests_passed = True

    for layer_name, layer in test_layers:
        print(f"\næ¸¬è©¦ {layer_name}:")
        print("-" * 30)

        try:
            layer = layer.to(device)

            # å‰µå»ºå„ªåŒ–å™¨
            optimizer = HinaAdaptive(
                layer.parameters(),
                lr=1e-3,
                edge_suppression=True,
                edge_penalty=0.1,
                spatial_awareness=True,
                frequency_penalty=0.05,
                memory_efficient=True
            )

            # å‰µå»ºæ¸¬è©¦æ•¸æ“š
            if isinstance(layer, nn.Conv1d):
                x = torch.randn(2, 16, 32, device=device)
            elif isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):
                x = torch.randn(2, 16, 32, 32, device=device)
            elif isinstance(layer, nn.Conv3d):
                x = torch.randn(2, 16, 16, 16, 16, device=device)
            else:
                x = torch.randn(2, 16, 32, 32, device=device)

            # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
            optimizer.zero_grad()
            output = layer(x)
            loss = torch.mean(output ** 2)
            loss.backward()
            optimizer.step()

            print(f"âœ… {layer_name} è™•ç†æˆåŠŸ")
            print(f"   - æ¬Šé‡å½¢ç‹€: {layer.weight.shape}")
            print(f"   - è¼¸å…¥å½¢ç‹€: {x.shape}")
            print(f"   - è¼¸å‡ºå½¢ç‹€: {output.shape}")
            print(f"   - æå¤±å€¼: {loss.item():.6f}")

        except Exception as e:
            print(f"âŒ {layer_name} è™•ç†å¤±æ•—: {e}")
            all_tests_passed = False

    return all_tests_passed

def test_complex_model_regularization():
    """æ¸¬è©¦è¤‡é›œæ¨¡å‹çš„æ­£å‰‡åŒ–è™•ç†"""
    print("\n" + "=" * 60)
    print("è¤‡é›œæ¨¡å‹æ­£å‰‡åŒ–è™•ç†æ¸¬è©¦")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # å‰µå»ºè¤‡é›œæ¨¡å‹
    class ComplexModel(nn.Module):
        def __init__(self):
            super().__init__()
            # å¤šç¨®é¡å‹çš„å±¤
            self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
            self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
            self.bn1 = nn.BatchNorm2d(32)
            self.bn2 = nn.BatchNorm2d(64)
            self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))
            self.linear1 = nn.Linear(128 * 4 * 4, 256)
            self.linear2 = nn.Linear(256, 128)
            self.linear3 = nn.Linear(128, 10)
            self.dropout = nn.Dropout(0.5)

        def forward(self, x):
            x = torch.relu(self.bn1(self.conv1(x)))
            x = torch.relu(self.bn2(self.conv2(x)))
            x = torch.relu(self.conv3(x))
            x = self.adaptive_pool(x)
            x = x.view(x.size(0), -1)
            x = torch.relu(self.linear1(x))
            x = self.dropout(x)
            x = torch.relu(self.linear2(x))
            x = self.linear3(x)
            return x

    try:
        model = ComplexModel().to(device)

        # å‰µå»ºå„ªåŒ–å™¨ï¼Œå•Ÿç”¨æ‰€æœ‰æ­£å‰‡åŒ–æŠ€è¡“
        optimizer = HinaAdaptive(
            model.parameters(),
            lr=1e-3,
            # æ‰€æœ‰æ­£å‰‡åŒ–æŠ€è¡“
            edge_suppression=True,
            edge_penalty=0.1,
            spatial_awareness=True,
            frequency_penalty=0.05,
            background_regularization=True,
            lora_rank_penalty=True,
            rank_penalty_strength=0.01,
            # å‹•æ…‹è‡ªé©æ‡‰
            use_dynamic_adaptation=True,
            # è¨˜æ†¶é«”å„ªåŒ–
            memory_efficient=True,
            vram_budget_gb=8.0
        )

        # å‰µå»ºæ¸¬è©¦æ•¸æ“š
        x = torch.randn(4, 3, 32, 32, device=device)
        y = torch.randint(0, 10, (4,), device=device)

        print("æ¨¡å‹åƒæ•¸çµ±è¨ˆ:")
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"   - ç¸½åƒæ•¸æ•¸é‡: {total_params:,}")
        print(f"   - å¯è¨“ç·´åƒæ•¸æ•¸é‡: {trainable_params:,}")

        # åŸ·è¡Œå¤šæ­¥è¨“ç·´
        print("\nåŸ·è¡Œè¨“ç·´æ­¥é©Ÿ:")
        for step in range(5):
            optimizer.zero_grad()
            output = model(x)
            loss = nn.functional.cross_entropy(output, y)
            loss.backward()
            optimizer.step()

            print(f"æ­¥é©Ÿ {step+1}: æå¤± = {loss.item():.4f}")

        # æª¢æŸ¥å„ªåŒ–å™¨ç‹€æ…‹
        info = optimizer.get_optimization_info()
        print(f"\nå„ªåŒ–å™¨ç‹€æ…‹:")
        print(f"   - å•Ÿç”¨çš„æ­£å‰‡åŒ–æŠ€è¡“: {sum(info['features'].values())}")
        print(f"   - å‹•æ…‹è‡ªé©æ‡‰: {info['features']['dynamic_adaptation']}")
        print(f"   - è¨˜æ†¶é«”å„ªåŒ–: {info['memory_optimization']['memory_efficient']}")

        # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨
        memory_stats = optimizer.get_memory_stats()
        print(f"   - è¨˜æ†¶é«”å£“åŠ›: {memory_stats['memory_pressure']:.2%}")

        print("âœ… è¤‡é›œæ¨¡å‹æ­£å‰‡åŒ–è™•ç†æ¸¬è©¦æˆåŠŸ")
        return True

    except Exception as e:
        print(f"âŒ è¤‡é›œæ¨¡å‹æ­£å‰‡åŒ–è™•ç†æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_batch_dimension_handling():
    """æ¸¬è©¦æ‰¹æ¬¡ç¶­åº¦è™•ç†"""
    print("\n" + "=" * 60)
    print("æ‰¹æ¬¡ç¶­åº¦è™•ç†æ¸¬è©¦")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°
    batch_sizes = [1, 2, 4, 8, 16]

    all_tests_passed = True

    for batch_size in batch_sizes:
        print(f"\næ¸¬è©¦æ‰¹æ¬¡å¤§å° {batch_size}:")
        print("-" * 30)

        try:
            # å‰µå»ºå·ç©å±¤
            layer = nn.Conv2d(3, 16, 3, padding=1).to(device)

            # å‰µå»ºå„ªåŒ–å™¨
            optimizer = HinaAdaptive(
                layer.parameters(),
                lr=1e-3,
                edge_suppression=True,
                background_regularization=True,
                memory_efficient=True
            )

            # å‰µå»ºæ¸¬è©¦æ•¸æ“š
            x = torch.randn(batch_size, 3, 32, 32, device=device)
            y = torch.randn(batch_size, 16, 32, 32, device=device)

            # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
            optimizer.zero_grad()
            output = layer(x)
            loss = nn.functional.mse_loss(output, y)
            loss.backward()
            optimizer.step()

            print(f"âœ… æ‰¹æ¬¡å¤§å° {batch_size} è™•ç†æˆåŠŸ")
            print(f"   - è¼¸å…¥å½¢ç‹€: {x.shape}")
            print(f"   - è¼¸å‡ºå½¢ç‹€: {output.shape}")
            print(f"   - æå¤±å€¼: {loss.item():.6f}")

        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡å¤§å° {batch_size} è™•ç†å¤±æ•—: {e}")
            all_tests_passed = False

    return all_tests_passed

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” å¤šç¶­å¼µé‡æ­£å‰‡åŒ–è™•ç†æ¸¬è©¦")
    print("=" * 60)

    tests = [
        ("å¤šç¶­å¼µé‡æ­£å‰‡åŒ–è™•ç†", test_multidim_regularization),
        ("å·ç©å±¤æ­£å‰‡åŒ–è™•ç†", test_conv_layers_regularization),
        ("è¤‡é›œæ¨¡å‹æ­£å‰‡åŒ–è™•ç†", test_complex_model_regularization),
        ("æ‰¹æ¬¡ç¶­åº¦è™•ç†", test_batch_dimension_handling),
    ]

    passed = 0
    total = len(tests)

    for test_name, test_func in tests:
        print(f"\nğŸ§ª åŸ·è¡Œ {test_name}...")
        try:
            if test_func():
                print(f"âœ… {test_name} é€šé")
                passed += 1
            else:
                print(f"âŒ {test_name} å¤±æ•—")
        except Exception as e:
            print(f"âŒ {test_name} ç•°å¸¸: {e}")

    # ç¸½çµçµæœ
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æ¸¬è©¦çµæœ: {passed}/{total} é€šé")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰å¤šç¶­å¼µé‡æ­£å‰‡åŒ–è™•ç†æ¸¬è©¦é€šéï¼")
        print("âœ… å„ç¨®æ­£å‰‡åŒ–æŠ€è¡“éƒ½èƒ½æ­£ç¢ºè™•ç†å¤šç¶­å¼µé‡")
    else:
        print(f"âš ï¸  {total - passed} å€‹æ¸¬è©¦å¤±æ•—")
        print("âŒ æŸäº›æ­£å‰‡åŒ–æŠ€è¡“å¯èƒ½å­˜åœ¨å¤šç¶­å¼µé‡è™•ç†å•é¡Œ")

    return passed == total

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)