#!/usr/bin/env python3
"""
å¤šç¶­å¼µé‡å‚…ç«‹è‘‰ç‰¹å¾µæå¤±ä¿®å¾©æ¸¬è©¦è…³æœ¬

æ¸¬è©¦ HinaAdaptive å„ªåŒ–å™¨åœ¨è™•ç†å·ç©å±¤æ¬Šé‡ç­‰å¤šç¶­å¼µé‡æ™‚æ˜¯å¦èƒ½æ­£ç¢ºå·¥ä½œ
åŸå§‹éŒ¯èª¤ï¼šIndexError: The shape of the mask [3, 3] at index 0 does not match the shape of the indexed tensor [40, 40, 3, 3]
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ åº«è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

from library.hina_adaptive import HinaAdaptive


class MultiDimTestModel(nn.Module):
    """åŒ…å«ä¸åŒç¶­åº¦åƒæ•¸çš„æ¸¬è©¦æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        # 4D å·ç©æ¬Šé‡ (æœƒè§¸ç™¼åŸå§‹éŒ¯èª¤)
        self.conv1 = nn.Conv2d(3, 40, kernel_size=3, padding=1)  # [40, 3, 3, 3]
        self.conv2 = nn.Conv2d(40, 40, kernel_size=3, padding=1)  # [40, 40, 3, 3] - åŸå§‹éŒ¯èª¤è§¸ç™¼

        # 2D å…¨é€£æ¥æ¬Šé‡
        self.fc1 = nn.Linear(40*16*16, 128)  # [128, 40*16*16]
        self.fc2 = nn.Linear(128, 10)  # [10, 128]

        # 1D åç½®é …
        self.bias = nn.Parameter(torch.randn(10))  # [10]

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = nn.AdaptiveAvgPool2d((16, 16))(x)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        x = x + self.bias
        return x


def test_multidim_tensor_handling():
    """æ¸¬è©¦å¤šç¶­å¼µé‡è™•ç†"""
    print("=== å¤šç¶­å¼µé‡å‚…ç«‹è‘‰ç‰¹å¾µæå¤±æ¸¬è©¦ ===")

    # å‰µå»ºæ¨¡å‹å’Œæ•¸æ“š
    model = MultiDimTestModel()
    input_data = torch.randn(2, 3, 32, 32)
    target = torch.randint(0, 10, (2,))

    print(f"æ¨¡å‹åƒæ•¸å½¢ç‹€:")
    for name, param in model.named_parameters():
        print(f"  {name}: {param.shape}")

    # å‰µå»ºå„ªåŒ–å™¨ï¼ˆå•Ÿç”¨å‚…ç«‹è‘‰ç‰¹å¾µæå¤±ï¼‰
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=0.001,
        fourier_feature_loss=True,
        super_resolution_mode=True,
        adaptive_frequency_weighting=True,
        fourier_high_freq_preservation=0.3,
        fourier_detail_enhancement=0.2,
        super_resolution_scale=4
    )

    print(f"\nå„ªåŒ–å™¨å‰µå»ºæˆåŠŸï¼Œå‚…ç«‹è‘‰ç‰¹å¾µæå¤±å·²å•Ÿç”¨")

    # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
    criterion = nn.CrossEntropyLoss()

    print(f"\né–‹å§‹è¨“ç·´æ¸¬è©¦...")
    for step in range(3):
        optimizer.zero_grad()

        # å‰å‘å‚³æ’­
        output = model(input_data)
        loss = criterion(output, target)

        print(f"\næ­¥é©Ÿ {step + 1}:")
        print(f"  æå¤±: {loss.item():.4f}")

        # åå‘å‚³æ’­
        loss.backward()

        # æª¢æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸
        grad_info = {}
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = torch.norm(param.grad).item()
                grad_info[name] = grad_norm
                print(f"  {name} æ¢¯åº¦ç¯„æ•¸: {grad_norm:.6f}")

        # å„ªåŒ–å™¨æ›´æ–°ï¼ˆé€™è£¡æœƒæ¸¬è©¦å‚…ç«‹è‘‰ç‰¹å¾µæå¤±ï¼‰
        try:
            optimizer.step()
            print(f"  âœ… å„ªåŒ–æ­¥é©ŸæˆåŠŸå®Œæˆ")
        except Exception as e:
            print(f"  âŒ å„ªåŒ–æ­¥é©Ÿå¤±æ•—: {e}")
            raise e

    print(f"\n=== æ¸¬è©¦å®Œæˆ ===")
    return True


def test_dimension_filtering():
    """æ¸¬è©¦ç¶­åº¦éæ¿¾é‚è¼¯"""
    print("\n=== ç¶­åº¦éæ¿¾é‚è¼¯æ¸¬è©¦ ===")

    # å‰µå»ºä¸åŒç¶­åº¦çš„åƒæ•¸é€²è¡Œæ¸¬è©¦
    test_cases = [
        ("1D å¼µé‡", torch.randn(100)),
        ("å° 2D å¼µé‡", torch.randn(4, 4)),  # å°æ–¼ 8x8ï¼Œæ‡‰è©²è¢«è·³é
        ("å¤§ 2D å¼µé‡", torch.randn(32, 32)),  # æ‡‰è©²è¢«è™•ç†
        ("3D å¼µé‡", torch.randn(10, 16, 16)),  # æ‡‰è©²è¢«è™•ç†
        ("å° 4D å¼µé‡", torch.randn(10, 20, 3, 3)),  # æœ€å¾Œå…©ç¶­å¤ªå°ï¼Œæ‡‰è©²è¢«è·³é
        ("å¤§ 4D å¼µé‡", torch.randn(40, 40, 8, 8)),  # æ‡‰è©²è¢«è™•ç†
    ]

    for case_name, test_tensor in test_cases:
        print(f"\næ¸¬è©¦ {case_name}: å½¢ç‹€ {test_tensor.shape}")

        # å‰µå»ºåŒ…å«å–®å€‹åƒæ•¸çš„æ¨¡å‹
        class SingleParamModel(nn.Module):
            def __init__(self, tensor):
                super().__init__()
                self.param = nn.Parameter(tensor.clone())

            def forward(self, x):
                return torch.sum(self.param)

        model = SingleParamModel(test_tensor)

        optimizer = HinaAdaptive(
            model.parameters(),
            lr=0.001,
            fourier_feature_loss=True,
            super_resolution_mode=True,
            adaptive_frequency_weighting=True
        )

        # æ¸¬è©¦ä¸€å€‹å„ªåŒ–æ­¥é©Ÿ
        try:
            loss = model(torch.tensor(1.0))
            loss.backward()
            optimizer.step()
            print(f"  âœ… è™•ç†æˆåŠŸ")
        except Exception as e:
            print(f"  âŒ è™•ç†å¤±æ•—: {e}")
            return False

    print(f"\nâœ… æ‰€æœ‰ç¶­åº¦éæ¿¾æ¸¬è©¦é€šé")
    return True


def test_shape_consistency():
    """æ¸¬è©¦å½¢ç‹€ä¸€è‡´æ€§"""
    print("\n=== å½¢ç‹€ä¸€è‡´æ€§æ¸¬è©¦ ===")

    # å‰µå»ºä¸€å€‹æœ‰ç‰¹å®šå½¢ç‹€çš„å·ç©å±¤
    conv = nn.Conv2d(16, 32, kernel_size=5, padding=2)  # [32, 16, 5, 5]

    optimizer = HinaAdaptive(
        conv.parameters(),
        lr=0.001,
        fourier_feature_loss=True,
        super_resolution_mode=True
    )

    # å‰µå»ºè¼¸å…¥å’Œç›®æ¨™
    input_data = torch.randn(1, 16, 28, 28)
    target = torch.randn(1, 32, 28, 28)

    print(f"å·ç©æ¬Šé‡å½¢ç‹€: {conv.weight.shape}")
    print(f"å·ç©åç½®å½¢ç‹€: {conv.bias.shape}")

    # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
    optimizer.zero_grad()
    output = conv(input_data)
    loss = nn.MSELoss()(output, target)
    loss.backward()

    # æª¢æŸ¥æ¢¯åº¦å½¢ç‹€
    print(f"æ¬Šé‡æ¢¯åº¦å½¢ç‹€: {conv.weight.grad.shape}")
    print(f"åç½®æ¢¯åº¦å½¢ç‹€: {conv.bias.grad.shape}")

    # åŸ·è¡Œå„ªåŒ–æ­¥é©Ÿ
    try:
        optimizer.step()
        print(f"âœ… å½¢ç‹€ä¸€è‡´æ€§æ¸¬è©¦é€šé")
        return True
    except Exception as e:
        print(f"âŒ å½¢ç‹€ä¸€è‡´æ€§æ¸¬è©¦å¤±æ•—: {e}")
        return False


if __name__ == "__main__":
    try:
        print("ğŸš€ é–‹å§‹å¤šç¶­å¼µé‡å‚…ç«‹è‘‰ç‰¹å¾µæå¤±ä¿®å¾©æ¸¬è©¦\n")

        # æ¸¬è©¦1ï¼šå¤šç¶­å¼µé‡è™•ç†
        success1 = test_multidim_tensor_handling()

        # æ¸¬è©¦2ï¼šç¶­åº¦éæ¿¾
        success2 = test_dimension_filtering()

        # æ¸¬è©¦3ï¼šå½¢ç‹€ä¸€è‡´æ€§
        success3 = test_shape_consistency()

        if success1 and success2 and success3:
            print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼å¤šç¶­å¼µé‡å‚…ç«‹è‘‰ç‰¹å¾µæå¤±ä¿®å¾©æˆåŠŸã€‚")
            print("\nä¿®å¾©å…§å®¹ï¼š")
            print("âœ… æ­£ç¢ºè™•ç†4Då·ç©æ¬Šé‡å¼µé‡")
            print("âœ… ç¶­åº¦éæ¿¾é‚è¼¯æ­£å¸¸å·¥ä½œ")
            print("âœ… FFTå’ŒIFFTæ“ä½œæ­£ç¢ºè™•ç†å¤šç¶­å¼µé‡")
            print("âœ… å½¢ç‹€ä¸€è‡´æ€§å¾—åˆ°ä¿è­‰")
            print("âœ… è‡ªé©æ‡‰é »ç‡æ¬Šé‡åŠŸèƒ½æ­£å¸¸")
        else:
            print("\nâŒ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦é€²ä¸€æ­¥æª¢æŸ¥ã€‚")

    except Exception as e:
        print(f"\nğŸ’¥ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()