#!/usr/bin/env python3
"""
HinaAdaptive å„ªåŒ–å™¨åŠŸèƒ½æ¸¬è©¦è…³æœ¬

æ¸¬è©¦ HinaAdaptive å„ªåŒ–å™¨çš„å„ç¨®åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

æ³¨æ„ï¼šæ½›åœ¨ç©ºé–“æª¢æ¸¬å’Œå‚…ç«‹è‘‰ç‰¹å¾µæå¤±åŠŸèƒ½å·²è¢«ç§»é™¤ï¼Œå› ç‚ºå®ƒå€‘ä¸é©ç”¨æ–¼
SD-Scripts çš„ latent space è¨“ç·´ç’°å¢ƒã€‚
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ åº«è·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from library.hina_adaptive import HinaAdaptive

def test_basic_functionality():
    """æ¸¬è©¦åŸºæœ¬åŠŸèƒ½"""
    print("=" * 60)
    print("åŸºæœ¬åŠŸèƒ½æ¸¬è©¦")
    print("=" * 60)

    # å‰µå»ºç°¡å–®æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(128, 64),
        nn.ReLU(),
        nn.Linear(64, 32),
        nn.ReLU(),
        nn.Linear(32, 10)
    )

    # å‰µå»ºå„ªåŒ–å™¨
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=1e-3,
        memory_efficient=True,
        vram_budget_gb=8.0
    )

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info = optimizer.get_optimization_info()
    print(f"å„ªåŒ–å™¨é¡å‹: {info['optimizer_type']}")
    print(f"è¨˜æ†¶é«”å„ªåŒ–: {info['memory_optimization']['memory_efficient']}")
    print(f"VRAM é ç®—: {info['memory_optimization']['vram_budget_gb']}GB")

    # æ¸¬è©¦ç°¡å–®è¨“ç·´
    x = torch.randn(8, 128)
    y = torch.randn(8, 10)

    for step in range(3):
        optimizer.zero_grad()
        output = model(x)
        loss = nn.functional.mse_loss(output, y)
        loss.backward()
        optimizer.step()

        print(f"æ­¥é©Ÿ {step+1}: æå¤± = {loss.item():.4f}")

    print("âœ… åŸºæœ¬åŠŸèƒ½æ¸¬è©¦é€šé")
    return True

def test_regularization_features():
    """æ¸¬è©¦æ­£å‰‡åŒ–åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æ­£å‰‡åŒ–åŠŸèƒ½æ¸¬è©¦")
    print("=" * 60)

    # å‰µå»º CNN æ¨¡å‹
    model = nn.Sequential(
        nn.Conv2d(3, 16, 3, padding=1),
        nn.ReLU(),
        nn.Conv2d(16, 32, 3, padding=1),
        nn.ReLU(),
        nn.AdaptiveAvgPool2d((4, 4)),
        nn.Flatten(),
        nn.Linear(32 * 4 * 4, 10)
    )

    # å‰µå»ºå„ªåŒ–å™¨ï¼Œå•Ÿç”¨å¤šç¨®æ­£å‰‡åŒ–æŠ€è¡“
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=1e-3,
        edge_suppression=True,
        edge_penalty=0.1,
        spatial_awareness=True,
        frequency_penalty=0.05,
        background_regularization=True,
        lora_rank_penalty=True,
        rank_penalty_strength=0.01,
        memory_efficient=True
    )

    # æª¢æŸ¥æ­£å‰‡åŒ–åŠŸèƒ½ç‹€æ…‹
    info = optimizer.get_optimization_info()
    print("å•Ÿç”¨çš„æ­£å‰‡åŒ–æŠ€è¡“:")
    if info['features']['edge_suppression']:
        print("  âœ… é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–")
    if info['features']['spatial_awareness']:
        print("  âœ… ç©ºé–“æ„ŸçŸ¥æ­£å‰‡åŒ–")
    if info['features']['background_regularization']:
        print("  âœ… èƒŒæ™¯æ­£å‰‡åŒ–")
    if info['features']['lora_rank_penalty']:
        print("  âœ… LoRA ä½ç§©æ­£å‰‡åŒ–")

    # æ¸¬è©¦è¨“ç·´
    x = torch.randn(4, 3, 32, 32)
    y = torch.randint(0, 10, (4,))

    for step in range(3):
        optimizer.zero_grad()
        output = model(x)
        loss = nn.functional.cross_entropy(output, y)
        loss.backward()
        optimizer.step()

        print(f"æ­¥é©Ÿ {step+1}: æå¤± = {loss.item():.4f}")

    print("âœ… æ­£å‰‡åŒ–åŠŸèƒ½æ¸¬è©¦é€šé")
    return True

def test_memory_optimization():
    """æ¸¬è©¦è¨˜æ†¶é«”å„ªåŒ–"""
    print("\n" + "=" * 60)
    print("è¨˜æ†¶é«”å„ªåŒ–æ¸¬è©¦")
    print("=" * 60)

    # å‰µå»ºè¼ƒå¤§çš„æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.ReLU(),
        nn.Linear(64, 10)
    )

    # å‰µå»ºå„ªåŒ–å™¨ï¼Œå•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=1e-3,
        memory_efficient=True,
        vram_budget_gb=4.0,
        reduce_precision=True,
        cpu_offload_states=True,
        max_buffer_memory_mb=200
    )

    # æª¢æŸ¥è¨˜æ†¶é«”è¨­ç½®
    info = optimizer.get_optimization_info()
    print(f"è¨˜æ†¶é«”å„ªåŒ–é…ç½®:")
    print(f"  - è¨˜æ†¶é«”æ•ˆç‡: {info['memory_optimization']['memory_efficient']}")
    print(f"  - VRAM é ç®—: {info['memory_optimization']['vram_budget_gb']}GB")
    print(f"  - ç²¾åº¦é™ä½: {info['memory_optimization']['reduce_precision']}")
    print(f"  - CPU ç‹€æ…‹å¸è¼‰: {info['memory_optimization']['cpu_offload_states']}")

    # æ¸¬è©¦è¨˜æ†¶é«”çµ±è¨ˆ
    memory_stats = optimizer.get_memory_stats()
    print(f"\nè¨˜æ†¶é«”çµ±è¨ˆ:")
    print(f"  - è¨˜æ†¶é«”å£“åŠ›: {memory_stats['memory_pressure']:.2%}")
    print(f"  - ç·©è¡æ± è¨˜æ†¶é«”: {memory_stats['buffer_pool_stats']['current_memory_mb']:.2f}MB")

    # æ¸¬è©¦è¨“ç·´
    x = torch.randn(16, 512)
    y = torch.randn(16, 10)

    for step in range(3):
        optimizer.zero_grad()
        output = model(x)
        loss = nn.functional.mse_loss(output, y)
        loss.backward()
        optimizer.step()

        print(f"æ­¥é©Ÿ {step+1}: æå¤± = {loss.item():.4f}")

    print("âœ… è¨˜æ†¶é«”å„ªåŒ–æ¸¬è©¦é€šé")
    return True

def test_advanced_features():
    """æ¸¬è©¦é«˜ç´šåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("é«˜ç´šåŠŸèƒ½æ¸¬è©¦")
    print("=" * 60)

    # å‰µå»ºæ¨¡å‹
    model = nn.Sequential(
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.ReLU(),
        nn.Linear(64, 10)
    )

    # å‰µå»ºå„ªåŒ–å™¨ï¼Œå•Ÿç”¨é«˜ç´šåŠŸèƒ½
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=1e-3,
        # é«˜ç´šåŠŸèƒ½
        use_dynamic_adaptation=True,
        use_tam=True,
        use_cautious=True,
        use_spd=True,
        use_orthogonal_grad=True,
        use_lr_mask=True,
        dynamic_weight_decay=True,
        # è¨˜æ†¶é«”å„ªåŒ–
        memory_efficient=True
    )

    # æª¢æŸ¥é«˜ç´šåŠŸèƒ½ç‹€æ…‹
    info = optimizer.get_optimization_info()
    print("å•Ÿç”¨çš„é«˜ç´šåŠŸèƒ½:")
    if info['features']['dynamic_adaptation']:
        print("  âœ… å‹•æ…‹è‡ªé©æ‡‰")
    if info['features']['tam']:
        print("  âœ… TAM é˜»å°¼")
    if info['features']['cautious']:
        print("  âœ… è¬¹æ…æ›´æ–°")
    if info['features']['spd']:
        print("  âœ… SPD æ­£å‰‡åŒ–")
    if info['features']['orthogonal_grad']:
        print("  âœ… æ­£äº¤æ¢¯åº¦æŠ•å½±")
    if info['features']['lr_mask']:
        print("  âœ… å­¸ç¿’ç‡é®ç½©")
    if info['features']['dynamic_weight_decay']:
        print("  âœ… å‹•æ…‹æ¬Šé‡è¡°æ¸›")

    # æ¸¬è©¦è¨“ç·´
    x = torch.randn(8, 256)
    y = torch.randn(8, 10)

    for step in range(5):
        optimizer.zero_grad()
        output = model(x)
        loss = nn.functional.mse_loss(output, y)
        loss.backward()
        optimizer.step()

        print(f"æ­¥é©Ÿ {step+1}: æå¤± = {loss.item():.4f}")

    print("âœ… é«˜ç´šåŠŸèƒ½æ¸¬è©¦é€šé")
    return True

def test_removed_features():
    """æ¸¬è©¦ç¢ºèªå·²ç§»é™¤çš„åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("å·²ç§»é™¤åŠŸèƒ½ç¢ºèªæ¸¬è©¦")
    print("=" * 60)

    # å‰µå»ºæ¨¡å‹
    model = nn.Linear(64, 32)

    # æ¸¬è©¦å·²ç§»é™¤çš„ fourier_feature_loss åƒæ•¸
    try:
        optimizer = HinaAdaptive(
            model.parameters(),
            lr=1e-3,
            fourier_feature_loss=True,  # é€™å€‹åƒæ•¸å·²è¢«ç§»é™¤
            memory_efficient=True
        )
        print("âŒ æ‡‰è©²å¼•ç™¼éŒ¯èª¤ä½†æ²’æœ‰ï¼Œfourier_feature_loss åƒæ•¸å¯èƒ½ä»ç„¶å­˜åœ¨")
        return False
    except TypeError as e:
        if "fourier_feature_loss" in str(e):
            print("âœ… ç¢ºèª fourier_feature_loss åƒæ•¸å·²è¢«ç§»é™¤")
        else:
            print(f"âŒ æ„å¤–çš„éŒ¯èª¤: {e}")
            return False

    # æ¸¬è©¦å·²ç§»é™¤çš„ super_resolution_mode åƒæ•¸
    try:
        optimizer = HinaAdaptive(
            model.parameters(),
            lr=1e-3,
            super_resolution_mode=True,  # é€™å€‹åƒæ•¸å·²è¢«ç§»é™¤
            memory_efficient=True
        )
        print("âŒ æ‡‰è©²å¼•ç™¼éŒ¯èª¤ä½†æ²’æœ‰ï¼Œsuper_resolution_mode åƒæ•¸å¯èƒ½ä»ç„¶å­˜åœ¨")
        return False
    except TypeError as e:
        if "super_resolution_mode" in str(e):
            print("âœ… ç¢ºèª super_resolution_mode åƒæ•¸å·²è¢«ç§»é™¤")
        else:
            print(f"âŒ æ„å¤–çš„éŒ¯èª¤: {e}")
            return False

    # æ¸¬è©¦å·²ç§»é™¤çš„ adaptive_frequency_weighting åƒæ•¸
    try:
        optimizer = HinaAdaptive(
            model.parameters(),
            lr=1e-3,
            adaptive_frequency_weighting=True,  # é€™å€‹åƒæ•¸å·²è¢«ç§»é™¤
            memory_efficient=True
        )
        print("âŒ æ‡‰è©²å¼•ç™¼éŒ¯èª¤ä½†æ²’æœ‰ï¼Œadaptive_frequency_weighting åƒæ•¸å¯èƒ½ä»ç„¶å­˜åœ¨")
        return False
    except TypeError as e:
        if "adaptive_frequency_weighting" in str(e):
            print("âœ… ç¢ºèª adaptive_frequency_weighting åƒæ•¸å·²è¢«ç§»é™¤")
        else:
            print(f"âŒ æ„å¤–çš„éŒ¯èª¤: {e}")
            return False

    print("âœ… å·²ç§»é™¤åŠŸèƒ½ç¢ºèªæ¸¬è©¦é€šé")
    return True

def test_optimization_performance():
    """æ¸¬è©¦å„ªåŒ–å™¨æ€§èƒ½"""
    print("\n" + "=" * 60)
    print("å„ªåŒ–å™¨æ€§èƒ½æ¸¬è©¦")
    print("=" * 60)

    # å‰µå»ºæ¨¡å‹
    model = nn.Sequential(
        nn.Linear(128, 64),
        nn.ReLU(),
        nn.Linear(64, 32),
        nn.ReLU(),
        nn.Linear(32, 10)
    )

    # å‰µå»ºå„ªåŒ–å™¨
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=1e-3,
        memory_efficient=True,
        vram_budget_gb=8.0
    )

    # æ¸¬è©¦å¤šæ¬¡è¨“ç·´æ­¥é©Ÿ
    x = torch.randn(16, 128)
    y = torch.randn(16, 10)

    initial_loss = None
    final_loss = None

    for step in range(10):
        optimizer.zero_grad()
        output = model(x)
        loss = nn.functional.mse_loss(output, y)

        if step == 0:
            initial_loss = loss.item()

        loss.backward()
        optimizer.step()

        if step == 9:
            final_loss = loss.item()

        if step % 3 == 0:
            print(f"æ­¥é©Ÿ {step+1}: æå¤± = {loss.item():.4f}")

    # è¨ˆç®—æ”¹å–„ç‡
    improvement = (initial_loss - final_loss) / initial_loss * 100
    print(f"\næ€§èƒ½çµæœ:")
    print(f"  - åˆå§‹æå¤±: {initial_loss:.4f}")
    print(f"  - æœ€çµ‚æå¤±: {final_loss:.4f}")
    print(f"  - æ”¹å–„ç‡: {improvement:.1f}%")

    if improvement > 0:
        print("âœ… å„ªåŒ–å™¨æ€§èƒ½æ¸¬è©¦é€šé")
        return True
    else:
        print("âŒ å„ªåŒ–å™¨æ€§èƒ½æ¸¬è©¦å¤±æ•—")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ” HinaAdaptive å„ªåŒ–å™¨åŠŸèƒ½æ¸¬è©¦")
    print("=" * 60)

    tests = [
        ("åŸºæœ¬åŠŸèƒ½", test_basic_functionality),
        ("æ­£å‰‡åŒ–åŠŸèƒ½", test_regularization_features),
        ("è¨˜æ†¶é«”å„ªåŒ–", test_memory_optimization),
        ("é«˜ç´šåŠŸèƒ½", test_advanced_features),
        ("å·²ç§»é™¤åŠŸèƒ½ç¢ºèª", test_removed_features),
        ("å„ªåŒ–å™¨æ€§èƒ½", test_optimization_performance),
    ]

    passed = 0
    total = len(tests)

    for test_name, test_func in tests:
        print(f"\nğŸ§ª åŸ·è¡Œ {test_name}...")
        try:
            if test_func():
                print(f"âœ… {test_name} é€šé")
                passed += 1
            else:
                print(f"âŒ {test_name} å¤±æ•—")
        except Exception as e:
            print(f"âŒ {test_name} ç•°å¸¸: {e}")

    # ç¸½çµçµæœ
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æ¸¬è©¦çµæœ: {passed}/{total} é€šé")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼")
        print("âœ… HinaAdaptive å„ªåŒ–å™¨åŠŸèƒ½æ­£å¸¸")
    else:
        print(f"âš ï¸  {total - passed} å€‹æ¸¬è©¦å¤±æ•—")
        print("âŒ æŸäº›åŠŸèƒ½å¯èƒ½å­˜åœ¨å•é¡Œ")

    return passed == total

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)