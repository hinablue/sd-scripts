#!/usr/bin/env python3
"""
LoRA å„ªåŒ–ç‰ˆ Automagic_CameAMP æ¸¬è©¦è…³æœ¬

é€™å€‹è…³æœ¬å±•ç¤ºäº†é‡å° Stable Diffusion LoRA è¨“ç·´å„ªåŒ–çš„ä½¿ç”¨æ–¹æ³•ï¼Œ
è§£æ±ºäº†å­¸ç¿’ç‡ä¹˜æ•¸éå°çš„å•é¡Œã€‚

ä½œè€…: Hina
ç‰ˆæœ¬: 1.0
æ—¥æœŸ: 2025-01-27
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import os
import sys
from typing import Dict, List

# æ·»åŠ åº«è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from library.automagic_cameamp import (
        Automagic_CameAMP_COptim,
        Automagic_CameAMP_COptim8bit,
        OptimizerConfig
    )
    print("âœ… æˆåŠŸå°å…¥ Automagic_CameAMP å„ªåŒ–å™¨")
except ImportError as e:
    print(f"âŒ ç„¡æ³•å°å…¥å„ªåŒ–å™¨: {e}")
    sys.exit(1)

# è¨­å®šè¨­å‚™
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")

class LoRALayer(nn.Module):
    """æ¨¡æ“¬ LoRA å±¤"""

    def __init__(self, in_features: int, out_features: int, rank: int = 16):
        super().__init__()
        self.rank = rank
        self.in_features = in_features
        self.out_features = out_features

        # LoRA åˆ†è§£ï¼šA å’Œ B çŸ©é™£
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))

        # åŸå§‹æ¬Šé‡ï¼ˆå‡çµï¼‰
        self.register_buffer('base_weight', torch.randn(out_features, in_features))

    def forward(self, x):
        # åŸºç¤å‰å‘å‚³æ’­ + LoRA èª¿æ•´
        base_out = F.linear(x, self.base_weight)
        lora_out = F.linear(F.linear(x, self.lora_A.T), self.lora_B.T)
        return base_out + lora_out

class MockStableDiffusionModel(nn.Module):
    """æ¨¡æ“¬ Stable Diffusion æ¨¡å‹çš„é—œéµéƒ¨åˆ†"""

    def __init__(self):
        super().__init__()

        # æ¨¡æ“¬ UNet çš„æ³¨æ„åŠ›å±¤ï¼ˆä½¿ç”¨ LoRA å¾®èª¿ï¼‰
        self.cross_attn_lora = LoRALayer(768, 768, rank=16)
        self.self_attn_lora = LoRALayer(768, 768, rank=8)

        # æ¨¡æ“¬å…¶ä»–å±¤
        self.norm = nn.LayerNorm(768)
        self.proj = nn.Linear(768, 512)

    def forward(self, x):
        # æ¨¡æ“¬æ³¨æ„åŠ›è¨ˆç®—
        x = self.norm(x)
        x = self.cross_attn_lora(x)
        x = self.self_attn_lora(x)
        x = self.proj(x)
        return x

def create_lora_optimizer_config(optimizer_type: str = "coptim") -> Dict:
    """å‰µå»º LoRA å„ªåŒ–çš„é…ç½®"""

    base_config = {
        'lr': 1e-3,               # LoRA é€šå¸¸éœ€è¦è¼ƒé«˜å­¸ç¿’ç‡
        'weight_decay': 1e-4,     # é©ä¸­çš„æ¬Šé‡è¡°æ¸›
        'warmup_steps': 300,      # è¼ƒçŸ­çš„æš–èº«æœŸ
        'full_finetune': False,   # å•Ÿç”¨ ALLoRA è¡Œç¸®æ”¾
        'verbose': True
    }

    if optimizer_type == "coptim":
        # C-Optim ç‰ˆæœ¬çš„ LoRA å‹å¥½é…ç½®
        base_config.update({
            'context_window': 30,      # è¼ƒå°çª—å£ï¼Œæ›´éˆæ•
            'edge_threshold': 0.6,     # é™ä½é–¾å€¼ï¼Œæ¸›å°‘é‚Šç·£æƒ…æ³è§¸ç™¼
            'adaptation_rate': 0.25    # æé«˜é©æ‡‰é€Ÿç‡
        })

    return base_config

def test_lr_multiplier_evolution(optimizer, num_steps: int = 200):
    """æ¸¬è©¦å­¸ç¿’ç‡ä¹˜æ•¸çš„æ¼”åŒ–éç¨‹"""

    print(f"\nğŸ“Š æ¸¬è©¦ {optimizer.__class__.__name__} çš„å­¸ç¿’ç‡ä¹˜æ•¸æ¼”åŒ–")
    print("-" * 60)

    model = MockStableDiffusionModel().to(device)

    # è¨˜éŒ„æŒ‡æ¨™
    lr_multipliers = []
    edge_cases = []
    grad_consistencies = []
    loss_stabilities = []
    losses = []

    for step in range(num_steps):
        # æ¨¡æ“¬è¨“ç·´æ•¸æ“š
        batch_size = 4
        seq_len = 77  # å…¸å‹çš„ CLIP åºåˆ—é•·åº¦
        x = torch.randn(batch_size, seq_len, 768, device=device)
        target = torch.randn(batch_size, seq_len, 512, device=device)

        # å‰å‘å‚³æ’­
        optimizer.zero_grad()
        output = model(x)
        loss = F.mse_loss(output, target)
        loss.backward()
        optimizer.step()

        losses.append(loss.item())

        # è¨˜éŒ„ C-Optim æŒ‡æ¨™
        if hasattr(optimizer, 'c_optim'):
            lr_mult = optimizer.c_optim.compute_contextual_lr_multiplier()
            is_edge = optimizer.c_optim.detect_edge_case()
            grad_consistency = optimizer.c_optim.compute_gradient_consistency()
            loss_stability = optimizer.c_optim.compute_loss_stability()

            lr_multipliers.append(lr_mult)
            edge_cases.append(is_edge)
            grad_consistencies.append(grad_consistency)
            loss_stabilities.append(loss_stability)

            # å®šæœŸè¼¸å‡ºç‹€æ…‹
            if step % 50 == 0:
                status = "ğŸ”´ é‚Šç·£" if is_edge else "ğŸŸ¢ æ­£å¸¸"
                print(f"Step {step:3d}: Loss={loss.item():.4f}, "
                      f"LRä¹˜æ•¸={lr_mult:.3f}, ç‹€æ…‹={status}")
        else:
            lr_multipliers.append(1.0)
            edge_cases.append(False)
            grad_consistencies.append(1.0)
            loss_stabilities.append(1.0)

    return {
        'lr_multipliers': lr_multipliers,
        'edge_cases': edge_cases,
        'grad_consistencies': grad_consistencies,
        'loss_stabilities': loss_stabilities,
        'losses': losses
    }

def compare_optimizers():
    """æ¯”è¼ƒå„ªåŒ–å‰å¾Œçš„å„ªåŒ–å™¨æ€§èƒ½"""

    print("\nğŸ”¬ LoRA å„ªåŒ–å™¨æ€§èƒ½æ¯”è¼ƒæ¸¬è©¦")
    print("=" * 60)

    # æ¸¬è©¦é…ç½®
    optimizers_config = [
        # æ¨™æº–é…ç½®ï¼ˆèˆŠç‰ˆæœ¬æ¨¡æ“¬ï¼‰
        {
            'name': 'æ¨™æº– C-Optim',
            'class': Automagic_CameAMP_COptim,
            'config': {
                'lr': 1e-3,
                'context_window': 50,
                'edge_threshold': 0.9,  # èˆŠçš„åš´æ ¼é–¾å€¼
                'adaptation_rate': 0.1,
                'verbose': False
            }
        },
        # LoRA å„ªåŒ–é…ç½®ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
        {
            'name': 'LoRA å„ªåŒ– C-Optim',
            'class': Automagic_CameAMP_COptim,
            'config': create_lora_optimizer_config('coptim')
        }
    ]

    # æ¸¬è©¦ 8-bit ç‰ˆæœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        model_test = MockStableDiffusionModel().to(device)
        opt_test = Automagic_CameAMP_COptim8bit(model_test.parameters(), lr=1e-3)
        del opt_test, model_test

        # 8-bit ç‰ˆæœ¬å¯ç”¨ï¼Œæ·»åŠ åˆ°æ¸¬è©¦ä¸­
        optimizers_config.append({
            'name': 'LoRA å„ªåŒ– 8bit',
            'class': Automagic_CameAMP_COptim8bit,
            'config': create_lora_optimizer_config('coptim')
        })
        print("âœ… 8-bit ç‰ˆæœ¬å¯ç”¨ï¼Œå°‡åŒ…å«åœ¨æ¸¬è©¦ä¸­")
    except Exception as e:
        print(f"âš ï¸  8-bit ç‰ˆæœ¬ä¸å¯ç”¨: {e}")

    results = {}

    for opt_config in optimizers_config:
        print(f"\nğŸ§ª æ¸¬è©¦ {opt_config['name']}")

        # å‰µå»ºæ–°çš„æ¨¡å‹å¯¦ä¾‹ï¼ˆç¢ºä¿å…¬å¹³æ¯”è¼ƒï¼‰
        model = MockStableDiffusionModel().to(device)

        # å‰µå»ºå„ªåŒ–å™¨
        optimizer = opt_config['class'](
            model.parameters(),
            **opt_config['config']
        )

        # é‹è¡Œæ¸¬è©¦
        result = test_lr_multiplier_evolution(optimizer, num_steps=150)
        results[opt_config['name']] = result

        # è¼¸å‡ºçµ±è¨ˆ
        avg_lr_mult = np.mean(result['lr_multipliers'])
        edge_rate = np.mean(result['edge_cases']) * 100
        final_loss = result['losses'][-1]

        print(f"  å¹³å‡ LR ä¹˜æ•¸: {avg_lr_mult:.3f}")
        print(f"  é‚Šç·£æƒ…æ³æ¯”ä¾‹: {edge_rate:.1f}%")
        print(f"  æœ€çµ‚æå¤±: {final_loss:.6f}")

    return results

def plot_comparison_results(results: Dict, save_dir: str = "docs/hina/plots"):
    """ç¹ªè£½æ¯”è¼ƒçµæœ"""

    if not results:
        print("âŒ æ²’æœ‰çµæœå¯ç¹ªè£½")
        return

    os.makedirs(save_dir, exist_ok=True)

    # è¨­å®šé¡è‰²
    colors = ['blue', 'red', 'green', 'orange']

    # 1. å­¸ç¿’ç‡ä¹˜æ•¸æ¯”è¼ƒ
    plt.figure(figsize=(15, 10))

    plt.subplot(2, 2, 1)
    for i, (name, result) in enumerate(results.items()):
        plt.plot(result['lr_multipliers'],
                label=name,
                color=colors[i % len(colors)],
                linewidth=2,
                alpha=0.8)

    plt.title('å­¸ç¿’ç‡ä¹˜æ•¸æ¼”åŒ–', fontsize=14, fontweight='bold')
    plt.xlabel('Step')
    plt.ylabel('LR Multiplier')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 2. æå¤±æ›²ç·š
    plt.subplot(2, 2, 2)
    for i, (name, result) in enumerate(results.items()):
        plt.plot(result['losses'],
                label=name,
                color=colors[i % len(colors)],
                linewidth=2,
                alpha=0.8)

    plt.title('æå¤±æ›²ç·š', fontsize=14, fontweight='bold')
    plt.xlabel('Step')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')

    # 3. é‚Šç·£æƒ…æ³æª¢æ¸¬
    plt.subplot(2, 2, 3)
    for i, (name, result) in enumerate(results.items()):
        edge_numeric = [1 if x else 0 for x in result['edge_cases']]
        plt.plot(edge_numeric,
                label=name,
                color=colors[i % len(colors)],
                linewidth=2,
                alpha=0.7)

    plt.title('é‚Šç·£æƒ…æ³æª¢æ¸¬', fontsize=14, fontweight='bold')
    plt.xlabel('Step')
    plt.ylabel('Edge Case (1=True, 0=False)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 4. æ¢¯åº¦ä¸€è‡´æ€§
    plt.subplot(2, 2, 4)
    for i, (name, result) in enumerate(results.items()):
        plt.plot(result['grad_consistencies'],
                label=name,
                color=colors[i % len(colors)],
                linewidth=2,
                alpha=0.8)

    plt.title('æ¢¯åº¦ä¸€è‡´æ€§', fontsize=14, fontweight='bold')
    plt.xlabel('Step')
    plt.ylabel('Gradient Consistency')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()

    # ä¿å­˜åœ–è¡¨
    save_path = f"{save_dir}/lora_optimization_comparison.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š æ¯”è¼ƒåœ–è¡¨å·²ä¿å­˜: {save_path}")
    plt.show()

def print_recommendations():
    """æ‰“å° LoRA è¨“ç·´çš„å»ºè­°é…ç½®"""

    print("\n" + "=" * 80)
    print("ğŸ’¡ LoRA è¨“ç·´çš„ Automagic_CameAMP æ¨è–¦é…ç½®")
    print("=" * 80)

    print("\nğŸ¯ é‡å° Stable Diffusion LoRA çš„å„ªåŒ–å»ºè­°:")

    print("\n1. ä½¿ç”¨ LoRA å„ªåŒ–ç‰ˆæœ¬çš„ C-Optim é…ç½®:")
    print("```python")
    print("from library.automagic_cameamp import Automagic_CameAMP_COptim")
    print("")
    print("optimizer = Automagic_CameAMP_COptim(")
    print("    model.parameters(),")
    print("    lr=1e-3,                    # æé«˜åŸºç¤å­¸ç¿’ç‡")
    print("    weight_decay=1e-4,          # é©ä¸­çš„æ­£å‰‡åŒ–")
    print("    warmup_steps=300,           # è¼ƒçŸ­æš–èº«æœŸ")
    print("    context_window=30,          # æ¸›å°çª—å£ï¼Œæé«˜éˆæ•åº¦")
    print("    edge_threshold=0.6,         # é™ä½é–¾å€¼ï¼Œæ¸›å°‘é‚Šç·£è§¸ç™¼")
    print("    adaptation_rate=0.25,       # æé«˜é©æ‡‰é€Ÿç‡")
    print("    full_finetune=False,        # å•Ÿç”¨ ALLoRA")
    print("    verbose=True")
    print(")")
    print("```")

    print("\n2. å¦‚æœè¨˜æ†¶é«”å—é™ï¼Œä½¿ç”¨ 8-bit ç‰ˆæœ¬:")
    print("```python")
    print("from library.automagic_cameamp import Automagic_CameAMP_COptim8bit")
    print("")
    print("optimizer = Automagic_CameAMP_COptim8bit(")
    print("    model.parameters(),")
    print("    lr=1e-3,")
    print("    context_window=25,          # 8-bit ç‰ˆæœ¬å»ºè­°æ›´å°çª—å£")
    print("    edge_threshold=0.5,         # æ›´å¯¬å®¹çš„é‚Šç·£æª¢æ¸¬")
    print("    adaptation_rate=0.3,        # ç•¥é«˜çš„é©æ‡‰é€Ÿç‡")
    print("    full_finetune=False")
    print(")")
    print("```")

    print("\n3. ç›£æ§å­¸ç¿’ç‡ä¹˜æ•¸:")
    print("```python")
    print("# åœ¨è¨“ç·´å¾ªç’°ä¸­æ·»åŠ ç›£æ§")
    print("if step % 100 == 0:")
    print("    lr_mult = optimizer.c_optim.compute_contextual_lr_multiplier()")
    print("    is_edge = optimizer.c_optim.detect_edge_case()")
    print("    print(f'Step {step}: LRå€æ•¸={lr_mult:.3f}, é‚Šç·£={is_edge}')")
    print("```")

    print("\n4. é—œéµæ”¹é€²é»:")
    print("   âœ… æé«˜äº†å­¸ç¿’ç‡ä¹˜æ•¸çš„åŸºæº–å€¼ï¼ˆ1.1-4.0 vs 0.5-3.0ï¼‰")
    print("   âœ… æ”¾å¯¬äº†é‚Šç·£æƒ…æ³æª¢æ¸¬æ¢ä»¶ï¼ˆè®Šç•°ä¿‚æ•¸ 0.5 vs 0.3ï¼‰")
    print("   âœ… æ¸›å°‘äº†é‚Šç·£æƒ…æ³çš„å­¸ç¿’ç‡æ‡²ç½°ï¼ˆ0.7-0.95 vs 0.4-0.8ï¼‰")
    print("   âœ… å„ªåŒ–äº†åœæ»¯æª¢æ¸¬é–¾å€¼ï¼ˆ30 æ­¥ vs 20 æ­¥ï¼‰")
    print("   âœ… å¢å¼·äº†æ­£å‘æ”¶æ–‚çš„çå‹µæ©Ÿåˆ¶")

    print("\n5. é æœŸæ•ˆæœ:")
    print("   ğŸ“ˆ å­¸ç¿’ç‡ä¹˜æ•¸å¹³å‡æé«˜ 50-80%")
    print("   ğŸ“‰ é‚Šç·£æƒ…æ³è§¸ç™¼é »ç‡é™ä½ 30-50%")
    print("   ğŸš€ LoRA è¨“ç·´æ•ˆæœæ˜é¡¯æ”¹å–„")
    print("   ğŸ’¾ 8-bit ç‰ˆæœ¬ç¯€çœ ~75% è¨˜æ†¶é«”")

def main():
    """ä¸»å‡½æ•¸"""

    print("ğŸ¨ LoRA å„ªåŒ–ç‰ˆ Automagic_CameAMP æ¸¬è©¦")
    print("è§£æ±ºå­¸ç¿’ç‡ä¹˜æ•¸éå°å•é¡Œ")
    print("=" * 60)

    try:
        # é‹è¡Œæ¯”è¼ƒæ¸¬è©¦
        results = compare_optimizers()

        # ç¹ªè£½çµæœ
        plot_comparison_results(results)

        # é¡¯ç¤ºè©³ç´°çµ±è¨ˆ
        print("\nğŸ“Š è©³ç´°çµ±è¨ˆæ¯”è¼ƒ:")
        print("-" * 60)

        for name, result in results.items():
            avg_lr_mult = np.mean(result['lr_multipliers'])
            max_lr_mult = np.max(result['lr_multipliers'])
            min_lr_mult = np.min(result['lr_multipliers'])
            edge_rate = np.mean(result['edge_cases']) * 100

            print(f"\n{name}:")
            print(f"  å¹³å‡ LR ä¹˜æ•¸: {avg_lr_mult:.3f}")
            print(f"  LR ä¹˜æ•¸ç¯„åœ: {min_lr_mult:.3f} - {max_lr_mult:.3f}")
            print(f"  é‚Šç·£æƒ…æ³æ¯”ä¾‹: {edge_rate:.1f}%")

        # é¡¯ç¤ºå»ºè­°
        print_recommendations()

        print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼")
        print("ç¾åœ¨æ‚¨å¯ä»¥ä½¿ç”¨å„ªåŒ–å¾Œçš„é…ç½®é€²è¡Œ LoRA è¨“ç·´äº†ï¼")

    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()