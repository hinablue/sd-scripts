#!/usr/bin/env python3
"""
å‹•æ…‹è‡ªé©æ‡‰åŠŸèƒ½æ¸¬è©¦è…³æœ¬

æ¸¬è©¦ HinaAdaptive å„ªåŒ–å™¨çš„ dynamic_adaptation åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ

æ³¨æ„ï¼šå‚…ç«‹è‘‰ç‰¹å¾µæå¤±åŠŸèƒ½å·²è¢«ç§»é™¤ï¼Œå› ç‚ºå®ƒä¸é©ç”¨æ–¼ SD-Scripts
çš„ latent space è¨“ç·´ç’°å¢ƒã€‚
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ åº«è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

from library.hina_adaptive import HinaAdaptive


class SimpleTestModel(nn.Module):
    """ç°¡å–®çš„æ¸¬è©¦æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.fc = nn.Linear(32 * 32 * 32, 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = x.view(x.size(0), -1)
        return self.fc(x)


def test_dynamic_adaptation():
    """æ¸¬è©¦å‹•æ…‹è‡ªé©æ‡‰åŠŸèƒ½"""
    print("=== å‹•æ…‹è‡ªé©æ‡‰åŠŸèƒ½æ¸¬è©¦ ===")

    # å‰µå»ºæ¨¡å‹å’Œæ•¸æ“š
    model = SimpleTestModel()
    input_data = torch.randn(2, 3, 32, 32)
    target = torch.randint(0, 10, (2,))

    # æ¸¬è©¦1ï¼šå•Ÿç”¨å‹•æ…‹è‡ªé©æ‡‰
    print("\n1. æ¸¬è©¦å•Ÿç”¨å‹•æ…‹è‡ªé©æ‡‰çš„å„ªåŒ–å™¨")
    optimizer_adaptive = HinaAdaptive(
        model.parameters(),
        lr=0.001,
        use_dynamic_adaptation=True,  # å•Ÿç”¨å‹•æ…‹è‡ªé©æ‡‰
        adaptation_strength=1.0,
        relationship_discovery_interval=100,
        importance_decay=0.95,
        compatibility_threshold=0.3,
        memory_efficient=True
    )

    print(f"å„ªåŒ–å™¨å‰µå»ºæˆåŠŸï¼Œuse_dynamic_adaptation = {optimizer_adaptive.use_dynamic_adaptation}")

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info = optimizer_adaptive.get_optimization_info()
    print(f"å‹•æ…‹è‡ªé©æ‡‰å·²å•Ÿç”¨ï¼š{info['features']['dynamic_adaptation']}")
    print(f"è‡ªé©æ‡‰å¼·åº¦ï¼š{info['adaptation_config']['adaptation_strength']}")
    print(f"é—œä¿‚ç™¼ç¾é–“éš”ï¼š{info['adaptation_config']['relationship_discovery_interval']}")
    print(f"é‡è¦æ€§è¡°æ¸›ï¼š{info['adaptation_config']['importance_decay']}")
    print(f"ç›¸å®¹æ€§é–¾å€¼ï¼š{info['adaptation_config']['compatibility_threshold']}")

    # æ¸¬è©¦2ï¼šæ¸¬è©¦é—œé–‰å‹•æ…‹è‡ªé©æ‡‰
    print("\n2. æ¸¬è©¦é—œé–‰å‹•æ…‹è‡ªé©æ‡‰çš„å„ªåŒ–å™¨")
    optimizer_static = HinaAdaptive(
        model.parameters(),
        lr=0.001,
        use_dynamic_adaptation=False,  # é—œé–‰å‹•æ…‹è‡ªé©æ‡‰
        memory_efficient=True
    )

    print(f"å„ªåŒ–å™¨å‰µå»ºæˆåŠŸï¼Œuse_dynamic_adaptation = {optimizer_static.use_dynamic_adaptation}")

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info_static = optimizer_static.get_optimization_info()
    print(f"å‹•æ…‹è‡ªé©æ‡‰å·²é—œé–‰ï¼š{info_static['features']['dynamic_adaptation']}")

    return True


def test_lr_mask_functionality():
    """æ¸¬è©¦å­¸ç¿’ç‡é®ç½©åŠŸèƒ½"""
    print("\n=== å­¸ç¿’ç‡é®ç½©åŠŸèƒ½æ¸¬è©¦ ===")

    model = SimpleTestModel()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # å‰µå»ºå„ªåŒ–å™¨ï¼Œå•Ÿç”¨å­¸ç¿’ç‡é®ç½©
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=0.001,
        use_lr_mask=True,  # å•Ÿç”¨å­¸ç¿’ç‡é®ç½©
        lr_bump=3e-6,
        min_lr=1e-7,
        max_lr=1e-3,
        warmup_steps=500,
        memory_efficient=True
    )

    print(f"å„ªåŒ–å™¨å‰µå»ºæˆåŠŸï¼Œuse_lr_mask = {optimizer.use_lr_mask}")

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info = optimizer.get_optimization_info()
    print(f"å­¸ç¿’ç‡é®ç½©å·²å•Ÿç”¨ï¼š{info['features']['lr_mask']}")

    # æ¸¬è©¦å¹¾æ­¥è¨“ç·´
    input_data = torch.randn(2, 3, 32, 32, device=device)
    target = torch.randint(0, 10, (2,), device=device)

    print("\né€²è¡Œå¹¾æ­¥è¨“ç·´æ¸¬è©¦...")
    for step in range(5):
        optimizer.zero_grad()
        output = model(input_data)
        loss = nn.functional.cross_entropy(output, target)
        loss.backward()
        optimizer.step()

        print(f"æ­¥é©Ÿ {step+1}: æå¤± = {loss.item():.4f}")

    print("âœ… å­¸ç¿’ç‡é®ç½©åŠŸèƒ½æ¸¬è©¦å®Œæˆ")
    return True


def test_edge_suppression_functionality():
    """æ¸¬è©¦é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–åŠŸèƒ½"""
    print("\n=== é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–åŠŸèƒ½æ¸¬è©¦ ===")

    model = SimpleTestModel()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # å‰µå»ºå„ªåŒ–å™¨ï¼Œå•Ÿç”¨é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=0.001,
        edge_suppression=True,  # å•Ÿç”¨é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–
        edge_penalty=0.1,
        edge_threshold=0.6,
        memory_efficient=True
    )

    print(f"å„ªåŒ–å™¨å‰µå»ºæˆåŠŸï¼Œedge_suppression = {optimizer.edge_suppression}")

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info = optimizer.get_optimization_info()
    print(f"é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–å·²å•Ÿç”¨ï¼š{info['features']['edge_suppression']}")
    print(f"é‚Šç·£æ‡²ç½°ï¼š{info['edge_overfitting_control']['edge_penalty']}")
    print(f"é‚Šç·£é–¾å€¼ï¼š{info['edge_overfitting_control']['edge_threshold']}")

    # æ¸¬è©¦å¹¾æ­¥è¨“ç·´
    input_data = torch.randn(2, 3, 32, 32, device=device)
    target = torch.randint(0, 10, (2,), device=device)

    print("\né€²è¡Œå¹¾æ­¥è¨“ç·´æ¸¬è©¦...")
    for step in range(5):
        optimizer.zero_grad()
        output = model(input_data)
        loss = nn.functional.cross_entropy(output, target)
        loss.backward()
        optimizer.step()

        print(f"æ­¥é©Ÿ {step+1}: æå¤± = {loss.item():.4f}")

    print("âœ… é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–åŠŸèƒ½æ¸¬è©¦å®Œæˆ")
    return True


def test_memory_optimization_functionality():
    """æ¸¬è©¦è¨˜æ†¶é«”å„ªåŒ–åŠŸèƒ½"""
    print("\n=== è¨˜æ†¶é«”å„ªåŒ–åŠŸèƒ½æ¸¬è©¦ ===")

    model = SimpleTestModel()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # å‰µå»ºå„ªåŒ–å™¨ï¼Œå•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=0.001,
        memory_efficient=True,  # å•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–
        vram_budget_gb=8.0,
        cpu_offload_states=True,
        reduce_precision=True,
        adaptive_features=True,
        max_buffer_memory_mb=500
    )

    print(f"å„ªåŒ–å™¨å‰µå»ºæˆåŠŸï¼Œmemory_efficient = {optimizer.memory_efficient}")

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info = optimizer.get_optimization_info()
    print(f"è¨˜æ†¶é«”å„ªåŒ–å·²å•Ÿç”¨ï¼š{info['memory_optimization']['memory_efficient']}")
    print(f"VRAM é ç®—ï¼š{info['memory_optimization']['vram_budget_gb']}GB")
    print(f"CPU ç‹€æ…‹å¸è¼‰ï¼š{info['memory_optimization']['cpu_offload_states']}")
    print(f"ç²¾åº¦é™ä½ï¼š{info['memory_optimization']['reduce_precision']}")

    # æ¸¬è©¦è¨˜æ†¶é«”çµ±è¨ˆ
    memory_stats = optimizer.get_memory_stats()
    print(f"\nè¨˜æ†¶é«”çµ±è¨ˆï¼š")
    print(f"è¨˜æ†¶é«”å£“åŠ›ï¼š{memory_stats['memory_pressure']:.2%}")
    print(f"ç·©è¡æ± è¨˜æ†¶é«”ï¼š{memory_stats['buffer_pool_stats']['current_memory_mb']:.2f}MB")

    # æ¸¬è©¦è¨˜æ†¶é«”å„ªåŒ–
    optimizer.optimize_for_vram(target_vram_gb=6.0)
    print("âœ… è¨˜æ†¶é«”å„ªåŒ–åŠŸèƒ½æ¸¬è©¦å®Œæˆ")
    return True


def test_regularization_combination():
    """æ¸¬è©¦æ­£å‰‡åŒ–æŠ€è¡“çµ„åˆ"""
    print("\n=== æ­£å‰‡åŒ–æŠ€è¡“çµ„åˆæ¸¬è©¦ ===")

    model = SimpleTestModel()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # å‰µå»ºå„ªåŒ–å™¨ï¼Œçµ„åˆå¤šç¨®æ­£å‰‡åŒ–æŠ€è¡“
    optimizer = HinaAdaptive(
        model.parameters(),
        lr=0.001,
        # çµ„åˆå¤šç¨®æ­£å‰‡åŒ–æŠ€è¡“
        edge_suppression=True,
        edge_penalty=0.1,
        spatial_awareness=True,
        frequency_penalty=0.05,
        background_regularization=True,
        lora_rank_penalty=True,
        rank_penalty_strength=0.01,
        # å‹•æ…‹è‡ªé©æ‡‰
        use_dynamic_adaptation=True,
        # è¨˜æ†¶é«”å„ªåŒ–
        memory_efficient=True,
        vram_budget_gb=8.0
    )

    print("å„ªåŒ–å™¨å‰µå»ºæˆåŠŸï¼Œçµ„åˆå¤šç¨®æ­£å‰‡åŒ–æŠ€è¡“")

    # ç²å–å„ªåŒ–å™¨ä¿¡æ¯
    info = optimizer.get_optimization_info()
    print(f"å•Ÿç”¨çš„æ­£å‰‡åŒ–æŠ€è¡“ï¼š")
    for feature, enabled in info['features'].items():
        if enabled:
            print(f"  âœ… {feature}")

    # æ¸¬è©¦å¹¾æ­¥è¨“ç·´
    input_data = torch.randn(2, 3, 32, 32, device=device)
    target = torch.randint(0, 10, (2,), device=device)

    print("\né€²è¡Œå¹¾æ­¥è¨“ç·´æ¸¬è©¦...")
    for step in range(5):
        optimizer.zero_grad()
        output = model(input_data)
        loss = nn.functional.cross_entropy(output, target)
        loss.backward()
        optimizer.step()

        print(f"æ­¥é©Ÿ {step+1}: æå¤± = {loss.item():.4f}")

    print("âœ… æ­£å‰‡åŒ–æŠ€è¡“çµ„åˆæ¸¬è©¦å®Œæˆ")
    return True


def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸ§ª HinaAdaptive å‹•æ…‹è‡ªé©æ‡‰åŠŸèƒ½æ¸¬è©¦")
    print("=" * 50)

    tests = [
        ("å‹•æ…‹è‡ªé©æ‡‰åŠŸèƒ½", test_dynamic_adaptation),
        ("å­¸ç¿’ç‡é®ç½©åŠŸèƒ½", test_lr_mask_functionality),
        ("é‚Šç·£æ„ŸçŸ¥æ­£å‰‡åŒ–åŠŸèƒ½", test_edge_suppression_functionality),
        ("è¨˜æ†¶é«”å„ªåŒ–åŠŸèƒ½", test_memory_optimization_functionality),
        ("æ­£å‰‡åŒ–æŠ€è¡“çµ„åˆ", test_regularization_combination),
    ]

    passed = 0
    total = len(tests)

    for test_name, test_func in tests:
        print(f"\nğŸ” åŸ·è¡Œ {test_name}...")
        try:
            if test_func():
                print(f"âœ… {test_name} é€šé")
                passed += 1
            else:
                print(f"âŒ {test_name} å¤±æ•—")
        except Exception as e:
            print(f"âŒ {test_name} ç•°å¸¸: {e}")

    # ç¸½çµçµæœ
    print("\n" + "=" * 50)
    print(f"ğŸ“Š æ¸¬è©¦çµæœ: {passed}/{total} é€šé")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼")
    else:
        print(f"âš ï¸  {total - passed} å€‹æ¸¬è©¦å¤±æ•—")

    return passed == total


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)