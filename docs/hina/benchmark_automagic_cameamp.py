#!/usr/bin/env python3
"""
Automagic_CameAMP å„ªåŒ–å™¨æ€§èƒ½åŸºæº–æ¸¬è©¦

é€™å€‹æ¸¬è©¦è…³æœ¬æ¯”è¼ƒ Automagic_CameAMP èˆ‡å…¶ä»–å¸¸è¦‹å„ªåŒ–å™¨çš„æ€§èƒ½ã€‚

ä½œè€…: Hina
ç‰ˆæœ¬: 1.0
æ—¥æœŸ: 2025-01-27
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as torch_optim
import time
import numpy as np
import matplotlib.pyplot as plt
import psutil
import gc
import os
import sys
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

# åŠ å…¥åº«è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from library.automagic_cameamp import (
        Automagic_CameAMP,
        Automagic_CameAMP8bit,
        Automagic_CameAMP_COptim,
        Automagic_CameAMP_COptim8bit
    )
    AUTOMAGIC_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç„¡æ³•å°å…¥ Automagic å„ªåŒ–å™¨: {e}")
    AUTOMAGIC_AVAILABLE = False

# è¨­å®šè¨­å‚™
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")

@dataclass
class BenchmarkConfig:
    """åŸºæº–æ¸¬è©¦é…ç½®"""
    model_sizes: List[str] = None
    batch_size: int = 64
    input_size: int = 784
    num_classes: int = 10
    num_epochs: int = 50
    num_warmup: int = 5
    measure_memory: bool = True
    save_plots: bool = True
    verbose: bool = True

    def __post_init__(self):
        if self.model_sizes is None:
            self.model_sizes = ['small', 'medium', 'large']

class BenchmarkModel(nn.Module):
    """å¯é…ç½®å¤§å°çš„æ¸¬è©¦æ¨¡å‹"""

    def __init__(self, input_size: int, num_classes: int, size: str = 'medium'):
        super().__init__()

        if size == 'small':
            hidden_sizes = [128, 64]
        elif size == 'medium':
            hidden_sizes = [512, 256, 128]
        elif size == 'large':
            hidden_sizes = [1024, 512, 256, 128]
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹å¤§å°: {size}")

        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.BatchNorm1d(hidden_size)
            ])
            prev_size = hidden_size

        layers.append(nn.Linear(prev_size, num_classes))
        self.network = nn.Sequential(*layers)

        # è¨ˆç®—åƒæ•¸æ•¸é‡
        self.param_count = sum(p.numel() for p in self.parameters())

    def forward(self, x):
        return self.network(x)

class MemoryTracker:
    """è¨˜æ†¶é«”ä½¿ç”¨è¿½è¹¤å™¨"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.peak_memory = 0
        self.current_memory = 0

    def update(self):
        if device.type == 'cuda':
            self.current_memory = torch.cuda.memory_allocated() / 1024**2  # MB
            self.peak_memory = max(self.peak_memory, self.current_memory)
        else:
            process = psutil.Process()
            self.current_memory = process.memory_info().rss / 1024**2  # MB
            self.peak_memory = max(self.peak_memory, self.current_memory)

    def get_memory_mb(self) -> float:
        return self.current_memory

    def get_peak_memory_mb(self) -> float:
        return self.peak_memory

class OptimizersComparison:
    """å„ªåŒ–å™¨æ¯”è¼ƒæ¸¬è©¦"""

    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.results = {}

    def create_optimizers(self, model_params) -> Dict[str, torch.optim.Optimizer]:
        """å‰µå»ºæ‰€æœ‰è¦æ¸¬è©¦çš„å„ªåŒ–å™¨"""
        optimizers = {}

        # PyTorch å…§å»ºå„ªåŒ–å™¨
        optimizers['Adam'] = torch_optim.Adam(model_params, lr=1e-3, weight_decay=1e-4)
        optimizers['AdamW'] = torch_optim.AdamW(model_params, lr=1e-3, weight_decay=1e-4)
        optimizers['SGD'] = torch_optim.SGD(model_params, lr=1e-3, momentum=0.9, weight_decay=1e-4)
        optimizers['RMSprop'] = torch_optim.RMSprop(model_params, lr=1e-3, weight_decay=1e-4)

        # Automagic å„ªåŒ–å™¨ (å¦‚æœå¯ç”¨)
        if AUTOMAGIC_AVAILABLE:
            try:
                optimizers['Automagic_CameAMP'] = Automagic_CameAMP(
                    model_params, lr=1e-3, weight_decay=1e-4, warmup_steps=50
                )
            except Exception as e:
                print(f"âš ï¸  ç„¡æ³•å‰µå»º Automagic_CameAMP: {e}")

            try:
                optimizers['Automagic_CameAMP_COptim'] = Automagic_CameAMP_COptim(
                    model_params, lr=1e-3, weight_decay=1e-4, warmup_steps=50,
                    context_window=30, edge_threshold=0.8
                )
            except Exception as e:
                print(f"âš ï¸  ç„¡æ³•å‰µå»º Automagic_CameAMP_COptim: {e}")

            # 8-bit ç‰ˆæœ¬ (å¦‚æœæ”¯æ´)
            try:
                optimizers['Automagic_CameAMP8bit'] = Automagic_CameAMP8bit(
                    model_params, lr=1e-3, weight_decay=1e-4, warmup_steps=50
                )
            except Exception as e:
                print(f"âš ï¸  ç„¡æ³•å‰µå»º 8-bit ç‰ˆæœ¬: {e}")

        return optimizers

    def generate_data(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆæ¸¬è©¦æ•¸æ“š"""
        x = torch.randn(self.config.batch_size, self.config.input_size, device=device)
        y = torch.randint(0, self.config.num_classes, (self.config.batch_size,), device=device)
        return x, y

    def benchmark_optimizer(self,
                          optimizer_name: str,
                          optimizer: torch.optim.Optimizer,
                          model: nn.Module) -> Dict[str, float]:
        """æ¸¬è©¦å–®å€‹å„ªåŒ–å™¨"""
        print(f"  ğŸ§ª æ¸¬è©¦ {optimizer_name}")

        model.train()
        memory_tracker = MemoryTracker()

        # è¨˜éŒ„æŒ‡æ¨™
        losses = []
        times = []
        memory_usage = []

        # æ¸…ç†è¨˜æ†¶é«”
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        gc.collect()

        start_time = time.time()
        memory_tracker.update()
        initial_memory = memory_tracker.get_memory_mb()

        try:
            for epoch in range(self.config.num_epochs):
                epoch_start = time.time()
                epoch_losses = []

                for batch_idx in range(20):  # æ¯å€‹ epoch 20 å€‹æ‰¹æ¬¡
                    x, y = self.generate_data()

                    optimizer.zero_grad()
                    output = model(x)
                    loss = F.cross_entropy(output, y)
                    loss.backward()
                    optimizer.step()

                    epoch_losses.append(loss.item())

                    # è¨˜éŒ„è¨˜æ†¶é«”ä½¿ç”¨
                    if self.config.measure_memory and batch_idx == 0:
                        memory_tracker.update()
                        memory_usage.append(memory_tracker.get_memory_mb())

                epoch_time = time.time() - epoch_start
                avg_loss = np.mean(epoch_losses)

                losses.append(avg_loss)
                times.append(epoch_time)

                # æš–èº«å¾Œæ‰é–‹å§‹è¨˜éŒ„
                if epoch >= self.config.num_warmup and self.config.verbose:
                    if epoch % 10 == 0:
                        print(f"    Epoch {epoch}: Loss = {avg_loss:.6f}, Time = {epoch_time:.3f}s")

        except Exception as e:
            print(f"    âŒ {optimizer_name} è¨“ç·´å¤±æ•—: {e}")
            return None

        total_time = time.time() - start_time
        memory_tracker.update()
        peak_memory = memory_tracker.get_peak_memory_mb()
        memory_overhead = peak_memory - initial_memory

        # è¨ˆç®—æŒ‡æ¨™ (è·³éæš–èº«)
        valid_losses = losses[self.config.num_warmup:]
        valid_times = times[self.config.num_warmup:]

        results = {
            'final_loss': valid_losses[-1] if valid_losses else float('inf'),
            'best_loss': min(valid_losses) if valid_losses else float('inf'),
            'avg_time_per_epoch': np.mean(valid_times) if valid_times else float('inf'),
            'total_time': total_time,
            'memory_overhead_mb': memory_overhead,
            'peak_memory_mb': peak_memory,
            'convergence_rate': len(valid_losses) - np.argmin(valid_losses) if valid_losses else 0,
            'losses': losses,
            'times': times,
            'memory_usage': memory_usage
        }

        print(f"    âœ… å®Œæˆ - æœ€çµ‚æå¤±: {results['final_loss']:.6f}, "
              f"å¹³å‡æ™‚é–“: {results['avg_time_per_epoch']:.3f}s, "
              f"è¨˜æ†¶é«”: {results['memory_overhead_mb']:.1f}MB")

        return results

    def run_benchmark(self, model_size: str) -> Dict[str, Dict]:
        """é‹è¡Œç‰¹å®šæ¨¡å‹å¤§å°çš„åŸºæº–æ¸¬è©¦"""
        print(f"\nğŸƒ é‹è¡Œ {model_size} æ¨¡å‹åŸºæº–æ¸¬è©¦")
        print("=" * 50)

        # å‰µå»ºæ¨¡å‹
        model = BenchmarkModel(
            self.config.input_size,
            self.config.num_classes,
            model_size
        ).to(device)

        print(f"ğŸ“Š æ¨¡å‹åƒæ•¸: {model.param_count:,}")

        # å‰µå»ºå„ªåŒ–å™¨
        optimizers = self.create_optimizers(model.parameters())
        print(f"ğŸ”§ æ¸¬è©¦ {len(optimizers)} å€‹å„ªåŒ–å™¨")

        results = {}

        for opt_name, optimizer in optimizers.items():
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹æ¬Šé‡ä»¥ç¢ºä¿å…¬å¹³æ¯”è¼ƒ
            model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)

            result = self.benchmark_optimizer(opt_name, optimizer, model)
            if result is not None:
                results[opt_name] = result

        return results

    def run_all_benchmarks(self) -> Dict[str, Dict]:
        """é‹è¡Œæ‰€æœ‰åŸºæº–æ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹ Automagic_CameAMP æ€§èƒ½åŸºæº–æ¸¬è©¦")
        print("ä½œè€…: Hina")
        print("ç‰ˆæœ¬: 1.0")
        print("=" * 60)

        all_results = {}

        for model_size in self.config.model_sizes:
            results = self.run_benchmark(model_size)
            if results:
                all_results[model_size] = results

        self.results = all_results
        return all_results

    def plot_results(self, save_dir: str = "docs/hina/plots"):
        """ç¹ªè£½åŸºæº–æ¸¬è©¦çµæœ"""
        if not self.results:
            print("âŒ æ²’æœ‰çµæœå¯ç¹ªè£½")
            return

        os.makedirs(save_dir, exist_ok=True)

        # é¡è‰²æ˜ å°„
        color_map = {
            'Adam': 'blue',
            'AdamW': 'green',
            'SGD': 'red',
            'RMSprop': 'orange',
            'Automagic_CameAMP': 'purple',
            'Automagic_CameAMP_COptim': 'brown',
            'Automagic_CameAMP8bit': 'pink'
        }

        # 1. æœ€çµ‚æå¤±æ¯”è¼ƒ
        fig, axes = plt.subplots(1, len(self.config.model_sizes), figsize=(15, 5))
        if len(self.config.model_sizes) == 1:
            axes = [axes]

        for i, model_size in enumerate(self.config.model_sizes):
            if model_size not in self.results:
                continue

            results = self.results[model_size]
            names = list(results.keys())
            final_losses = [results[name]['final_loss'] for name in names]
            colors = [color_map.get(name, 'gray') for name in names]

            bars = axes[i].bar(names, final_losses, color=colors, alpha=0.7)
            axes[i].set_title(f'{model_size.title()} Model - Final Loss')
            axes[i].set_ylabel('Loss')
            axes[i].tick_params(axis='x', rotation=45)
            axes[i].set_yscale('log')

            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for bar, loss in zip(bars, final_losses):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height,
                           f'{loss:.4f}', ha='center', va='bottom', fontsize=8)

        plt.tight_layout()
        if self.config.save_plots:
            plt.savefig(f"{save_dir}/benchmark_final_loss.png", dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æœ€çµ‚æå¤±åœ–å·²ä¿å­˜: {save_dir}/benchmark_final_loss.png")
        plt.show()

        # 2. è¨“ç·´æ™‚é–“æ¯”è¼ƒ
        fig, axes = plt.subplots(1, len(self.config.model_sizes), figsize=(15, 5))
        if len(self.config.model_sizes) == 1:
            axes = [axes]

        for i, model_size in enumerate(self.config.model_sizes):
            if model_size not in self.results:
                continue

            results = self.results[model_size]
            names = list(results.keys())
            avg_times = [results[name]['avg_time_per_epoch'] for name in names]
            colors = [color_map.get(name, 'gray') for name in names]

            bars = axes[i].bar(names, avg_times, color=colors, alpha=0.7)
            axes[i].set_title(f'{model_size.title()} Model - Training Speed')
            axes[i].set_ylabel('Time per Epoch (s)')
            axes[i].tick_params(axis='x', rotation=45)

            # æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for bar, time_val in zip(bars, avg_times):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height,
                           f'{time_val:.3f}s', ha='center', va='bottom', fontsize=8)

        plt.tight_layout()
        if self.config.save_plots:
            plt.savefig(f"{save_dir}/benchmark_training_time.png", dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š è¨“ç·´æ™‚é–“åœ–å·²ä¿å­˜: {save_dir}/benchmark_training_time.png")
        plt.show()

        # 3. è¨˜æ†¶é«”ä½¿ç”¨æ¯”è¼ƒ
        if self.config.measure_memory:
            fig, axes = plt.subplots(1, len(self.config.model_sizes), figsize=(15, 5))
            if len(self.config.model_sizes) == 1:
                axes = [axes]

            for i, model_size in enumerate(self.config.model_sizes):
                if model_size not in self.results:
                    continue

                results = self.results[model_size]
                names = list(results.keys())
                memory_usage = [results[name]['memory_overhead_mb'] for name in names]
                colors = [color_map.get(name, 'gray') for name in names]

                bars = axes[i].bar(names, memory_usage, color=colors, alpha=0.7)
                axes[i].set_title(f'{model_size.title()} Model - Memory Usage')
                axes[i].set_ylabel('Memory Overhead (MB)')
                axes[i].tick_params(axis='x', rotation=45)

                # æ·»åŠ æ•¸å€¼æ¨™ç±¤
                for bar, memory in zip(bars, memory_usage):
                    height = bar.get_height()
                    axes[i].text(bar.get_x() + bar.get_width()/2., height,
                               f'{memory:.1f}MB', ha='center', va='bottom', fontsize=8)

            plt.tight_layout()
            if self.config.save_plots:
                plt.savefig(f"{save_dir}/benchmark_memory_usage.png", dpi=300, bbox_inches='tight')
                print(f"ğŸ“Š è¨˜æ†¶é«”ä½¿ç”¨åœ–å·²ä¿å­˜: {save_dir}/benchmark_memory_usage.png")
            plt.show()

        # 4. æå¤±æ›²ç·š (é¸æ“‡ä¸€å€‹æ¨¡å‹å¤§å°)
        if self.config.model_sizes:
            model_size = self.config.model_sizes[0]  # ä½¿ç”¨ç¬¬ä¸€å€‹æ¨¡å‹å¤§å°
            if model_size in self.results:
                plt.figure(figsize=(12, 8))

                for name, result in self.results[model_size].items():
                    if 'losses' in result:
                        losses = result['losses'][self.config.num_warmup:]  # è·³éæš–èº«
                        color = color_map.get(name, 'gray')
                        plt.plot(losses, label=name, color=color, linewidth=2, alpha=0.8)

                plt.title(f'{model_size.title()} Model - Loss Curves', fontsize=16, fontweight='bold')
                plt.xlabel('Epoch', fontsize=12)
                plt.ylabel('Loss', fontsize=12)
                plt.legend()
                plt.grid(True, alpha=0.3)
                plt.yscale('log')

                if self.config.save_plots:
                    plt.savefig(f"{save_dir}/benchmark_loss_curves.png", dpi=300, bbox_inches='tight')
                    print(f"ğŸ“Š æå¤±æ›²ç·šåœ–å·²ä¿å­˜: {save_dir}/benchmark_loss_curves.png")
                plt.show()

    def print_summary(self):
        """æ‰“å°åŸºæº–æ¸¬è©¦ç¸½çµ"""
        if not self.results:
            print("âŒ æ²’æœ‰åŸºæº–æ¸¬è©¦çµæœ")
            return

        print("\n" + "=" * 80)
        print("ğŸ“‹ Automagic_CameAMP åŸºæº–æ¸¬è©¦ç¸½çµ")
        print("=" * 80)

        for model_size, results in self.results.items():
            print(f"\nğŸ” {model_size.upper()} æ¨¡å‹çµæœ:")
            print("-" * 40)

            # å‰µå»ºçµæœè¡¨æ ¼
            headers = ["å„ªåŒ–å™¨", "æœ€çµ‚æå¤±", "æœ€ä½³æå¤±", "å¹³å‡æ™‚é–“(s)", "è¨˜æ†¶é«”(MB)"]

            # æ”¶é›†æ•¸æ“š
            data = []
            for name, result in results.items():
                data.append([
                    name,
                    f"{result['final_loss']:.6f}",
                    f"{result['best_loss']:.6f}",
                    f"{result['avg_time_per_epoch']:.3f}",
                    f"{result['memory_overhead_mb']:.1f}"
                ])

            # æ‰“å°è¡¨æ ¼
            col_widths = [max(len(str(row[i])) for row in [headers] + data) + 2
                         for i in range(len(headers))]

            def print_row(row):
                print("|" + "|".join(f" {str(item).ljust(col_widths[i])} "
                                     for i, item in enumerate(row)) + "|")

            print_row(headers)
            print("|" + "|".join("-" * (w + 2) for w in col_widths) + "|")
            for row in data:
                print_row(row)

            # åˆ†ææœ€ä½³è¡¨ç¾
            if results:
                best_loss = min(results.items(), key=lambda x: x[1]['final_loss'])
                fastest = min(results.items(), key=lambda x: x[1]['avg_time_per_epoch'])
                most_memory_efficient = min(results.items(), key=lambda x: x[1]['memory_overhead_mb'])

                print(f"\nğŸ’¡ {model_size.title()} æ¨¡å‹åˆ†æ:")
                print(f"   ğŸ† æœ€ä½³æå¤±: {best_loss[0]} ({best_loss[1]['final_loss']:.6f})")
                print(f"   âš¡ æœ€å¿«é€Ÿåº¦: {fastest[0]} ({fastest[1]['avg_time_per_epoch']:.3f}s/epoch)")
                print(f"   ğŸ’¾ æœ€çœè¨˜æ†¶é«”: {most_memory_efficient[0]} ({most_memory_efficient[1]['memory_overhead_mb']:.1f}MB)")

        # æ•´é«”æ¨è–¦
        print(f"\nğŸ¯ ç¸½é«”æ¨è–¦:")
        print("=" * 40)

        # æª¢æŸ¥ Automagic å„ªåŒ–å™¨æ˜¯å¦åœ¨æ¸¬è©¦ä¸­
        automagic_optimizers = []
        for model_size, results in self.results.items():
            for name in results.keys():
                if 'Automagic' in name and name not in automagic_optimizers:
                    automagic_optimizers.append(name)

        if automagic_optimizers:
            print("âœ… Automagic å„ªåŒ–å™¨å¯ç”¨:")
            for opt in automagic_optimizers:
                print(f"   - {opt}")
            print("\nğŸ“ˆ ä½¿ç”¨å»ºè­°:")
            print("   â€¢ ä¸€èˆ¬è¨“ç·´: Automagic_CameAMP")
            print("   â€¢ å¤§æ¨¡å‹: Automagic_CameAMP8bit")
            print("   â€¢ é«˜ç´šåŠŸèƒ½: Automagic_CameAMP_COptim")
            print("   â€¢ ç”Ÿç”¢ç’°å¢ƒ: Automagic_CameAMP_COptim8bit")
        else:
            print("âš ï¸  æ²’æœ‰æ¸¬è©¦ Automagic å„ªåŒ–å™¨")
            print("   è«‹ç¢ºä¿ library/automagic_cameamp.py å¯ç”¨")

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    if not AUTOMAGIC_AVAILABLE:
        print("âŒ Automagic å„ªåŒ–å™¨ä¸å¯ç”¨ï¼Œåƒ…æ¸¬è©¦ PyTorch å…§å»ºå„ªåŒ–å™¨")

    # æ¸¬è©¦é…ç½®
    config = BenchmarkConfig(
        model_sizes=['small', 'medium'],  # æ¸›å°‘æ¸¬è©¦æ™‚é–“
        num_epochs=30,
        num_warmup=5,
        batch_size=64,
        measure_memory=True,
        save_plots=True,
        verbose=True
    )

    print(f"ğŸ“‹ åŸºæº–æ¸¬è©¦é…ç½®:")
    print(f"   æ¨¡å‹å¤§å°: {config.model_sizes}")
    print(f"   è¨“ç·´è¼ªæ•¸: {config.num_epochs}")
    print(f"   æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"   æš–èº«è¼ªæ•¸: {config.num_warmup}")

    # é‹è¡ŒåŸºæº–æ¸¬è©¦
    comparison = OptimizersComparison(config)
    results = comparison.run_all_benchmarks()

    if results:
        # ç¹ªè£½çµæœ
        comparison.plot_results()

        # æ‰“å°ç¸½çµ
        comparison.print_summary()

        print(f"\nâœ… åŸºæº–æ¸¬è©¦å®Œæˆï¼")
        print(f"   æ¸¬è©¦äº† {len(config.model_sizes)} ç¨®æ¨¡å‹å¤§å°")
        print(f"   ç¸½å…± {sum(len(r) for r in results.values())} å€‹å„ªåŒ–å™¨æ¸¬è©¦")
    else:
        print("âŒ åŸºæº–æ¸¬è©¦å¤±æ•—ï¼Œæ²’æœ‰æˆåŠŸçš„çµæœ")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸  åŸºæº–æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ åŸºæº–æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

    print("\nğŸ‰ Automagic_CameAMP åŸºæº–æ¸¬è©¦å®Œæˆï¼")