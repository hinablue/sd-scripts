#!/usr/bin/env python3
"""
Automagic_CameAMP å„ªåŒ–å™¨å®Œæ•´æ¸¬è©¦å¥—ä»¶

é€™å€‹æ¸¬è©¦æ–‡ä»¶æ¼”ç¤ºäº†æ‰€æœ‰å››å€‹ç‰ˆæœ¬çš„ Automagic_CameAMP å„ªåŒ–å™¨çš„ä½¿ç”¨æ–¹æ³•ï¼š
1. Automagic_CameAMP - åŸºç¤ç‰ˆæœ¬
2. Automagic_CameAMP8bit - 8-bit é‡åŒ–ç‰ˆæœ¬
3. Automagic_CameAMP_COptim - ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰ˆæœ¬
4. Automagic_CameAMP_COptim8bit - å…¨åŠŸèƒ½ç‰ˆæœ¬

ä½œè€…: Hina
ç‰ˆæœ¬: 2.0
æ—¥æœŸ: 2025-01-27
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import time
import os
import sys
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

# åŠ å…¥åº«è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from library.automagic_cameamp import (
        Automagic_CameAMP,
        Automagic_CameAMP8bit,
        Automagic_CameAMP_COptim,
        Automagic_CameAMP_COptim8bit,
        OptimizerConfig
    )
except ImportError as e:
    print(f"âŒ ç„¡æ³•å°å…¥å„ªåŒ–å™¨: {e}")
    print("è«‹ç¢ºä¿ library/automagic_cameamp.py å­˜åœ¨ä¸”å¯ç”¨")
    sys.exit(1)

# è¨­å®šè¨­å‚™
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")

@dataclass
class TestConfig:
    """æ¸¬è©¦é…ç½®"""
    batch_size: int = 32
    input_size: int = 128
    hidden_sizes: List[int] = None
    output_size: int = 10
    num_epochs: int = 20
    num_batches: int = 50
    save_plots: bool = True
    verbose: bool = True

    def __post_init__(self):
        if self.hidden_sizes is None:
            self.hidden_sizes = [256, 128, 64]

class TestModel(nn.Module):
    """æ¸¬è©¦ç”¨çš„ç¥ç¶“ç¶²è·¯æ¨¡å‹"""

    def __init__(self, config: TestConfig):
        super().__init__()
        self.config = config

        layers = []
        prev_size = config.input_size

        # éš±è—å±¤
        for hidden_size in config.hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.1),
                nn.BatchNorm1d(hidden_size)
            ])
            prev_size = hidden_size

        # è¼¸å‡ºå±¤
        layers.append(nn.Linear(prev_size, config.output_size))

        self.network = nn.Sequential(*layers)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """åˆå§‹åŒ–æ¬Šé‡"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)

    def forward(self, x):
        return self.network(x)

class OptimizerTester:
    """å„ªåŒ–å™¨æ¸¬è©¦å™¨"""

    def __init__(self, test_config: TestConfig):
        self.config = test_config
        self.results = {}

    def create_synthetic_data(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆåˆæˆæ¸¬è©¦æ•¸æ“š"""
        # ç”Ÿæˆåˆ†é¡æ•¸æ“š
        X = torch.randn(
            self.config.batch_size,
            self.config.input_size,
            device=device
        )
        y = torch.randint(
            0,
            self.config.output_size,
            (self.config.batch_size,),
            device=device
        )
        return X, y

    def test_optimizer(self,
                      optimizer_class,
                      optimizer_name: str,
                      optimizer_kwargs: Optional[Dict] = None) -> Dict[str, List[float]]:
        """æ¸¬è©¦å–®å€‹å„ªåŒ–å™¨"""
        print(f"\nğŸ§ª æ¸¬è©¦ {optimizer_name}")
        print("=" * 50)

        if optimizer_kwargs is None:
            optimizer_kwargs = {}

        # å‰µå»ºæ¨¡å‹
        model = TestModel(self.config).to(device)

        # å‰µå»ºå„ªåŒ–å™¨
        try:
            optimizer = optimizer_class(
                model.parameters(),
                lr=1e-3,
                weight_decay=1e-4,
                warmup_steps=100,
                verbose=False,
                **optimizer_kwargs
            )
            print(f"âœ… {optimizer_name} åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ {optimizer_name} åˆå§‹åŒ–å¤±æ•—: {e}")
            return None

        # æ¸¬è©¦æŒ‡æ¨™
        losses = []
        learning_rates = []
        gradient_norms = []
        times = []

        # é¡å¤–æŒ‡æ¨™ (C-Optim ç‰ˆæœ¬)
        contextual_multipliers = []
        edge_cases = []

        model.train()

        for epoch in range(self.config.num_epochs):
            epoch_losses = []
            epoch_start = time.time()

            for batch_idx in range(self.config.num_batches):
                # ç”Ÿæˆæ•¸æ“š
                X, y = self.create_synthetic_data()

                # å‰å‘å‚³æ’­
                optimizer.zero_grad()
                outputs = model(X)
                loss = F.cross_entropy(outputs, y)

                # åå‘å‚³æ’­
                loss.backward()

                # è¨ˆç®—æ¢¯åº¦ç¯„æ•¸
                total_norm = 0
                for p in model.parameters():
                    if p.grad is not None:
                        param_norm = p.grad.norm(2)
                        total_norm += param_norm.item() ** 2
                total_norm = total_norm ** 0.5

                # å„ªåŒ–å™¨æ­¥é©Ÿ
                step_loss = optimizer.step(closure=lambda: loss)
                if step_loss is not None:
                    loss = step_loss

                # è¨˜éŒ„æŒ‡æ¨™
                epoch_losses.append(loss.item())
                gradient_norms.append(total_norm)

                # è¨˜éŒ„å­¸ç¿’ç‡
                if hasattr(optimizer, '_get_group_lr'):
                    current_lr = optimizer._get_group_lr(optimizer.param_groups[0])
                else:
                    current_lr = optimizer.param_groups[0]['lr']
                learning_rates.append(current_lr)

                # C-Optim ç‰¹æ®ŠæŒ‡æ¨™
                if hasattr(optimizer, 'c_optim'):
                    lr_mult = optimizer.c_optim.compute_contextual_lr_multiplier()
                    is_edge = optimizer.c_optim.detect_edge_case()
                    contextual_multipliers.append(lr_mult)
                    edge_cases.append(is_edge)

            # è¨˜éŒ„æ™‚é–“å’Œæå¤±
            epoch_time = time.time() - epoch_start
            avg_loss = np.mean(epoch_losses)
            losses.append(avg_loss)
            times.append(epoch_time)

            if self.config.verbose and epoch % 5 == 0:
                print(f"Epoch {epoch:2d}: Loss = {avg_loss:.6f}, "
                      f"LR = {current_lr:.6f}, Time = {epoch_time:.2f}s")

                # C-Optim é¡å¤–ä¿¡æ¯
                if hasattr(optimizer, 'c_optim'):
                    if contextual_multipliers:
                        print(f"          ä¸Šä¸‹æ–‡ä¹˜æ•¸ = {contextual_multipliers[-1]:.3f}, "
                              f"é‚Šç·£æƒ…æ³ = {edge_cases[-1]}")

        # è¨ˆç®—æœ€çµ‚çµ±è¨ˆ
        final_loss = losses[-1]
        min_loss = min(losses)
        avg_time = np.mean(times)
        convergence_speed = len(losses) - np.argmin(losses)

        print(f"ğŸ“Š {optimizer_name} çµæœ:")
        print(f"   æœ€çµ‚æå¤±: {final_loss:.6f}")
        print(f"   æœ€ä½³æå¤±: {min_loss:.6f}")
        print(f"   å¹³å‡æ™‚é–“: {avg_time:.2f}s/epoch")
        print(f"   æ”¶æ–‚é€Ÿåº¦: {convergence_speed} epochs to min")

        # è¿”å›çµæœ
        results = {
            'losses': losses,
            'learning_rates': learning_rates,
            'gradient_norms': gradient_norms,
            'times': times,
            'final_loss': final_loss,
            'min_loss': min_loss,
            'avg_time': avg_time,
            'convergence_speed': convergence_speed
        }

        # C-Optim ç‰ˆæœ¬çš„é¡å¤–çµæœ
        if contextual_multipliers:
            results['contextual_multipliers'] = contextual_multipliers
            results['edge_cases'] = edge_cases

        return results

    def run_all_tests(self) -> Dict[str, Any]:
        """é‹è¡Œæ‰€æœ‰å„ªåŒ–å™¨æ¸¬è©¦"""
        print("ğŸš€ é–‹å§‹ Automagic_CameAMP å„ªåŒ–å™¨å¥—ä»¶æ¸¬è©¦")
        print("=" * 60)

        optimizers_to_test = [
            (Automagic_CameAMP, "Automagic_CameAMP (åŸºç¤ç‰ˆ)", {}),
            (Automagic_CameAMP_COptim, "Automagic_CameAMP_COptim (ä¸Šä¸‹æ–‡æ„ŸçŸ¥)", {
                'context_window': 50,
                'edge_threshold': 0.8,
                'adaptation_rate': 0.15
            })
        ]

        # æ¸¬è©¦ 8-bit ç‰ˆæœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            # æ¸¬è©¦ bitsandbytes å¯ç”¨æ€§
            test_model = TestModel(self.config).to(device)
            test_optimizer = Automagic_CameAMP8bit(test_model.parameters(), lr=1e-3)
            del test_optimizer, test_model

            optimizers_to_test.extend([
                (Automagic_CameAMP8bit, "Automagic_CameAMP8bit (8-bit)", {}),
                (Automagic_CameAMP_COptim8bit, "Automagic_CameAMP_COptim8bit (å…¨åŠŸèƒ½)", {
                    'context_window': 30,
                    'edge_threshold': 0.7,
                    'adaptation_rate': 0.2
                })
            ])
            print("âœ… 8-bit ç‰ˆæœ¬å¯ç”¨ï¼Œå°‡åŒ…å«åœ¨æ¸¬è©¦ä¸­")
        except Exception as e:
            print(f"âš ï¸  8-bit ç‰ˆæœ¬ä¸å¯ç”¨: {e}")
            print("   è·³é 8-bit ç‰ˆæœ¬æ¸¬è©¦")

        # é‹è¡Œæ¸¬è©¦
        for optimizer_class, optimizer_name, kwargs in optimizers_to_test:
            result = self.test_optimizer(optimizer_class, optimizer_name, kwargs)
            if result is not None:
                self.results[optimizer_name] = result

        return self.results

    def plot_results(self, save_dir: str = "docs/hina/plots"):
        """ç¹ªè£½æ¸¬è©¦çµæœ"""
        if not self.results:
            print("âŒ æ²’æœ‰çµæœå¯ç¹ªè£½")
            return

        # å‰µå»ºä¿å­˜ç›®éŒ„
        os.makedirs(save_dir, exist_ok=True)

        # è¨­å®šç¹ªåœ–æ¨£å¼
        plt.style.use('default')
        colors = ['blue', 'red', 'green', 'orange', 'purple']

        # 1. æå¤±æ›²ç·š
        plt.figure(figsize=(12, 8))
        for i, (name, results) in enumerate(self.results.items()):
            plt.plot(results['losses'],
                    label=name,
                    color=colors[i % len(colors)],
                    linewidth=2,
                    alpha=0.8)

        plt.title('Automagic_CameAMP å„ªåŒ–å™¨æå¤±æ¯”è¼ƒ', fontsize=16, fontweight='bold')
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.yscale('log')

        if self.config.save_plots:
            plt.savefig(f"{save_dir}/loss_comparison.png", dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æå¤±æ¯”è¼ƒåœ–å·²ä¿å­˜: {save_dir}/loss_comparison.png")
        plt.show()

        # 2. å­¸ç¿’ç‡è®ŠåŒ–
        plt.figure(figsize=(12, 8))
        for i, (name, results) in enumerate(self.results.items()):
            # åªé¡¯ç¤ºå‰ 200 å€‹é»é¿å…éæ–¼å¯†é›†
            lr_data = results['learning_rates'][:200] if len(results['learning_rates']) > 200 else results['learning_rates']
            plt.plot(lr_data,
                    label=name,
                    color=colors[i % len(colors)],
                    linewidth=1.5,
                    alpha=0.7)

        plt.title('å­¸ç¿’ç‡è®ŠåŒ–å°æ¯”', fontsize=16, fontweight='bold')
        plt.xlabel('Step', fontsize=12)
        plt.ylabel('Learning Rate', fontsize=12)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.yscale('log')

        if self.config.save_plots:
            plt.savefig(f"{save_dir}/learning_rate_comparison.png", dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å­¸ç¿’ç‡æ¯”è¼ƒåœ–å·²ä¿å­˜: {save_dir}/learning_rate_comparison.png")
        plt.show()

        # 3. æ¢¯åº¦ç¯„æ•¸
        plt.figure(figsize=(12, 8))
        for i, (name, results) in enumerate(self.results.items()):
            grad_data = results['gradient_norms'][:200] if len(results['gradient_norms']) > 200 else results['gradient_norms']
            plt.plot(grad_data,
                    label=name,
                    color=colors[i % len(colors)],
                    linewidth=1.5,
                    alpha=0.7)

        plt.title('æ¢¯åº¦ç¯„æ•¸è®ŠåŒ–', fontsize=16, fontweight='bold')
        plt.xlabel('Step', fontsize=12)
        plt.ylabel('Gradient Norm', fontsize=12)
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.3)
        plt.yscale('log')

        if self.config.save_plots:
            plt.savefig(f"{save_dir}/gradient_norm_comparison.png", dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æ¢¯åº¦ç¯„æ•¸æ¯”è¼ƒåœ–å·²ä¿å­˜: {save_dir}/gradient_norm_comparison.png")
        plt.show()

        # 4. C-Optim ç‰¹æ®ŠæŒ‡æ¨™ (å¦‚æœæœ‰çš„è©±)
        c_optim_results = {name: results for name, results in self.results.items()
                          if 'contextual_multipliers' in results}

        if c_optim_results:
            plt.figure(figsize=(12, 10))

            # ä¸Šä¸‹æ–‡ä¹˜æ•¸
            plt.subplot(2, 1, 1)
            for i, (name, results) in enumerate(c_optim_results.items()):
                mult_data = results['contextual_multipliers'][:200] if len(results['contextual_multipliers']) > 200 else results['contextual_multipliers']
                plt.plot(mult_data,
                        label=name,
                        color=colors[i % len(colors)],
                        linewidth=1.5)

            plt.title('ä¸Šä¸‹æ–‡å­¸ç¿’ç‡ä¹˜æ•¸', fontsize=14, fontweight='bold')
            plt.ylabel('LR Multiplier', fontsize=12)
            plt.legend(fontsize=10)
            plt.grid(True, alpha=0.3)

            # é‚Šç·£æƒ…æ³
            plt.subplot(2, 1, 2)
            for i, (name, results) in enumerate(c_optim_results.items()):
                edge_data = results['edge_cases'][:200] if len(results['edge_cases']) > 200 else results['edge_cases']
                edge_numeric = [1 if x else 0 for x in edge_data]
                plt.plot(edge_numeric,
                        label=name,
                        color=colors[i % len(colors)],
                        linewidth=1.5,
                        alpha=0.7)

            plt.title('é‚Šç·£æƒ…æ³æª¢æ¸¬', fontsize=14, fontweight='bold')
            plt.xlabel('Step', fontsize=12)
            plt.ylabel('Edge Case (1=True, 0=False)', fontsize=12)
            plt.legend(fontsize=10)
            plt.grid(True, alpha=0.3)

            plt.tight_layout()

            if self.config.save_plots:
                plt.savefig(f"{save_dir}/c_optim_metrics.png", dpi=300, bbox_inches='tight')
                print(f"ğŸ“Š C-Optim æŒ‡æ¨™åœ–å·²ä¿å­˜: {save_dir}/c_optim_metrics.png")
            plt.show()

    def print_summary(self):
        """æ‰“å°æ¸¬è©¦ç¸½çµ"""
        if not self.results:
            print("âŒ æ²’æœ‰æ¸¬è©¦çµæœ")
            return

        print("\n" + "=" * 80)
        print("ğŸ“‹ Automagic_CameAMP å„ªåŒ–å™¨æ¸¬è©¦ç¸½çµ")
        print("=" * 80)

        # å‰µå»ºçµæœè¡¨æ ¼
        headers = ["å„ªåŒ–å™¨", "æœ€çµ‚æå¤±", "æœ€ä½³æå¤±", "å¹³å‡æ™‚é–“", "æ”¶æ–‚é€Ÿåº¦"]
        data = []

        for name, results in self.results.items():
            data.append([
                name.split('(')[0].strip(),  # ç°¡åŒ–åç¨±
                f"{results['final_loss']:.6f}",
                f"{results['min_loss']:.6f}",
                f"{results['avg_time']:.2f}s",
                f"{results['convergence_speed']} epochs"
            ])

        # æ‰“å°è¡¨æ ¼
        col_widths = [max(len(str(row[i])) for row in [headers] + data) + 2
                     for i in range(len(headers))]

        def print_row(row):
            print("|" + "|".join(f" {str(item).ljust(col_widths[i])} "
                                 for i, item in enumerate(row)) + "|")

        # è¡¨æ ¼æ¨™é¡Œ
        print_row(headers)
        print("|" + "|".join("-" * (w + 2) for w in col_widths) + "|")

        # è¡¨æ ¼æ•¸æ“š
        for row in data:
            print_row(row)

        # æ¨è–¦
        print("\nğŸ’¡ ä½¿ç”¨å»ºè­°:")
        print("=" * 40)

        best_final = min(self.results.items(), key=lambda x: x[1]['final_loss'])
        fastest = min(self.results.items(), key=lambda x: x[1]['avg_time'])
        best_convergence = min(self.results.items(), key=lambda x: x[1]['convergence_speed'])

        print(f"ğŸ† æœ€ä½³æœ€çµ‚æå¤±: {best_final[0]}")
        print(f"âš¡ æœ€å¿«è¨“ç·´é€Ÿåº¦: {fastest[0]}")
        print(f"ğŸ¯ æœ€å¿«æ”¶æ–‚é€Ÿåº¦: {best_convergence[0]}")

        # è¨˜æ†¶é«”ä½¿ç”¨å»ºè­°
        print("\nğŸ’¾ è¨˜æ†¶é«”ä½¿ç”¨æŒ‡å—:")
        has_8bit = any('8bit' in name for name in self.results.keys())
        if has_8bit:
            print("âœ… 8-bit ç‰ˆæœ¬å¯ç”¨ - æ¨è–¦ç”¨æ–¼å¤§æ¨¡å‹ (ç¯€çœ ~75% è¨˜æ†¶é«”)")
        else:
            print("âš ï¸  8-bit ç‰ˆæœ¬ä¸å¯ç”¨ - éœ€è¦å®‰è£ bitsandbytes")

        has_coptim = any('COptim' in name for name in self.results.keys())
        if has_coptim:
            print("âœ… C-Optim ç‰ˆæœ¬å¯ç”¨ - æ¨è–¦ç”¨æ–¼éœ€è¦æ™ºèƒ½èª¿æ•´çš„å ´æ™¯")

def run_comprehensive_test():
    """é‹è¡Œå®Œæ•´æ¸¬è©¦"""
    print("ğŸ§ª Automagic_CameAMP å„ªåŒ–å™¨æ¸¬è©¦å¥—ä»¶")
    print("ä½œè€…: Hina")
    print("ç‰ˆæœ¬: 2.0")
    print("æ—¥æœŸ: 2025-01-27")
    print("=" * 60)

    # æ¸¬è©¦é…ç½®
    test_config = TestConfig(
        batch_size=64,
        input_size=128,
        hidden_sizes=[256, 128, 64],
        output_size=10,
        num_epochs=15,
        num_batches=30,
        save_plots=True,
        verbose=True
    )

    print(f"ğŸ“‹ æ¸¬è©¦é…ç½®:")
    print(f"   æ‰¹æ¬¡å¤§å°: {test_config.batch_size}")
    print(f"   è¼¸å…¥ç¶­åº¦: {test_config.input_size}")
    print(f"   éš±è—å±¤: {test_config.hidden_sizes}")
    print(f"   è¼¸å‡ºé¡åˆ¥: {test_config.output_size}")
    print(f"   è¨“ç·´è¼ªæ•¸: {test_config.num_epochs}")
    print(f"   æ¯è¼ªæ‰¹æ¬¡: {test_config.num_batches}")

    # å‰µå»ºæ¸¬è©¦å™¨
    tester = OptimizerTester(test_config)

    # é‹è¡Œæ¸¬è©¦
    results = tester.run_all_tests()

    if results:
        # ç¹ªè£½çµæœ
        tester.plot_results()

        # æ‰“å°ç¸½çµ
        tester.print_summary()

        print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼å…±æ¸¬è©¦äº† {len(results)} å€‹å„ªåŒ–å™¨")
    else:
        print("âŒ æ¸¬è©¦å¤±æ•—ï¼Œæ²’æœ‰æˆåŠŸçš„çµæœ")

def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ä½¿ç”¨æ–¹æ³•"""
    print("\n" + "=" * 60)
    print("ğŸ¯ åŸºæœ¬ä½¿ç”¨æ–¹æ³•æ¼”ç¤º")
    print("=" * 60)

    # å‰µå»ºç°¡å–®æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(100, 50),
        nn.ReLU(),
        nn.Linear(50, 10)
    ).to(device)

    print("ğŸ“¦ æ¨¡å‹çµæ§‹:")
    print(model)

    # æ¼”ç¤ºå„ç¨®å„ªåŒ–å™¨å‰µå»º
    optimizers = {}

    # 1. åŸºç¤ç‰ˆæœ¬
    try:
        optimizers['åŸºç¤ç‰ˆ'] = Automagic_CameAMP(
            model.parameters(),
            lr=1e-3,
            weight_decay=1e-4,
            warmup_steps=100
        )
        print("âœ… åŸºç¤ç‰ˆæœ¬å‰µå»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ åŸºç¤ç‰ˆæœ¬å‰µå»ºå¤±æ•—: {e}")

    # 2. C-Optim ç‰ˆæœ¬
    try:
        optimizers['ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰ˆ'] = Automagic_CameAMP_COptim(
            model.parameters(),
            lr=1e-3,
            context_window=50,
            edge_threshold=0.8
        )
        print("âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰ˆæœ¬å‰µå»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰ˆæœ¬å‰µå»ºå¤±æ•—: {e}")

    # 3. 8-bit ç‰ˆæœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        optimizers['8-bitç‰ˆ'] = Automagic_CameAMP8bit(
            model.parameters(),
            lr=1e-3,
            weight_decay=1e-4
        )
        print("âœ… 8-bit ç‰ˆæœ¬å‰µå»ºæˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸  8-bit ç‰ˆæœ¬ä¸å¯ç”¨: {e}")

    # 4. å…¨åŠŸèƒ½ç‰ˆæœ¬
    try:
        optimizers['å…¨åŠŸèƒ½ç‰ˆ'] = Automagic_CameAMP_COptim8bit(
            model.parameters(),
            lr=1e-3,
            context_window=30
        )
        print("âœ… å…¨åŠŸèƒ½ç‰ˆæœ¬å‰µå»ºæˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸  å…¨åŠŸèƒ½ç‰ˆæœ¬ä¸å¯ç”¨: {e}")

    print(f"\nğŸ‰ æˆåŠŸå‰µå»º {len(optimizers)} å€‹å„ªåŒ–å™¨")

    # ç°¡å–®è¨“ç·´æ¼”ç¤º
    if optimizers:
        print("\nğŸš€ ç°¡å–®è¨“ç·´æ¼”ç¤º (åŸºç¤ç‰ˆæœ¬):")

        optimizer_name, optimizer = next(iter(optimizers.items()))
        print(f"ä½¿ç”¨å„ªåŒ–å™¨: {optimizer_name}")

        # è¨“ç·´æ•¸æ“š
        X = torch.randn(32, 100, device=device)
        y = torch.randint(0, 10, (32,), device=device)

        # è¨“ç·´å¹¾æ­¥
        for step in range(5):
            optimizer.zero_grad()
            outputs = model(X)
            loss = F.cross_entropy(outputs, y)
            loss.backward()
            optimizer.step()

            print(f"Step {step + 1}: Loss = {loss.item():.6f}")

        print("âœ… è¨“ç·´æ¼”ç¤ºå®Œæˆ")

if __name__ == "__main__":
    try:
        # åŸºæœ¬ä½¿ç”¨æ¼”ç¤º
        demo_basic_usage()

        # å®Œæ•´æ¸¬è©¦
        run_comprehensive_test()

    except KeyboardInterrupt:
        print("\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

    print(f"\nğŸ‰ Automagic_CameAMP æ¸¬è©¦å®Œæˆï¼")
    print("æ„Ÿè¬ä½¿ç”¨ Automagic_CameAMP å„ªåŒ–å™¨å¥—ä»¶ ğŸš€")