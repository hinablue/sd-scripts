#!/usr/bin/env python3
"""
æ”¹é€²ç‰ˆ C-Optim ä¸Šä¸‹æ–‡æ„ŸçŸ¥å„ªåŒ–å™¨æ¸¬è©¦

é€™å€‹æ¸¬è©¦å±•ç¤ºäº†æ”¹é€²å¾Œçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å­¸ç¿’ç‡èª¿æ•´æ©Ÿåˆ¶ï¼Œ
è§£æ±ºäº†åŸç‰ˆæœ¬å­¸ç¿’æ•ˆæœä½çš„å•é¡Œã€‚

ä¸»è¦æ”¹é€²ï¼š
1. æ›´æ™ºèƒ½çš„å­¸ç¿’ç‡ä¹˜æ•¸è¨ˆç®—
2. å¤šç¶­åº¦çš„ç©©å®šæ€§è©•ä¼°
3. å‹•æ…‹é‚Šç·£æƒ…æ³è™•ç†
4. æ”¶æ–‚é€Ÿåº¦è‡ªé©æ‡‰èª¿æ•´
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from library.automagic_cameamp import Automagic_CameAMP_COptim, Automagic_CameAMP_COptim8bit

class TestModel(nn.Module):
    """æ¸¬è©¦ç”¨çš„ç¥ç¶“ç¶²è·¯æ¨¡å‹"""

    def __init__(self, input_size=100, hidden_sizes=[64, 32], output_size=10):
        super().__init__()
        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(0.1)
            ])
            prev_size = hidden_size

        layers.append(nn.Linear(prev_size, output_size))
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

def create_test_data(batch_size=64, input_size=100, output_size=10, num_batches=100):
    """å‰µå»ºæ¸¬è©¦æ•¸æ“š"""
    data = []
    for _ in range(num_batches):
        x = torch.randn(batch_size, input_size)
        # å‰µå»ºæœ‰ä¸€å®šæ¨¡å¼çš„æ¨™ç±¤
        y = torch.randint(0, output_size, (batch_size,))
        data.append((x, y))
    return data

def run_training_comparison():
    """é‹è¡Œè¨“ç·´æ¯”è¼ƒæ¸¬è©¦"""

    print("ğŸ”¬ æ”¹é€²ç‰ˆ C-Optim ä¸Šä¸‹æ–‡æ„ŸçŸ¥å„ªåŒ–å™¨æ¸¬è©¦")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")

    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    test_data = create_test_data()

    # æ¸¬è©¦é…ç½®
    configs = [
        {
            'name': 'æ¨™æº– AdamW',
            'optimizer_class': torch.optim.AdamW,
            'kwargs': {'lr': 1e-3, 'weight_decay': 1e-4}
        },
        {
            'name': 'æ”¹é€²ç‰ˆ C-Optim (32-bit)',
            'optimizer_class': Automagic_CameAMP_COptim,
            'kwargs': {
                'lr': 1e-3,
                'context_window': 50,
                'edge_threshold': 0.8,  # é™ä½é–¾å€¼ï¼Œæ›´å®¹æ˜“è§¸ç™¼æ”¹é€²é‚è¼¯
                'adaptation_rate': 0.2,
                'verbose': True
            }
        },
        {
            'name': 'æ”¹é€²ç‰ˆ C-Optim (8-bit)',
            'optimizer_class': Automagic_CameAMP_COptim8bit,
            'kwargs': {
                'lr': 1e-3,
                'context_window': 50,
                'edge_threshold': 0.8,
                'adaptation_rate': 0.2,
                'verbose': True
            }
        }
    ]

    results = {}

    for config in configs:
        print(f"\n{'='*50}")
        print(f"æ¸¬è©¦: {config['name']}")
        print(f"{'='*50}")

        # å‰µå»ºæ¨¡å‹
        model = TestModel().to(device)

        try:
            # å‰µå»ºå„ªåŒ–å™¨
            optimizer = config['optimizer_class'](
                model.parameters(),
                **config['kwargs']
            )

            # è¨“ç·´
            losses, lr_multipliers, edge_cases = train_model(
                model, optimizer, test_data, device, config['name']
            )

            results[config['name']] = {
                'losses': losses,
                'lr_multipliers': lr_multipliers,
                'edge_cases': edge_cases,
                'final_loss': losses[-1],
                'convergence_step': find_convergence_step(losses)
            }

        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}")
            results[config['name']] = {'error': str(e)}

    # é¡¯ç¤ºçµæœä¸¦ç¹ªåœ–
    display_results(results)
    plot_training_curves(results)

    return results

def train_model(model, optimizer, test_data, device, optimizer_name):
    """è¨“ç·´æ¨¡å‹ä¸¦æ”¶é›†æŒ‡æ¨™"""
    model.train()
    losses = []
    lr_multipliers = []
    edge_cases = []

    print("é–‹å§‹è¨“ç·´...")

    for epoch, (x, y) in enumerate(test_data):
        x, y = x.to(device), y.to(device)

        # å‰å‘å‚³æ’­
        outputs = model(x)
        loss = F.cross_entropy(outputs, y)

        # åå‘å‚³æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        losses.append(loss.item())

        # æ”¶é›† C-Optim ç‰¹å®šæŒ‡æ¨™
        if hasattr(optimizer, 'c_optim'):
            lr_mult = optimizer.c_optim.compute_contextual_lr_multiplier()
            is_edge = optimizer.c_optim.detect_edge_case()
            grad_consistency = optimizer.c_optim.compute_gradient_consistency()
            loss_stability = optimizer.c_optim.compute_loss_stability()

            lr_multipliers.append(lr_mult)
            edge_cases.append(is_edge)

            if epoch % 20 == 0:
                print(f"Epoch {epoch:3d}: Loss={loss.item():.4f}, "
                      f"LRå€æ•¸={lr_mult:.3f}, é‚Šç·£={is_edge}, "
                      f"æ¢¯åº¦ä¸€è‡´æ€§={grad_consistency:.3f}, "
                      f"æå¤±ç©©å®šæ€§={loss_stability:.3f}")
        else:
            lr_multipliers.append(1.0)  # æ¨™æº–å„ªåŒ–å™¨æ²’æœ‰ä¹˜æ•¸
            edge_cases.append(False)

            if epoch % 20 == 0:
                print(f"Epoch {epoch:3d}: Loss={loss.item():.4f}")

    return losses, lr_multipliers, edge_cases

def find_convergence_step(losses, threshold=0.01):
    """æ‰¾åˆ°æ”¶æ–‚æ­¥æ•¸ï¼ˆæå¤±è®ŠåŒ–å°æ–¼é–¾å€¼ï¼‰"""
    if len(losses) < 10:
        return len(losses)

    for i in range(10, len(losses)):
        recent_losses = losses[i-10:i]
        if max(recent_losses) - min(recent_losses) < threshold:
            return i
    return len(losses)

def display_results(results):
    """é¡¯ç¤ºè¨“ç·´çµæœ"""
    print(f"\n{'='*60}")
    print("ğŸ“Š è¨“ç·´çµæœæ¯”è¼ƒ")
    print(f"{'='*60}")

    for name, result in results.items():
        if 'error' in result:
            print(f"{name}: âŒ éŒ¯èª¤ - {result['error']}")
        else:
            print(f"\nğŸ“ˆ {name}:")
            print(f"  æœ€çµ‚æå¤±: {result['final_loss']:.6f}")
            print(f"  æ”¶æ–‚æ­¥æ•¸: {result['convergence_step']}")

            if result['lr_multipliers']:
                avg_lr_mult = np.mean(result['lr_multipliers'])
                edge_rate = np.mean(result['edge_cases']) * 100
                print(f"  å¹³å‡ LR ä¹˜æ•¸: {avg_lr_mult:.3f}")
                print(f"  é‚Šç·£æƒ…æ³æ¯”ä¾‹: {edge_rate:.1f}%")

def plot_training_curves(results):
    """ç¹ªè£½è¨“ç·´æ›²ç·š"""
    plt.figure(figsize=(15, 10))

    # å­åœ–1: æå¤±æ›²ç·š
    plt.subplot(2, 3, 1)
    for name, result in results.items():
        if 'losses' in result:
            plt.plot(result['losses'], label=name, linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('ğŸ“‰ è¨“ç·´æå¤±æ¯”è¼ƒ')
    plt.legend()
    plt.yscale('log')
    plt.grid(True, alpha=0.3)

    # å­åœ–2: å­¸ç¿’ç‡ä¹˜æ•¸
    plt.subplot(2, 3, 2)
    for name, result in results.items():
        if 'lr_multipliers' in result and any(x != 1.0 for x in result['lr_multipliers']):
            plt.plot(result['lr_multipliers'], label=name, linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('LR Multiplier')
    plt.title('ğŸ“Š å­¸ç¿’ç‡ä¹˜æ•¸è®ŠåŒ–')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å­åœ–3: é‚Šç·£æƒ…æ³æª¢æ¸¬
    plt.subplot(2, 3, 3)
    for name, result in results.items():
        if 'edge_cases' in result:
            edge_smooth = np.convolve(
                [1 if x else 0 for x in result['edge_cases']],
                np.ones(10)/10, mode='same'
            )
            plt.plot(edge_smooth, label=name, linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Edge Case Rate')
    plt.title('âš ï¸ é‚Šç·£æƒ…æ³æª¢æ¸¬ç‡')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å­åœ–4: æ”¶æ–‚æ€§æ¯”è¼ƒ
    plt.subplot(2, 3, 4)
    names = []
    final_losses = []
    convergence_steps = []

    for name, result in results.items():
        if 'final_loss' in result:
            names.append(name.replace(' ', '\n'))
            final_losses.append(result['final_loss'])
            convergence_steps.append(result['convergence_step'])

    x = np.arange(len(names))
    plt.bar(x, final_losses, alpha=0.7)
    plt.xlabel('Optimizer')
    plt.ylabel('Final Loss')
    plt.title('ğŸ¯ æœ€çµ‚æå¤±æ¯”è¼ƒ')
    plt.xticks(x, names, rotation=45)
    plt.grid(True, alpha=0.3)

    # å­åœ–5: æ”¶æ–‚é€Ÿåº¦
    plt.subplot(2, 3, 5)
    plt.bar(x, convergence_steps, alpha=0.7, color='orange')
    plt.xlabel('Optimizer')
    plt.ylabel('Convergence Steps')
    plt.title('âš¡ æ”¶æ–‚é€Ÿåº¦æ¯”è¼ƒ')
    plt.xticks(x, names, rotation=45)
    plt.grid(True, alpha=0.3)

    # å­åœ–6: æå¤±æ”¹å–„è¶¨å‹¢
    plt.subplot(2, 3, 6)
    for name, result in results.items():
        if 'losses' in result and len(result['losses']) > 10:
            losses = np.array(result['losses'])
            improvement_rate = []
            for i in range(10, len(losses)):
                rate = (losses[i-10] - losses[i]) / losses[i-10]
                improvement_rate.append(rate)
            plt.plot(improvement_rate, label=name, linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Improvement Rate')
    plt.title('ğŸ“ˆ æå¤±æ”¹å–„ç‡')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('improved_coptim_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def analyze_improvement_effectiveness():
    """åˆ†ææ”¹é€²æ•ˆæœ"""
    print(f"\n{'='*60}")
    print("ğŸ” æ”¹é€²æ•ˆæœåˆ†æ")
    print(f"{'='*60}")

    print("\nğŸ“‹ ä¸»è¦æ”¹é€²é»ï¼š")
    print("1. âœ… å¤šç¶­åº¦æå¤±è¶¨å‹¢åˆ†æï¼ˆçŸ­æœŸ + é•·æœŸï¼‰")
    print("2. âœ… å‹•æ…‹å­¸ç¿’ç‡é‚Šç•Œèª¿æ•´ï¼ˆåŸºæ–¼ç©©å®šæ€§ï¼‰")
    print("3. âœ… åœæ»¯æª¢æ¸¬å’Œçªç ´æ©Ÿåˆ¶")
    print("4. âœ… æ”¶æ–‚é€Ÿåº¦è‡ªé©æ‡‰å› å­")
    print("5. âœ… æ™ºèƒ½é‚Šç·£æƒ…æ³è™•ç†")

    print("\nğŸ¯ é æœŸæ•ˆæœï¼š")
    print("â€¢ æ›´ç©æ¥µçš„å­¸ç¿’ç‡èª¿æ•´ï¼ˆ1.2-3.0 vs åŸç‰ˆ 1.2ï¼‰")
    print("â€¢ æ›´å¥½çš„åœæ»¯ç‹€æ…‹è™•ç†")
    print("â€¢ æ›´ç©©å®šçš„é‚Šç·£æƒ…æ³æ¢å¾©")
    print("â€¢ æ›´å¿«çš„æ”¶æ–‚é€Ÿåº¦")

if __name__ == "__main__":
    print("ğŸš€ æ”¹é€²ç‰ˆ C-Optim ä¸Šä¸‹æ–‡æ„ŸçŸ¥å„ªåŒ–å™¨æ¸¬è©¦")

    try:
        # é‹è¡Œæ¯”è¼ƒæ¸¬è©¦
        results = run_training_comparison()

        # åˆ†ææ”¹é€²æ•ˆæœ
        analyze_improvement_effectiveness()

        print(f"\n{'='*60}")
        print("âœ… æ¸¬è©¦å®Œæˆï¼æŸ¥çœ‹ç”Ÿæˆçš„åœ–è¡¨ä»¥äº†è§£æ”¹é€²æ•ˆæœã€‚")
        print(f"{'='*60}")

    except Exception as e:
        print(f"âŒ æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()