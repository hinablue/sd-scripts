# Automagic_CameAMP å¿«é€Ÿå…¥é–€æŒ‡å—

## ğŸš€ 5åˆ†é˜ä¸Šæ‰‹

### 1. åŸºæœ¬å®‰è£

```bash
# ç¢ºä¿å·²å®‰è£ PyTorch
pip install torch torchvision

# å¯é¸ï¼šå®‰è£ 8-bit æ”¯æ´
pip install bitsandbytes

# å¯é¸ï¼šå®‰è£ç¹ªåœ–æ”¯æ´
pip install matplotlib
```

### 2. æœ€ç°¡å–®çš„ä½¿ç”¨

```python
from library.automagic_cameamp import Automagic_CameAMP
import torch
import torch.nn as nn

# å‰µå»ºæ¨¡å‹
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.ReLU(),
    nn.Linear(50, 10)
)

# å‰µå»ºå„ªåŒ–å™¨
optimizer = Automagic_CameAMP(
    model.parameters(),
    lr=1e-3,          # å­¸ç¿’ç‡
    weight_decay=1e-4  # æ¬Šé‡è¡°æ¸›
)

# è¨“ç·´å¾ªç’°
for epoch in range(100):
    # ä½ çš„æ•¸æ“š
    x = torch.randn(32, 100)
    y = torch.randint(0, 10, (32,))

    # è¨“ç·´æ­¥é©Ÿ
    optimizer.zero_grad()
    output = model(x)
    loss = nn.CrossEntropyLoss()(output, y)
    loss.backward()
    optimizer.step()

    if epoch % 20 == 0:
        print(f"Epoch {epoch}: Loss = {loss.item():.4f}")
```

## ğŸ¯ é¸æ“‡åˆé©çš„ç‰ˆæœ¬

### å¿«é€Ÿæ±ºç­–æ¨¹

```
éœ€è¦ç¯€çœè¨˜æ†¶é«”å—ï¼Ÿ
â”œâ”€ æ˜¯ â†’ éœ€è¦æ™ºèƒ½èª¿æ•´å—ï¼Ÿ
â”‚      â”œâ”€ æ˜¯ â†’ Automagic_CameAMP_COptim8bit
â”‚      â””â”€ å¦ â†’ Automagic_CameAMP8bit
â””â”€ å¦ â†’ éœ€è¦æ™ºèƒ½èª¿æ•´å—ï¼Ÿ
       â”œâ”€ æ˜¯ â†’ Automagic_CameAMP_COptim
       â””â”€ å¦ â†’ Automagic_CameAMP
```

### å ´æ™¯æ¨è–¦

| å ´æ™¯ | æ¨è–¦ç‰ˆæœ¬ | ç†ç”± |
|------|----------|------|
| åˆå­¸è€…ã€å°æ¨¡å‹ | `Automagic_CameAMP` | ç©©å®šå¯é  |
| å¤§æ¨¡å‹è¨“ç·´ | `Automagic_CameAMP8bit` | ç¯€çœè¨˜æ†¶é«” |
| ç ”ç©¶å¯¦é©— | `Automagic_CameAMP_COptim` | æ™ºèƒ½èª¿æ•´ |
| ç”Ÿç”¢ç’°å¢ƒ | `Automagic_CameAMP_COptim8bit` | å…¨åŠŸèƒ½ |

## ğŸ› ï¸ å¸¸ç”¨é…ç½®

### 1. å°æ¨¡å‹ (< 100M åƒæ•¸)

```python
optimizer = Automagic_CameAMP(
    model.parameters(),
    lr=1e-3,
    warmup_steps=500,
    weight_decay=1e-4
)
```

### 2. ä¸­ç­‰æ¨¡å‹ (100M - 1B åƒæ•¸)

```python
optimizer = Automagic_CameAMP8bit(
    model.parameters(),
    lr=5e-4,
    warmup_steps=1000,
    weight_decay=5e-4
)
```

### 3. å¤§æ¨¡å‹ (> 1B åƒæ•¸)

```python
optimizer = Automagic_CameAMP_COptim8bit(
    model.parameters(),
    lr=1e-4,
    warmup_steps=2000,
    weight_decay=1e-3,
    context_window=50,
    edge_threshold=0.8
)
```

### 4. LoRA å¾®èª¿

```python
optimizer = Automagic_CameAMP(
    model.parameters(),
    lr=1e-3,
    full_finetune=False,  # å•Ÿç”¨ ALLoRA
    eta=2.0,
    warmup_steps=100
)
```

## ğŸ“Š ç›£æ§è¨“ç·´ç‹€æ…‹

### åŸºç¤ç›£æ§

```python
# åœ¨è¨“ç·´å¾ªç’°ä¸­æ·»åŠ 
if step % 100 == 0:
    # ç²å–ç•¶å‰å­¸ç¿’ç‡
    current_lr = optimizer.param_groups[0]['lr']

    # å¦‚æœæ˜¯ Automagic å„ªåŒ–å™¨ï¼Œç²å–å¹³å‡å­¸ç¿’ç‡
    if hasattr(optimizer, '_get_group_lr'):
        avg_lr = optimizer._get_group_lr(optimizer.param_groups[0])
        print(f"Step {step}: Loss = {loss:.4f}, LR = {avg_lr:.6f}")
    else:
        print(f"Step {step}: Loss = {loss:.4f}, LR = {current_lr:.6f}")
```

### é«˜ç´šç›£æ§ (C-Optim ç‰ˆæœ¬)

```python
# C-Optim ç‰ˆæœ¬çš„é¡å¤–ç›£æ§
if hasattr(optimizer, 'c_optim'):
    lr_multiplier = optimizer.c_optim.compute_contextual_lr_multiplier()
    is_edge_case = optimizer.c_optim.detect_edge_case()
    grad_consistency = optimizer.c_optim.compute_gradient_consistency()

    print(f"ä¸Šä¸‹æ–‡ä¹˜æ•¸: {lr_multiplier:.3f}")
    print(f"é‚Šç·£æƒ…æ³: {is_edge_case}")
    print(f"æ¢¯åº¦ä¸€è‡´æ€§: {grad_consistency:.3f}")
```

## ğŸ”§ å¸¸è¦‹å•é¡Œè§£æ±º

### 1. 8-bit åˆå§‹åŒ–å¤±æ•—

```python
try:
    optimizer = Automagic_CameAMP8bit(model.parameters(), lr=1e-3)
except RuntimeError as e:
    print(f"8-bit ä¸å¯ç”¨: {e}")
    # å›é€€åˆ° 32-bit ç‰ˆæœ¬
    optimizer = Automagic_CameAMP(model.parameters(), lr=1e-3)
```

### 2. å­¸ç¿’ç‡å¤ªå°

```python
# æª¢æŸ¥å­¸ç¿’ç‡é®ç½©
for group in optimizer.param_groups:
    for p in group['params']:
        if p.grad is not None:
            state = optimizer.state[p]
            if 'lr_mask' in state:
                lr_mask = state['lr_mask']
                print(f"LR ç¯„åœ: {lr_mask.min():.6f} - {lr_mask.max():.6f}")
```

### 3. è¨˜æ†¶é«”ä¸è¶³

```python
# ä½¿ç”¨ 8-bit ç‰ˆæœ¬
optimizer = Automagic_CameAMP8bit(model.parameters(), lr=1e-3)

# æˆ–è€…æ¸›å°‘æ‰¹æ¬¡å¤§å°
batch_size = 16  # å¾ 64 æ¸›å°‘åˆ° 16
```

### 4. è¨“ç·´ä¸ç©©å®š

```python
# é™ä½å­¸ç¿’ç‡
optimizer = Automagic_CameAMP(
    model.parameters(),
    lr=5e-4,  # å¾ 1e-3 é™ä½åˆ° 5e-4
    weight_decay=1e-3  # å¢åŠ æ­£å‰‡åŒ–
)

# æˆ–ä½¿ç”¨ C-Optim ç‰ˆæœ¬çš„è‡ªå‹•èª¿æ•´
optimizer = Automagic_CameAMP_COptim(
    model.parameters(),
    lr=1e-3,
    edge_threshold=0.8,  # æ›´æ•æ„Ÿçš„é‚Šç·£æª¢æ¸¬
    adaptation_rate=0.2   # æ›´ç©æ¥µçš„èª¿æ•´
)
```

## ğŸ¯ å¯¦ç”¨æŠ€å·§

### 1. å­¸ç¿’ç‡èª¿åº¦å™¨é…åˆ

```python
from torch.optim.lr_scheduler import CosineAnnealingLR

optimizer = Automagic_CameAMP(model.parameters(), lr=1e-3)
scheduler = CosineAnnealingLR(optimizer, T_max=100)

for epoch in range(100):
    # è¨“ç·´...
    scheduler.step()
```

### 2. æ¢¯åº¦è£å‰ª

```python
import torch.nn.utils as utils

for epoch in range(100):
    optimizer.zero_grad()
    loss = model(x, y)
    loss.backward()

    # æ¢¯åº¦è£å‰ª
    utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    optimizer.step()
```

### 3. æ··åˆç²¾åº¦è¨“ç·´

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
optimizer = Automagic_CameAMP(model.parameters(), lr=1e-3)

for epoch in range(100):
    optimizer.zero_grad()

    with autocast():
        output = model(x)
        loss = criterion(output, y)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### 4. æª¢æŸ¥é»ä¿å­˜

```python
# ä¿å­˜
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss.item(),
}
torch.save(checkpoint, 'checkpoint.pth')

# è¼‰å…¥
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch']
```

## âš¡ æ€§èƒ½èª¿å„ª

### 1. æš–èº«æ­¥æ•¸èª¿æ•´

```python
# å°æ¨¡å‹ï¼šå¿«é€Ÿæš–èº«
warmup_steps = 500

# å¤§æ¨¡å‹ï¼šè¼ƒé•·æš–èº«
warmup_steps = 2000

# å¾®èª¿ï¼šçŸ­æš–èº«
warmup_steps = 100
```

### 2. C-Optim åƒæ•¸èª¿å„ª

```python
# ç©©å®šè¨“ç·´
optimizer = Automagic_CameAMP_COptim(
    model.parameters(),
    context_window=100,   # è¼ƒå¤§çª—å£ï¼Œæ›´ç©©å®š
    edge_threshold=0.9,   # è¼ƒé«˜é–¾å€¼ï¼Œä¿å®ˆèª¿æ•´
    adaptation_rate=0.1   # è¼ƒä½é€Ÿç‡ï¼Œç·©æ…¢èª¿æ•´
)

# å¿«é€Ÿèª¿æ•´
optimizer = Automagic_CameAMP_COptim(
    model.parameters(),
    context_window=30,    # è¼ƒå°çª—å£ï¼Œæ›´éˆæ•
    edge_threshold=0.7,   # è¼ƒä½é–¾å€¼ï¼Œç©æ¥µèª¿æ•´
    adaptation_rate=0.3   # è¼ƒé«˜é€Ÿç‡ï¼Œå¿«é€Ÿèª¿æ•´
)
```

## ğŸ“ å®Œæ•´ç¤ºä¾‹

```python
#!/usr/bin/env python3
"""Automagic_CameAMP å®Œæ•´è¨“ç·´ç¤ºä¾‹"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from library.automagic_cameamp import Automagic_CameAMP_COptim

# è¨­å®šè¨­å‚™
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# å‰µå»ºæ¨¡å‹
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(784, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.layers(x)

# åˆå§‹åŒ–
model = SimpleModel().to(device)
optimizer = Automagic_CameAMP_COptim(
    model.parameters(),
    lr=1e-3,
    weight_decay=1e-4,
    warmup_steps=1000,
    context_window=50,
    edge_threshold=0.8,
    verbose=True
)

# è¨“ç·´å¾ªç’°
for epoch in range(100):
    model.train()

    # å‡è¨­æœ‰ DataLoader
    for batch_idx in range(100):  # æ¨¡æ“¬ 100 å€‹æ‰¹æ¬¡
        # ç”Ÿæˆå‡æ•¸æ“š
        data = torch.randn(64, 784, device=device)
        target = torch.randint(0, 10, (64,), device=device)

        # è¨“ç·´æ­¥é©Ÿ
        optimizer.zero_grad()
        output = model(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        optimizer.step()

        # ç›£æ§
        if batch_idx % 20 == 0:
            avg_lr = optimizer._get_group_lr(optimizer.param_groups[0])
            lr_mult = optimizer.c_optim.compute_contextual_lr_multiplier()
            is_edge = optimizer.c_optim.detect_edge_case()

            print(f'Epoch {epoch}, Batch {batch_idx}: '
                  f'Loss={loss:.4f}, LR={avg_lr:.6f}, '
                  f'Mult={lr_mult:.3f}, Edge={is_edge}')

print("âœ… è¨“ç·´å®Œæˆï¼")
```

## ğŸ‰ æ­å–œï¼

æ‚¨å·²ç¶“æŒæ¡äº† Automagic_CameAMP çš„åŸºæœ¬ä½¿ç”¨æ–¹æ³•ã€‚æ›´å¤šè©³ç´°ä¿¡æ¯è«‹åƒè€ƒï¼š

- ğŸ“– å®Œæ•´èªªæ˜æ–‡ä»¶ï¼š`README_Automagic_CameAMP.md`
- ğŸ§ª æ¸¬è©¦ç¯„ä¾‹ï¼š`test_automagic_cameamp.py`
- ğŸ”¬ é€²éšåŠŸèƒ½ï¼š`README_COptim_Improvements.md`

é–‹å§‹æ‚¨çš„é«˜æ•ˆè¨“ç·´ä¹‹æ—…å§ï¼ğŸš€