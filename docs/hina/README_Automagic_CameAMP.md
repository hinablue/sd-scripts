# Automagic_CameAMP å„ªåŒ–å™¨å®Œæ•´èªªæ˜

## ğŸ“‹ æ¦‚è¿°

`Automagic_CameAMP` æ˜¯ä¸€å€‹å…ˆé€²çš„æ·±åº¦å­¸ç¿’å„ªåŒ–å™¨ï¼Œæ•´åˆäº†å¤šç¨®æœ€æ–°çš„å„ªåŒ–æŠ€è¡“ï¼Œæä¾›é«˜æ•ˆä¸”ç©©å®šçš„æ¨¡å‹è¨“ç·´é«”é©—ã€‚è©²å„ªåŒ–å™¨ç³»åˆ—åŒ…å«å››å€‹ç‰ˆæœ¬ï¼Œå¾åŸºç¤åˆ°é«˜ç´šåŠŸèƒ½é€æ­¥å¢å¼·ã€‚

## ğŸš€ å„ªåŒ–å™¨ç‰ˆæœ¬å°æ¯”

### ç‰ˆæœ¬æ¦‚è¦½

| ç‰ˆæœ¬ | ç‰¹æ€§ | è¨˜æ†¶é«”ä½¿ç”¨ | é©ç”¨å ´æ™¯ |
|------|------|------------|----------|
| `Automagic_CameAMP` | åŸºç¤ç‰ˆæœ¬ | 100% | ä¸€èˆ¬è¨“ç·´ï¼Œç©©å®šå¯é  |
| `Automagic_CameAMP8bit` | 8-bit é‡åŒ– | ~25% | å¤§æ¨¡å‹ï¼Œè¨˜æ†¶é«”å—é™ |
| `Automagic_CameAMP_COptim` | ä¸Šä¸‹æ–‡æ„ŸçŸ¥ | 100% | é«˜ç´šè¨“ç·´ï¼Œæ™ºèƒ½èª¿æ•´ |
| `Automagic_CameAMP_COptim8bit` | å…¨åŠŸèƒ½ç‰ˆ | ~30% | å¤§æ¨¡å‹é«˜ç´šè¨“ç·´ |

## ğŸ”§ æ ¸å¿ƒæŠ€è¡“

### 1. **CAME (Confidence-guided Adaptive Memory Efficient Optimization)**
```python
# è‡ªä¿¡å¿ƒå¼•å°çš„è‡ªé©æ‡‰è¨˜æ†¶é«˜æ•ˆå„ªåŒ–
exp_avg_sq.mul_(beta2).add_(grad.pow(2) + eps1, alpha=1 - beta2)
scaled_grad = grad.clone().mul_(exp_avg_sq.rsqrt())
```
- **è«–æ–‡**: [CAME: Confidence-guided Adaptive Memory Efficient Optimization](https://arxiv.org/pdf/2411.02853)
- **åŠŸèƒ½**: æä¾›ç©©å®šä¸”é«˜æ•ˆçš„æ¢¯åº¦ç¸®æ”¾æ©Ÿåˆ¶

### 2. **AGR (Adaptive Gradient Regularization)**
```python
# è‡ªé©æ‡‰æ¢¯åº¦æ­£å‰‡åŒ–
abs_grad = torch.abs(p.grad)
alpha = abs_grad / sum_abs_all_group_grads
grad = p.grad * (1 - alpha)
```
- **è«–æ–‡**: [Adaptive Gradient Regularization](https://arxiv.org/pdf/2407.16944)
- **åŠŸèƒ½**: æ¸›å°‘æ¢¯åº¦å™ªéŸ³ï¼Œæé«˜è¨“ç·´ç©©å®šæ€§

### 3. **Torque-Aware Momentum (TAM)**
```python
# æ‰­çŸ©æ„ŸçŸ¥å‹•é‡ï¼ˆæ—©æœŸè¨“ç·´ï¼‰
corr = normalize(exp_avg, p=2.0, dim=0).mul_(normalize(scaled_grad, p=2.0, dim=0))
s.mul_(decay_rate).add_(corr, alpha=1.0 - decay_rate)
```
- **è«–æ–‡**: [Torque-Aware Momentum](https://arxiv.org/abs/2412.18790)
- **åŠŸèƒ½**: æ—©æœŸè¨“ç·´éšæ®µçš„æ™ºèƒ½å‹•é‡èª¿æ•´

### 4. **Consistency Momentum**
```python
# ä¸€è‡´æ€§å‹•é‡ï¼ˆå¾ŒæœŸè¨“ç·´ï¼‰
beta1_t = max(beta1 * group['beta1_decay'] ** state["step"], 0.4)
exp_avg.mul_(beta1_t).add_(scaled_grad, alpha=1 - beta1_t)
```
- **è«–æ–‡**: [Towards Faster Training of Diffusion Models](https://arxiv.org/abs/2404.07946)
- **åŠŸèƒ½**: åŸºæ–¼ä¸€è‡´æ€§ç¾è±¡çš„å‹•é‡æ›´æ–°

### 5. **Grams (Gradient Descent with Adaptive Momentum Scaling)**
```python
# è‡ªé©æ‡‰å‹•é‡ç¸®æ”¾æ¢¯åº¦ä¸‹é™
grams_update = update_p.abs() * grad.sign()
alpha = 1.0 * group['beta1_decay'] ** state["step"]
update_p = alpha * grams_update + (1 - alpha) * update_p
```
- **è«–æ–‡**: [Grams: Gradient Descent with Adaptive Momentum Scaling](https://arxiv.org/abs/2412.17107)
- **åŠŸèƒ½**: å‹•æ…‹èª¿æ•´å‹•é‡ç¸®æ”¾

### 6. **Orthogonal Gradient**
```python
# æ­£äº¤æ¢¯åº¦æŠ•å½±ï¼ˆæ—©æœŸæš–èº«ï¼‰
proj = torch.dot(w, g) / torch.dot(w, w).add(1e-30)
g_orth = g.sub_(w, alpha=proj)
```
- **è«–æ–‡**: [Grokking at the Edge of Numerical Stability](https://arxiv.org/abs/2501.04697)
- **åŠŸèƒ½**: æé«˜æ•¸å€¼ç©©å®šæ€§ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸

### 7. **AdaBelief Variance Estimation**
```python
# AdaBelief è®Šç•°æ•¸ä¼°è¨ˆ
res = (scaled_grad - exp_avg_bar).pow(2) + eps2
exp_avg_res.mul_(beta3).add_(res, alpha=1.0 - beta3)
update_p = exp_avg.clone().mul_(exp_avg_res.rsqrt())
```
- **è«–æ–‡**: [AdaBelief Optimizer](https://arxiv.org/abs/2010.07468)
- **åŠŸèƒ½**: åŸºæ–¼æ¢¯åº¦é æ¸¬çš„è‡ªé©æ‡‰å­¸ç¿’ç‡

### 8. **Automagic Learning Rate Mask**
```python
# è‡ªå‹•å­¸ç¿’ç‡é®ç½©
sign_agree = torch.where(last_polarity == current_polarity, 1.0, -1.0)
new_lr = torch.where(sign_agree > 0, lr_mask + lr_bump, lr_mask - lr_bump)
```
- **ä¾†æº**: [Automagic Optimizer](https://github.com/ostris/ai-toolkit)
- **åŠŸèƒ½**: åƒæ•¸ç´šåˆ¥çš„å­¸ç¿’ç‡è‡ªé©æ‡‰èª¿æ•´

### 9. **ALLoRA (Adaptive Learning Rate Mitigates LoRA Fatal Flaws)**
```python
# è¡Œç¸®æ”¾ï¼ˆé©ç”¨æ–¼ LoRAï¼‰
row_norm = p.norm(dim=1, keepdim=True)
state["row_scaling"] = 1.0 / torch.sqrt(row_norm + 1.0 / (group['eta']**2))
```
- **è«–æ–‡**: [ALLoRA: Adaptive Learning Rate Mitigates LoRA Fatal Flaws](https://arxiv.org/abs/2410.09692)
- **åŠŸèƒ½**: é‡å° LoRA å¾®èª¿çš„ç‰¹æ®Šå„ªåŒ–

### 10. **Adaptive Weight Decay**
```python
# è‡ªé©æ‡‰æ¬Šé‡è¡°æ¸›
norm_grad = (param_abs_grad - mean_norm) / std_norm
theta = 2 / (1 + torch.exp(-ada_alpha * norm_grad))
p.data.mul_(1 - new_lr * group["weight_decay"] * theta)
```
- **è«–æ–‡**: [Adaptive Weight Decay for Deep Neural Networks](https://arxiv.org/abs/1907.08931)
- **åŠŸèƒ½**: æ ¹æ“šæ¢¯åº¦æ¨¡å¼å‹•æ…‹èª¿æ•´æ¬Šé‡è¡°æ¸›

## âš™ï¸ é…ç½®åƒæ•¸

### OptimizerConfig é¡åˆ¥

```python
@dataclass
class OptimizerConfig:
    lr: float = 1e-6                    # åŸºç¤å­¸ç¿’ç‡
    min_lr: float = 1e-7                # æœ€å°å­¸ç¿’ç‡
    max_lr: float = 1e-3                # æœ€å¤§å­¸ç¿’ç‡
    lr_bump: float = 3e-6               # å­¸ç¿’ç‡èª¿æ•´å¹…åº¦
    eps: Tuple[float, float, float] = (1e-30, 1e-16, 1e-8)  # æ•¸å€¼ç©©å®šæ€§åƒæ•¸
    clip_threshold: float = 1.0         # æ¢¯åº¦è£å‰ªé–¾å€¼
    betas: Tuple[float, float, float] = (0.8, 0.99, 0.999)  # å‹•é‡åƒæ•¸
    eta: float = 2.0                    # ALLoRA åƒæ•¸
    beta1_decay: float = 0.9995         # Beta1 è¡°æ¸›ç‡
    weight_decay: float = 5e-4          # æ¬Šé‡è¡°æ¸›
    warmup_steps: int = 500             # æš–èº«æ­¥æ•¸
    came: bool = True                   # æ˜¯å¦ä½¿ç”¨ CAME
    full_finetune: bool = False         # æ˜¯å¦å…¨é‡å¾®èª¿
    verbose: bool = False               # è©³ç´°è¼¸å‡º
```

### åƒæ•¸è©³è§£

#### åŸºç¤åƒæ•¸
- **`lr`**: åŸºç¤å­¸ç¿’ç‡ï¼Œå»ºè­°ç¯„åœ 1e-6 åˆ° 1e-3
- **`min_lr` / `max_lr`**: å­¸ç¿’ç‡çš„å‹•æ…‹èª¿æ•´é‚Šç•Œ
- **`lr_bump`**: å­¸ç¿’ç‡é®ç½©çš„èª¿æ•´å¹…åº¦

#### ç©©å®šæ€§åƒæ•¸
- **`eps`**: ä¸‰å€‹ä¸åŒå ´æ™¯çš„ epsilon å€¼
  - `eps[0]`: CAME å’Œæ•¸å€¼ç©©å®šæ€§ (1e-30)
  - `eps[1]`: AdaBelief è®Šç•°æ•¸ (1e-16)
  - `eps[2]`: å…¶ä»–è¨ˆç®— (1e-8)
- **`clip_threshold`**: RMS æ­£è¦åŒ–çš„è£å‰ªé–¾å€¼

#### å‹•é‡åƒæ•¸
- **`betas`**: ä¸‰éšå‹•é‡åƒæ•¸
  - `beta1`: ä¸€éšå‹•é‡è¡°æ¸› (0.8)
  - `beta2`: äºŒéšå‹•é‡è¡°æ¸› (0.99)
  - `beta3`: AdaBelief è¡°æ¸› (0.999)
- **`beta1_decay`**: Beta1 çš„æ™‚é–“è¡°æ¸›ç‡

#### è¨“ç·´æ§åˆ¶
- **`warmup_steps`**: æš–èº«éšæ®µçš„æ­¥æ•¸
- **`weight_decay`**: L2 æ­£å‰‡åŒ–å¼·åº¦
- **`came`**: æ˜¯å¦ä½¿ç”¨ CAME ç®—æ³•ï¼ˆvs AdaBeliefï¼‰

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### 1. åŸºç¤ç‰ˆæœ¬ - Automagic_CameAMP

```python
from library.automagic_cameamp import Automagic_CameAMP

# å‰µå»ºå„ªåŒ–å™¨
optimizer = Automagic_CameAMP(
    model.parameters(),
    lr=1e-3,
    weight_decay=1e-4,
    warmup_steps=1000,
    verbose=True
)

# è¨“ç·´å¾ªç’°
for epoch in range(num_epochs):
    for batch in dataloader:
        loss = model(batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 2. 8-bit é‡åŒ–ç‰ˆæœ¬ - Automagic_CameAMP8bit

```python
from library.automagic_cameamp import Automagic_CameAMP8bit

# éœ€è¦ bitsandbytes æ”¯æ´
try:
    optimizer = Automagic_CameAMP8bit(
        model.parameters(),
        lr=1e-3,
        weight_decay=1e-4,
        verbose=True
    )
except RuntimeError as e:
    print(f"8-bit ä¸å¯ç”¨: {e}")
    # å›é€€åˆ° 32-bit ç‰ˆæœ¬
```

### 3. ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰ˆæœ¬ - Automagic_CameAMP_COptim

```python
from library.automagic_cameamp import Automagic_CameAMP_COptim

optimizer = Automagic_CameAMP_COptim(
    model.parameters(),
    lr=1e-3,
    context_window=100,          # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    edge_threshold=0.95,         # é‚Šç·£æƒ…æ³æª¢æ¸¬é–¾å€¼
    adaptation_rate=0.1,         # é©æ‡‰é€Ÿç‡
    momentum_scales=[1, 5, 20, 100]  # å¤šå°ºåº¦å‹•é‡
)

# ç›£æ§ä¸Šä¸‹æ–‡ç‹€æ…‹
for batch in dataloader:
    loss = model(batch)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # æª¢æŸ¥ä¸Šä¸‹æ–‡ç‹€æ…‹
    if batch_idx % 100 == 0:
        lr_mult = optimizer.c_optim.compute_contextual_lr_multiplier()
        is_edge = optimizer.c_optim.detect_edge_case()
        print(f"LR ä¹˜æ•¸: {lr_mult:.3f}, é‚Šç·£æƒ…æ³: {is_edge}")
```

### 4. å…¨åŠŸèƒ½ç‰ˆæœ¬ - Automagic_CameAMP_COptim8bit

```python
from library.automagic_cameamp import Automagic_CameAMP_COptim8bit

optimizer = Automagic_CameAMP_COptim8bit(
    model.parameters(),
    lr=1e-3,
    context_window=50,           # 8-bit ç‰ˆæœ¬å»ºè­°è¼ƒå°çš„çª—å£
    edge_threshold=0.8,
    adaptation_rate=0.2,
    verbose=True
)
```

## ğŸ“Š æ€§èƒ½æ¯”è¼ƒ

### è¨˜æ†¶é«”ä½¿ç”¨

| å„ªåŒ–å™¨ç‰ˆæœ¬ | è¨˜æ†¶é«”ä½¿ç”¨ç‡ | åƒæ•¸é‡æ”¯æ´ | é‡åŒ–é–‹éŠ· |
|------------|-------------|------------|----------|
| Automagic_CameAMP | 100% | æ¨™æº– | ç„¡ |
| Automagic_CameAMP8bit | ~25% | å¤§æ¨¡å‹ | è¼•å¾® |
| Automagic_CameAMP_COptim | 105% | æ¨™æº– | ç„¡ |
| Automagic_CameAMP_COptim8bit | ~30% | å¤§æ¨¡å‹ | è¼•å¾® |

### ç‰¹æ€§å°æ¯”

| ç‰¹æ€§ | åŸºç¤ç‰ˆ | 8-bitç‰ˆ | C-Optimç‰ˆ | å…¨åŠŸèƒ½ç‰ˆ |
|------|--------|---------|-----------|-----------|
| CAME ç®—æ³• | âœ… | âœ… | âœ… | âœ… |
| AGR æ­£å‰‡åŒ– | âœ… | âœ… | âœ… | âœ… |
| å‹•é‡åˆ‡æ› | âœ… | âœ… | âœ… | âœ… |
| å­¸ç¿’ç‡é®ç½© | âœ… | âœ… | âœ… | âœ… |
| 8-bit é‡åŒ– | âŒ | âœ… | âŒ | âœ… |
| ä¸Šä¸‹æ–‡æ„ŸçŸ¥ | âŒ | âŒ | âœ… | âœ… |
| å¤šå°ºåº¦å‹•é‡ | âŒ | âŒ | âœ… | âœ… |
| é‚Šç·£æª¢æ¸¬ | âŒ | âŒ | âœ… | âœ… |

## ğŸ¯ é©ç”¨å ´æ™¯

### 1. åŸºç¤ç‰ˆ (Automagic_CameAMP)
- **é©ç”¨**: ä¸€èˆ¬æ·±åº¦å­¸ç¿’è¨“ç·´
- **å„ªå‹¢**: ç©©å®šå¯é ï¼ŒåŠŸèƒ½å…¨é¢
- **å ´æ™¯**:
  - ä¸­å°å‹æ¨¡å‹è¨“ç·´
  - ç©©å®šæ€§å„ªå…ˆçš„å ´æ™¯
  - åˆæ¬¡ä½¿ç”¨å»ºè­°ç‰ˆæœ¬

### 2. 8-bit ç‰ˆ (Automagic_CameAMP8bit)
- **é©ç”¨**: è¨˜æ†¶é«”å—é™çš„å¤§æ¨¡å‹è¨“ç·´
- **å„ªå‹¢**: å¤§å¹…ç¯€çœè¨˜æ†¶é«”ï¼ˆ75%ï¼‰
- **å ´æ™¯**:
  - å¤§èªè¨€æ¨¡å‹å¾®èª¿
  - GPU è¨˜æ†¶é«”ä¸è¶³
  - æˆæœ¬æ•æ„Ÿçš„è¨“ç·´

### 3. ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç‰ˆ (Automagic_CameAMP_COptim)
- **é©ç”¨**: éœ€è¦æ™ºèƒ½èª¿æ•´çš„é«˜ç´šè¨“ç·´
- **å„ªå‹¢**: è‡ªé©æ‡‰å­¸ç¿’ç‡ï¼Œåœæ»¯æª¢æ¸¬
- **å ´æ™¯**:
  - è¤‡é›œçš„è¨“ç·´ä»»å‹™
  - éœ€è¦æœ€ä½³æ€§èƒ½
  - ç ”ç©¶å’Œå¯¦é©—

### 4. å…¨åŠŸèƒ½ç‰ˆ (Automagic_CameAMP_COptim8bit)
- **é©ç”¨**: å¤§æ¨¡å‹çš„é«˜ç´šè¨“ç·´
- **å„ªå‹¢**: çµåˆè¨˜æ†¶é«”æ•ˆç‡å’Œæ™ºèƒ½èª¿æ•´
- **å ´æ™¯**:
  - å¤§æ¨¡å‹çš„é«˜ç´šå¾®èª¿
  - è³‡æºå—é™ä½†éœ€è¦æœ€ä½³æ€§èƒ½
  - ç”Ÿç”¢ç’°å¢ƒæ¨è–¦

## âš¡ è¨“ç·´éšæ®µç‰¹æ€§

### æš–èº«éšæ®µ (0 ~ warmup_steps/2)
- **Torque-Aware Momentum**: æ‰­çŸ©æ„ŸçŸ¥å‹•é‡èª¿æ•´
- **Orthogonal Gradient**: æ­£äº¤æ¢¯åº¦æŠ•å½±
- **Adaptive Weight Decay**: è‡ªé©æ‡‰æ¬Šé‡è¡°æ¸›
- **å­¸ç¿’ç‡é®ç½©å»ºç«‹**: åƒæ•¸ç´šå­¸ç¿’ç‡å„ªåŒ–

### ä¸­æœŸéšæ®µ (warmup_steps/2 ~ warmup_steps)
- **Consistency Momentum**: åˆ‡æ›åˆ°ä¸€è‡´æ€§å‹•é‡
- **å­¸ç¿’ç‡é®ç½©ç©©å®š**: ç¹¼çºŒå„ªåŒ–å­¸ç¿’ç‡åˆ†å¸ƒ

### ç©©å®šéšæ®µ (warmup_steps+)
- **å…¨åŠŸèƒ½é‹è¡Œ**: æ‰€æœ‰å„ªåŒ–æŠ€è¡“å”åŒå·¥ä½œ
- **å‹•æ…‹èª¿æ•´**: åŸºæ–¼è¨“ç·´ç‹€æ…‹çš„å¯¦æ™‚å„ªåŒ–

## ğŸ” ç‹€æ…‹ç®¡ç†

### ç‹€æ…‹å­—å…¸çµæ§‹

```python
state = {
    'step': int,                    # ç•¶å‰æ­¥æ•¸
    'lr_mask': Tensor,              # å­¸ç¿’ç‡é®ç½©
    'avg_lr': float,                # å¹³å‡å­¸ç¿’ç‡
    'exp_avg': Tensor,              # ä¸€éšå‹•é‡
    'exp_avg_sq': Tensor,           # äºŒéšå‹•é‡ (CAME)
    'exp_avg_res': Tensor,          # AdaBelief æ®˜å·®
    's': Tensor,                    # TAM ç‹€æ…‹ (æš–èº«æœŸ)
    'last_polarity': Tensor,        # æ¢¯åº¦æ¥µæ€§æ­·å²
    'lr_max': float,                # æœ€å¤§å­¸ç¿’ç‡è¨˜éŒ„
    'row_scaling': Tensor,          # ALLoRA è¡Œç¸®æ”¾ (å¯é¸)
}
```

### 8-bit ç‰ˆæœ¬é¡å¤–ç‹€æ…‹

```python
# æ¯å€‹å¼µé‡å°æ‡‰çš„é‡åŒ–ç‹€æ…‹
state_8bit = {
    'lr_mask_q': Tensor,            # é‡åŒ–çš„å­¸ç¿’ç‡é®ç½©
    'lr_mask_q_scale': Tensor,      # é‡åŒ–æ¯”ä¾‹å› å­
    'exp_avg_q': Tensor,            # é‡åŒ–çš„å‹•é‡
    'exp_avg_q_scale': Tensor,      # å‹•é‡æ¯”ä¾‹å› å­
    # ... å…¶ä»–å¼µé‡çš„é‡åŒ–ç‰ˆæœ¬
}
```

### C-Optim ç‰ˆæœ¬é¡å¤–ç‹€æ…‹

```python
c_optim_state = {
    'c_optim_context': Dict,        # ä¸Šä¸‹æ–‡ä¿¡æ¯
    'edge_case_count': int,         # é‚Šç·£æƒ…æ³è¨ˆæ•¸
    'contextual_lr_multiplier': float,  # ä¸Šä¸‹æ–‡å­¸ç¿’ç‡ä¹˜æ•¸
    'momentum_scale_*': Tensor,     # å¤šå°ºåº¦å‹•é‡
    'momentum_count_*': int,        # å‹•é‡è¨ˆæ•¸å™¨
}
```

## ğŸ› ï¸ ç‹€æ…‹ä¿å­˜èˆ‡è¼‰å…¥

```python
# ä¿å­˜å„ªåŒ–å™¨ç‹€æ…‹
checkpoint = {
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
    'loss': loss,
}
torch.save(checkpoint, 'checkpoint.pth')

# è¼‰å…¥å„ªåŒ–å™¨ç‹€æ…‹
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# æª¢æŸ¥ç‰ˆæœ¬ç›¸å®¹æ€§
if 'magic_version' in checkpoint['optimizer_state_dict']:
    version = checkpoint['optimizer_state_dict']['magic_version']
    print(f"è¼‰å…¥çš„å„ªåŒ–å™¨ç‰ˆæœ¬: {version}")
```

## ğŸ’¡ æœ€ä½³å¯¦è¸

### 1. åƒæ•¸èª¿å„ªå»ºè­°

#### å­¸ç¿’ç‡è¨­å®š
```python
# å°æ¨¡å‹ (< 100M åƒæ•¸)
lr = 1e-3, warmup_steps = 500

# ä¸­ç­‰æ¨¡å‹ (100M - 1B åƒæ•¸)
lr = 5e-4, warmup_steps = 1000

# å¤§æ¨¡å‹ (> 1B åƒæ•¸)
lr = 1e-4, warmup_steps = 2000
```

#### è¨˜æ†¶é«”å„ªåŒ–
```python
# è¨˜æ†¶é«”å……è¶³
optimizer = Automagic_CameAMP_COptim(...)

# è¨˜æ†¶é«”å—é™
optimizer = Automagic_CameAMP_COptim8bit(
    ...,
    context_window=30,  # æ¸›å°‘ä¸Šä¸‹æ–‡çª—å£
)
```

### 2. ç›£æ§å»ºè­°

```python
# åŸºç¤ç›£æ§
if step % 100 == 0:
    avg_lr = optimizer._get_group_lr(optimizer.param_groups[0])
    print(f"å¹³å‡å­¸ç¿’ç‡: {avg_lr:.6f}")

# é«˜ç´šç›£æ§ (C-Optim ç‰ˆæœ¬)
if hasattr(optimizer, 'c_optim'):
    lr_mult = optimizer.c_optim.compute_contextual_lr_multiplier()
    is_edge = optimizer.c_optim.detect_edge_case()
    grad_consistency = optimizer.c_optim.compute_gradient_consistency()

    print(f"ä¸Šä¸‹æ–‡ä¹˜æ•¸: {lr_mult:.3f}")
    print(f"é‚Šç·£æƒ…æ³: {is_edge}")
    print(f"æ¢¯åº¦ä¸€è‡´æ€§: {grad_consistency:.3f}")
```

### 3. æ•…éšœæ’é™¤

#### å¸¸è¦‹å•é¡Œ
1. **8-bit åˆå§‹åŒ–å¤±æ•—**
   ```python
   # æª¢æŸ¥ bitsandbytes å®‰è£
   try:
       import bitsandbytes
       print(f"bitsandbytes ç‰ˆæœ¬: {bitsandbytes.__version__}")
   except ImportError:
       print("è«‹å®‰è£: pip install bitsandbytes")
   ```

2. **å­¸ç¿’ç‡éå°/éå¤§**
   ```python
   # æª¢æŸ¥å­¸ç¿’ç‡é®ç½©åˆ†å¸ƒ
   for group in optimizer.param_groups:
       for p in group['params']:
           if p.grad is not None:
               state = optimizer.state[p]
               if 'lr_mask' in state:
                   lr_mask = state['lr_mask']
                   print(f"LR ç¯„åœ: {lr_mask.min():.6f} - {lr_mask.max():.6f}")
   ```

3. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ•ˆæœä¸ä½³**
   ```python
   # èª¿æ•´ C-Optim åƒæ•¸
   optimizer = Automagic_CameAMP_COptim(
       ...,
       edge_threshold=0.8,    # é™ä½é–¾å€¼ï¼Œæ›´ç©æ¥µèª¿æ•´
       adaptation_rate=0.2,   # æé«˜é©æ‡‰é€Ÿç‡
       context_window=50,     # æ¸›å°‘çª—å£ï¼Œæ›´éˆæ•
   )
   ```

## ğŸ“š åƒè€ƒæ–‡ç»

1. [CAME: Confidence-guided Adaptive Memory Efficient Optimization](https://arxiv.org/pdf/2411.02853)
2. [Adaptive Gradient Regularization](https://arxiv.org/pdf/2407.16944)
3. [Torque-Aware Momentum](https://arxiv.org/abs/2412.18790)
4. [Consistency Phenomenon in Diffusion Models](https://arxiv.org/abs/2404.07946)
5. [Grams: Gradient Descent with Adaptive Momentum Scaling](https://arxiv.org/abs/2412.17107)
6. [Grokking at the Edge of Numerical Stability](https://arxiv.org/abs/2501.04697)
7. [AdaBelief Optimizer](https://arxiv.org/abs/2010.07468)
8. [ALLoRA: Adaptive Learning Rate Mitigates LoRA Fatal Flaws](https://arxiv.org/abs/2410.09692)
9. [Adaptive Weight Decay for Deep Neural Networks](https://arxiv.org/abs/1907.08931)
10. [Automagic Optimizer Implementation](https://github.com/ostris/ai-toolkit)

## ğŸ“„ æˆæ¬Š

æœ¬å¯¦ç¾åŸºæ–¼ç›¸é—œè«–æ–‡å’Œé–‹æºé …ç›®ï¼Œéµå¾ªå°æ‡‰çš„æˆæ¬Šæ¢æ¬¾ã€‚ä½¿ç”¨æ™‚è«‹ç¢ºä¿ç¬¦åˆç›¸é—œæˆæ¬Šè¦æ±‚ã€‚