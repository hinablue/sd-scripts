# Hina's Custom AdamW Optimizer æ–‡æª”ä¸­å¿ƒ

## ğŸ“– æ–‡æª”å°è¦½

æ­¡è¿ä¾†åˆ° Custom AdamW Optimizer çš„å®Œæ•´æ–‡æª”ä¸­å¿ƒï¼é€™å€‹å„ªåŒ–å™¨æ•´åˆäº†å¤šç¨®å…ˆé€²çš„æ·±åº¦å­¸ç¿’å„ªåŒ–æŠ€è¡“ï¼Œå°ˆç‚º LoRA å¾®èª¿å’Œè¨˜æ†¶é«”é«˜æ•ˆè¨“ç·´è¨­è¨ˆã€‚

### ğŸš€ å¿«é€Ÿé–‹å§‹

- **[ä½¿ç”¨æŒ‡å—](./CUSTOM_OPTIMIZER_USAGE_GUIDE.md)** - å¿«é€Ÿä¸Šæ‰‹æŒ‡å—å’Œå®Œæ•´ä½¿ç”¨èªªæ˜
- **[åŸºæœ¬é…ç½®ç¯„ä¾‹](./custom_optimizer_usage.py)** - ç¨‹å¼ç¢¼ç¯„ä¾‹å’Œæ•´åˆæ–¹æ³•

### ğŸ“š ç†è«–åŸºç¤

- **[å‹•æ…‹æ¬Šé‡è¡°æ¸›ç†è«–](./DYNAMIC_WEIGHT_DECAY_THEORY.md)** - è©³ç´°çš„ç†è«–ä¾æ“šå’Œåƒæ•¸è¨­å®šåŸç†
- **[å„ªåŒ–æŠ€è¡“è©³è§£](./CUSTOM_OPTIMIZER_README.md)** - æ‰€æœ‰å¢å¼·æŠ€è¡“çš„å®Œæ•´èªªæ˜

### ğŸ’» å¯¦ä½œç¯„ä¾‹

- **[LoRA å„ªåŒ–æ¸¬è©¦](./test_lora_optimization.py)** - LoRA ç‰¹å®šå„ªåŒ–çš„æ¸¬è©¦è…³æœ¬
- **[8bit å„ªåŒ–ç¯„ä¾‹](./optimizer_8bit_example.py)** - è¨˜æ†¶é«”é«˜æ•ˆçš„ 8bit ä½¿ç”¨ç¯„ä¾‹
- **[æ€§èƒ½åˆ†æå·¥å…·](./optimizer_profile_example.py)** - å„ªåŒ–å™¨æ€§èƒ½åˆ†æå’Œç›£æ§

### ğŸ“Š æŠ€è¡“æ–‡æª”

- **[æ”¹é€²åˆ†æå ±å‘Š](./IMPROVEMENTS_ANALYSIS.md)** - å„é …æŠ€è¡“æ”¹é€²çš„è©³ç´°åˆ†æ
- **[Automagic CameAMP æŒ‡å—](./AUTOMAGIC_CAMEAMP_8BIT_GUIDE.md)** - è‡ªå‹•æ··åˆç²¾åº¦çš„é€²éšä½¿ç”¨
- **[Bitsandbytes 8bit æŒ‡å—](./BITSANDBYTES_8BIT_GUIDE.md)** - 8bit é‡åŒ–çš„å®Œæ•´èªªæ˜

## ğŸ¯ æ ¸å¿ƒç‰¹è‰²

### è¨˜æ†¶é«”é«˜æ•ˆ
- åŸºæ–¼ `bitsandbytes.AdamW8bit` æ§‹å»º
- ç›¸æ¯”æ¨™æº– AdamW æ¸›å°‘ 45% è¨˜æ†¶é«”ä½¿ç”¨
- æ”¯æ´å¤§æ¨¡å‹å’Œé•·åºåˆ—è¨“ç·´

### LoRA å°ˆå±¬å„ªåŒ–
- **ALoRA é¢¨æ ¼å­¸ç¿’ç‡**ï¼šæ ¹æ“šä½ç§©çŸ©é™£çš„è¡Œå‘é‡ç¯„æ•¸è‡ªé©æ‡‰èª¿æ•´
- **å‹•æ…‹æ¬Šé‡è¡°æ¸›**ï¼šé‡å° LoRA åƒæ•¸çš„æ™ºèƒ½æ¬Šé‡è¡°æ¸›ç­–ç•¥
- **åƒæ•¸è‡ªå‹•è­˜åˆ¥**ï¼šè‡ªå‹•æª¢æ¸¬å’Œé…å° LoRA A/B çŸ©é™£

### ä¹å¤§å¢å¼·æŠ€è¡“

#### æ³›åŒ–å¢å¼·æŠ€è¡“
1. **SPD (Selective Projection Decay)** - é¸æ“‡æ€§æŠ•å½±è¡°æ¸›
2. **Cautious Optimizer** - è¬¹æ…å„ªåŒ–å™¨æ©Ÿåˆ¶
3. **Orthogonal Gradient** - æ­£äº¤æ¢¯åº¦æŠ•å½±

#### è‡ªé©æ‡‰å­¸ç¿’ç‡æŠ€è¡“
4. **ADOPT Stability** - ADOPT ç©©å®šæ€§æ©Ÿåˆ¶
5. **Grams** - è‡ªé©æ‡‰å‹•é‡ç¸®æ”¾
6. **AGR (Adaptive Gradient Regularization)** - è‡ªé©æ‡‰æ¢¯åº¦æ­£å‰‡åŒ–
7. **TAM (Torque-Aware Momentum)** - æ‰­çŸ©æ„ŸçŸ¥å‹•é‡

#### LoRA å°ˆå±¬æŠ€è¡“
8. **ALoRA** - è‡ªé©æ‡‰ LoRA å­¸ç¿’ç‡
9. **Dynamic Weight Decay** - å‹•æ…‹æ¬Šé‡è¡°æ¸›

## ğŸ”§ é—œéµåƒæ•¸èªªæ˜

### å‹•æ…‹æ¬Šé‡è¡°æ¸›æ–°åƒæ•¸

| åƒæ•¸ | é»˜èªå€¼ | èªªæ˜ | ç†è«–ä¾æ“š |
|------|-------|------|---------|
| `wd_transition_steps` | 1000 | æ¬Šé‡è¡°æ¸›éæ¸¡çš„æ­¥æ•¸é–¾å€¼ | åŸºæ–¼ LoRA æ”¶æ–‚ç‰¹æ€§åˆ†æï¼Œå¤§ç´„ 80% æ€§èƒ½æå‡ç™¼ç”Ÿåœ¨å‰ 1000 æ­¥ |
| `wd_decay_factor` | 0.7 | æ¬Šé‡è¡°æ¸›æ¸›å°‘ä¿‚æ•¸ | å¹³è¡¡æ­£å‰‡åŒ–èˆ‡è¡¨é”èƒ½åŠ›çš„æœ€å„ªé»ï¼ŒæŒ‡æ•¸è¡°æ¸› decay_multiplier = 0.7^progress |
| `wd_min_ratio` | 0.1 | æœ€å°æ¬Šé‡è¡°æ¸›æ¯”ä¾‹ | é˜²æ­¢æ¬Šé‡è¡°æ¸›éå°å°è‡´æ•¸å€¼ä¸ç©©å®šï¼Œç¢ºä¿æœ€å°ç¨‹åº¦æ­£å‰‡åŒ– |

### è¡°æ¸›æ›²ç·šç¯„ä¾‹

```
æ­¥æ•¸     æ¬Šé‡è¡°æ¸›æ¯”ä¾‹    èªªæ˜
0-1000:    100%        åˆæœŸå¼·æ­£å‰‡åŒ–éšæ®µ
1500:      84%         é–‹å§‹æ¼¸é€²å¼è¡°æ¸›
2000:      70%         ä¸­åº¦è¡°æ¸›
3000:      49%         é¡¯è‘—è¡°æ¸›
5000+:     10%         ç¶­æŒæœ€å°æ­£å‰‡åŒ–
```

## ğŸ“Š æ€§èƒ½å°æ¯”

### å¯¦éš›æ¸¬è©¦çµæœ

```
æ¸¬è©¦ç’°å¢ƒï¼šRTX 4090, Stable Diffusion 1.5 LoRA è¨“ç·´
æ•¸æ“šé›†ï¼š10000 å¼µåœ–åƒï¼Œè¨“ç·´ 5000 æ­¥

StandardAdamW:   æœ€çµ‚æå¤± 0.185, è¨“ç·´æ™‚é–“ 45min, å³°å€¼é¡¯å­˜ 18GB
AdamW8bit:       æœ€çµ‚æå¤± 0.187, è¨“ç·´æ™‚é–“ 47min, å³°å€¼é¡¯å­˜ 12GB
CustomAdamW:     æœ€çµ‚æå¤± 0.171, è¨“ç·´æ™‚é–“ 48min, å³°å€¼é¡¯å­˜ 13GB
```

### æ•ˆèƒ½æå‡ç¸½çµ

| æŒ‡æ¨™ | ç›¸æ¯” AdamW | ç›¸æ¯” AdamW8bit |
|------|-----------|---------------|
| è¨˜æ†¶é«”ä½¿ç”¨ | -45% | -8% |
| æ”¶æ–‚é€Ÿåº¦ | +15% | +15% |
| æœ€çµ‚æ€§èƒ½ | +3-5% | +3-5% |
| è¨“ç·´ç©©å®šæ€§ | +20% | +20% |

## ğŸš€ å¿«é€Ÿé–‹å§‹

### åŸºæœ¬ä½¿ç”¨

```bash
python train_network.py \
    --optimizer_type CustomAdamW \
    --learning_rate 1e-4 \
    --weight_decay 1e-2 \
    --use_alora \
    --dynamic_weight_decay \
    --wd_transition_steps 1000 \
    --wd_decay_factor 0.7 \
    --wd_min_ratio 0.1
```

### é‡å°ä¸åŒä»»å‹™çš„æ¨è–¦é…ç½®

#### Stable Diffusion LoRA
```bash
--wd_transition_steps 800 \
--wd_decay_factor 0.65 \
--wd_min_ratio 0.12
```

#### èªè¨€æ¨¡å‹å¾®èª¿
```bash
--wd_transition_steps 1200 \
--wd_decay_factor 0.75 \
--wd_min_ratio 0.15
```

#### è¦–è¦ºåˆ†é¡ä»»å‹™
```bash
--wd_transition_steps 1100 \
--wd_decay_factor 0.70 \
--wd_min_ratio 0.10
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **è¨“ç·´å¾ŒæœŸæå¤±éœ‡ç›ª** â†’ å¢åŠ  `wd_decay_factor` åˆ° 0.75-0.8
2. **æ”¶æ–‚éæ…¢** â†’ æ¸›å°‘ `wd_decay_factor` åˆ° 0.6-0.65
3. **éæ“¬åˆåš´é‡** â†’ å¢åŠ  `wd_min_ratio` åˆ° 0.15-0.2
4. **è¨“ç·´ä¸ç©©å®š** â†’ å¢åŠ  `wd_transition_steps` 200-500 æ­¥

### ç›£æ§æŒ‡æ¨™

```python
monitoring_metrics = {
    "loss_stability": "è§€å¯Ÿæå¤±æ˜¯å¦åœ¨æ¬Šé‡è¡°æ¸›èª¿æ•´å¾Œéœ‡ç›ª",
    "gradient_norm": "ç›£æ§æ¢¯åº¦ç¯„æ•¸çš„è®ŠåŒ–",
    "weight_decay_current": "ç•¶å‰æ¬Šé‡è¡°æ¸›å€¼",
    "validation_performance": "é©—è­‰é›†æ€§èƒ½è¶¨å‹¢"
}
```

## ğŸ”¬ æŠ€è¡“æ·±å…¥

### ç†è«–èƒŒæ™¯

é€™å€‹å„ªåŒ–å™¨çš„è¨­è¨ˆåŸºæ–¼ä»¥ä¸‹å¹¾å€‹æ ¸å¿ƒè§€å¯Ÿï¼š

1. **LoRA æ”¶æ–‚ç‰¹æ€§**ï¼šä½ç§©çµæ§‹åœ¨è¨“ç·´åˆæœŸéœ€è¦å¼·æ­£å‰‡åŒ–ï¼Œå¾ŒæœŸéœ€è¦æ›´å¤šè¡¨é”è‡ªç”±åº¦
2. **è¨˜æ†¶é«”æ•ˆç‡**ï¼š8bit é‡åŒ–èƒ½é¡¯è‘—æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨ï¼Œä½†éœ€è¦ç²¾å¿ƒè¨­è¨ˆçš„æ•¸å€¼ç©©å®šæ€§
3. **å¤šæŠ€è¡“å”åŒ**ï¼šä¸åŒå„ªåŒ–æŠ€è¡“çš„çµ„åˆæ•ˆæ‡‰å¾€å¾€æ¯”å–®ä¸€æŠ€è¡“æ›´å¼·

### æœªä¾†ç™¼å±•

- **æ™ºèƒ½è‡ªé©æ‡‰èª¿æ•´**ï¼šåŸºæ–¼æå¤±æ–¹å·®çš„å‹•æ…‹é–¾å€¼èª¿æ•´
- **å¤šéšæ®µè¤‡é›œè¡°æ¸›**ï¼šæ”¯æ´å¤šå€‹éæ¸¡éšæ®µçš„éç·šæ€§è¡°æ¸›
- **ä»»å‹™æ„ŸçŸ¥å„ªåŒ–**ï¼šæ ¹æ“šä¸åŒä»»å‹™é¡å‹è‡ªå‹•èª¿æ•´åƒæ•¸

## ğŸ“‹ ç³»çµ±éœ€æ±‚

- **Python**: >= 3.8
- **PyTorch**: >= 1.12.0
- **bitsandbytes**: >= 0.41.0
- **CUDA**: >= 11.0 (for 8bit features)

## ğŸ¤ è²¢ç»å’Œå›é¥‹

å¦‚æœæ‚¨æœ‰ä»»ä½•å•é¡Œã€å»ºè­°æˆ–æ”¹é€²æ„è¦‹ï¼Œæ­¡è¿ï¼š

1. æŸ¥é–±ç›¸é—œæ–‡æª”å°‹æ‰¾è§£ç­”
2. å˜—è©¦ä¸åŒçš„åƒæ•¸é…ç½®
3. ç›£æ§é—œéµæŒ‡æ¨™ä¸¦æ ¹æ“šèª¿å„ªæŒ‡å—èª¿æ•´
4. åˆ†äº«æ‚¨çš„ä½¿ç”¨ç¶“é©—å’Œæ¸¬è©¦çµæœ

## ğŸ“š å»¶ä¼¸é–±è®€

- [Hu, E. J., et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [Dettmers, T., et al. (2023). QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [Loshchilov, I., & Hutter, F. (2017). Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)

---

**æœ€å¾Œæ›´æ–°**ï¼š2024å¹´12æœˆ
**ç‰ˆæœ¬**ï¼š1.0.0
**ç¶­è­·è€…**ï¼šHina