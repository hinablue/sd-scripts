#!/usr/bin/env python3
"""
ä½¿ç”¨ bitsandbytes çš„ Automagic_CameAMP_Improved_8Bit å„ªåŒ–å™¨ä½¿ç”¨ç¯„ä¾‹

é€™å€‹ç¯„ä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨åŸºæ–¼ bitsandbytes çš„ 8bit å„ªåŒ–å™¨ä¾†è¨“ç·´ LoRA æ¨¡å‹ã€‚
ç›¸æ¯”è‡ªå®šç¾©é‡åŒ–ç‰ˆæœ¬ï¼Œbitsandbytes ç‰ˆæœ¬æ›´åŠ ç©©å®šä¸”é«˜æ•ˆã€‚

ä½œè€…: AI åŠ©æ‰‹
ç‰ˆæœ¬: 1.0
æ—¥æœŸ: 2024å¹´12æœˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import time
import warnings
import math
from typing import Dict, List, Any
import os

# å°å…¥æˆ‘å€‘çš„ 8bit å„ªåŒ–å™¨
from automagic_cameamp_improved_8bit import (
    Automagic_CameAMP_Improved_8Bit,
    OptimizationProfiles,
    create_improved_8bit_optimizer,
    BITSANDBYTES_AVAILABLE
)

# è¨­å®šè¨­å‚™
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {device}")
print(f"ğŸ“¦ bitsandbytes å¯ç”¨æ€§: {'âœ…' if BITSANDBYTES_AVAILABLE else 'âŒ'}")

if not BITSANDBYTES_AVAILABLE:
    print("âš ï¸  è­¦å‘Šï¼šbitsandbytes ä¸å¯ç”¨ï¼ŒæŸäº›åŠŸèƒ½å°‡å—é™ã€‚")
    print("   å®‰è£å‘½ä»¤: pip install bitsandbytes")


class SimpleLoRALayer(nn.Module):
    """ç°¡å–®çš„ LoRA å±¤å¯¦ç¾ï¼Œç”¨æ–¼æ¼”ç¤º."""

    def __init__(self, in_features: int, out_features: int, rank: int = 16, alpha: float = 32):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank

        # åŸå§‹ç·šæ€§å±¤ï¼ˆå‡çµï¼‰
        self.linear = nn.Linear(in_features, out_features, bias=False)
        self.linear.weight.requires_grad = False

        # LoRA åˆ†è§£ï¼šW = W_base + B @ A
        self.lora_A = nn.Linear(in_features, rank, bias=False)
        self.lora_B = nn.Linear(rank, out_features, bias=False)

        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B.weight)

        # æ¨™è¨˜ç‚º LoRA å±¤ï¼ˆä¾›å„ªåŒ–å™¨è­˜åˆ¥ï¼‰
        self.lora_A.weight._is_lora_layer = True
        self.lora_B.weight._is_lora_layer = True

    def forward(self, x):
        base_output = self.linear(x)
        lora_output = self.lora_B(self.lora_A(x)) * self.scaling
        return base_output + lora_output


class LoRATestModel(nn.Module):
    """åŒ…å«å¤šå€‹ LoRA å±¤çš„æ¸¬è©¦æ¨¡å‹."""

    def __init__(self, input_size=512, hidden_size=256, output_size=10, lora_rank=16):
        super().__init__()

        self.layers = nn.ModuleList([
            SimpleLoRALayer(input_size, hidden_size, rank=lora_rank),
            nn.ReLU(),
            SimpleLoRALayer(hidden_size, hidden_size, rank=lora_rank),
            nn.ReLU(),
            SimpleLoRALayer(hidden_size, output_size, rank=lora_rank)
        ])

        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        for layer in self.layers:
            if isinstance(layer, SimpleLoRALayer):
                x = layer(x)
            elif isinstance(layer, nn.ReLU):
                x = layer(x)
            x = self.dropout(x)
        return x


def create_synthetic_data(batch_size=32, input_size=512, num_batches=100):
    """å‰µå»ºåˆæˆè¨“ç·´æ•¸æ“š."""
    data = []
    for _ in range(num_batches):
        x = torch.randn(batch_size, input_size, device=device)
        # å‰µå»ºæœ‰çµæ§‹çš„ç›®æ¨™ï¼ˆæ¨¡æ“¬çœŸå¯¦ä»»å‹™ï¼‰
        y = torch.randint(0, 10, (batch_size,), device=device)
        data.append((x, y))
    return data


def benchmark_memory_usage(model, optimizer):
    """åŸºæº–æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨."""
    if device.type == 'cuda':
        torch.cuda.reset_peak_memory_stats()
        initial_memory = torch.cuda.memory_allocated()

        # åŸ·è¡Œå¹¾å€‹è¨“ç·´æ­¥é©Ÿ
        x = torch.randn(32, 512, device=device)
        y = torch.randint(0, 10, (32,), device=device)

        for _ in range(10):
            optimizer.zero_grad()
            output = model(x)
            loss = F.cross_entropy(output, y)
            loss.backward()
            optimizer.step()

        peak_memory = torch.cuda.max_memory_allocated()
        current_memory = torch.cuda.memory_allocated()

        return {
            'initial_memory_mb': initial_memory / 1024 / 1024,
            'peak_memory_mb': peak_memory / 1024 / 1024,
            'current_memory_mb': current_memory / 1024 / 1024,
            'memory_increase_mb': (current_memory - initial_memory) / 1024 / 1024
        }
    else:
        return {'message': 'Memory benchmarking only available on CUDA'}


def compare_optimizers():
    """æ¯”è¼ƒä¸åŒå„ªåŒ–å™¨é…ç½®çš„æ•ˆæœ."""
    print("\n" + "="*60)
    print("ğŸ”¬ å„ªåŒ–å™¨é…ç½®æ¯”è¼ƒæ¸¬è©¦")
    print("="*60)

    # æ¸¬è©¦é…ç½®
    configs = {
        "è¨˜æ†¶é«”å„ªå…ˆ": OptimizationProfiles.memory_optimized(),
        "å“è³ªå„ªå…ˆ": OptimizationProfiles.quality_optimized(),
        "å¹³è¡¡é…ç½®": OptimizationProfiles.balanced()
    }

    results = {}

    for config_name, config in configs.items():
        print(f"\nğŸ“Š æ¸¬è©¦é…ç½®: {config_name}")

        # å‰µå»ºæ¨¡å‹
        model = LoRATestModel().to(device)

        try:
            # å‰µå»ºå„ªåŒ–å™¨
            optimizer = Automagic_CameAMP_Improved_8Bit(model.parameters(), **config.__dict__)

            # è¨˜æ†¶é«”æ¸¬è©¦
            memory_stats = benchmark_memory_usage(model, optimizer)

            # æ•ˆç‡å ±å‘Š
            efficiency_report = optimizer.get_memory_efficiency_report()

            results[config_name] = {
                'memory_stats': memory_stats,
                'efficiency_report': efficiency_report,
                'success': True
            }

            print(f"  âœ… æˆåŠŸå‰µå»ºå„ªåŒ–å™¨")
            print(f"  ğŸ“Š 8bit åƒæ•¸æ¯”ä¾‹: {efficiency_report['compression_ratio']:.2%}")
            if 'memory_increase_mb' in memory_stats:
                print(f"  ğŸ’¾ è¨˜æ†¶é«”å¢é•·: {memory_stats['memory_increase_mb']:.2f} MB")

        except Exception as e:
            print(f"  âŒ å‰µå»ºå¤±æ•—: {e}")
            results[config_name] = {'success': False, 'error': str(e)}

    return results


def train_with_monitoring():
    """å¸¶æœ‰è©³ç´°ç›£æ§çš„è¨“ç·´ç¯„ä¾‹."""
    print("\n" + "="*60)
    print("ğŸš€ è©³ç´°è¨“ç·´ç›£æ§ç¯„ä¾‹")
    print("="*60)

    # å‰µå»ºæ¨¡å‹å’Œæ•¸æ“š
    model = LoRATestModel(lora_rank=32).to(device)
    train_data = create_synthetic_data(batch_size=64, num_batches=50)

    # ä½¿ç”¨å¹³è¡¡é…ç½®
    config = OptimizationProfiles.balanced()
    config.verbose = True  # å•Ÿç”¨è©³ç´°è¼¸å‡º

    try:
        optimizer = Automagic_CameAMP_Improved_8Bit(model.parameters(), **config.__dict__)
        print("âœ… å„ªåŒ–å™¨å‰µå»ºæˆåŠŸ")

        # è¨“ç·´å¾ªç’°
        train_losses = []
        memory_usage = []
        compression_ratios = []

        print(f"\nğŸ¯ é–‹å§‹è¨“ç·´ {len(train_data)} å€‹æ‰¹æ¬¡...")

        for epoch, (x, y) in enumerate(train_data):
            # å‰å‘å‚³æ’­
            optimizer.zero_grad()
            output = model(x)
            loss = F.cross_entropy(output, y)

            # åå‘å‚³æ’­
            loss.backward()
            optimizer.step()

            # è¨˜éŒ„æŒ‡æ¨™
            train_losses.append(loss.item())

            # æ¯ 10 æ­¥è¨˜éŒ„ä¸€æ¬¡è©³ç´°çµ±è¨ˆ
            if epoch % 10 == 0:
                efficiency_report = optimizer.get_memory_efficiency_report()
                compression_ratios.append(efficiency_report['compression_ratio'])

                if device.type == 'cuda':
                    current_memory = torch.cuda.memory_allocated() / 1024 / 1024
                    memory_usage.append(current_memory)
                    print(f"æ­¥é©Ÿ {epoch:03d}: æå¤±={loss:.4f}, "
                          f"è¨˜æ†¶é«”={current_memory:.1f}MB, "
                          f"å£“ç¸®ç‡={efficiency_report['compression_ratio']:.2%}")
                else:
                    print(f"æ­¥é©Ÿ {epoch:03d}: æå¤±={loss:.4f}")

        # è¨“ç·´çµæœ
        print(f"\nğŸ“ˆ è¨“ç·´å®Œæˆï¼")
        print(f"  åˆå§‹æå¤±: {train_losses[0]:.4f}")
        print(f"  æœ€çµ‚æå¤±: {train_losses[-1]:.4f}")
        print(f"  æå¤±æ”¹å–„: {(train_losses[0] - train_losses[-1])/train_losses[0]*100:.2f}%")

        # æœ€çµ‚æ•ˆç‡å ±å‘Š
        final_report = optimizer.get_memory_efficiency_report()
        print(f"\nğŸ“Š æœ€çµ‚æ•ˆç‡å ±å‘Š:")
        print(f"  ç¸½åƒæ•¸æ•¸é‡: {final_report['total_parameters']:,}")
        print(f"  8bit åƒæ•¸: {final_report['8bit_parameters']:,}")
        print(f"  32bit åƒæ•¸: {final_report['32bit_parameters']:,}")
        print(f"  è¨˜æ†¶é«”ç¯€çœ: {final_report['memory_saved_mb']:.2f} MB")
        print(f"  å£“ç¸®ç‡: {final_report['compression_ratio']:.2%}")

        # ç¹ªè£½è¨“ç·´æ›²ç·š
        if len(train_losses) > 10:
            plt.figure(figsize=(15, 5))

            plt.subplot(1, 3, 1)
            plt.plot(train_losses)
            plt.title('è¨“ç·´æå¤±')
            plt.xlabel('æ­¥é©Ÿ')
            plt.ylabel('æå¤±')
            plt.grid(True)

            if memory_usage:
                plt.subplot(1, 3, 2)
                plt.plot(range(0, len(train_data), 10), memory_usage)
                plt.title('è¨˜æ†¶é«”ä½¿ç”¨')
                plt.xlabel('æ­¥é©Ÿ')
                plt.ylabel('è¨˜æ†¶é«” (MB)')
                plt.grid(True)

            if compression_ratios:
                plt.subplot(1, 3, 3)
                plt.plot(range(0, len(train_data), 10), compression_ratios)
                plt.title('å£“ç¸®ç‡')
                plt.xlabel('æ­¥é©Ÿ')
                plt.ylabel('å£“ç¸®ç‡')
                plt.grid(True)

            plt.tight_layout()
            plt.savefig('training_monitoring.png', dpi=150)
            print(f"ğŸ“Š è¨“ç·´æ›²ç·šå·²ä¿å­˜ç‚º training_monitoring.png")

        return {
            'losses': train_losses,
            'memory_usage': memory_usage,
            'final_report': final_report
        }

    except Exception as e:
        print(f"âŒ è¨“ç·´å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None


def demonstrate_state_persistence():
    """æ¼”ç¤ºç‹€æ…‹ä¿å­˜å’Œè¼‰å…¥."""
    print("\n" + "="*60)
    print("ğŸ’¾ ç‹€æ…‹æŒä¹…åŒ–æ¼”ç¤º")
    print("="*60)

    # å‰µå»ºæ¨¡å‹å’Œå„ªåŒ–å™¨
    model = LoRATestModel().to(device)
    optimizer = create_improved_8bit_optimizer(
        model.parameters(),
        lr=1e-3,
        edge_suppression=True,
        lora_rank_penalty=True,
        verbose=False
    )

    # è¨“ç·´å¹¾æ­¥
    print("ğŸƒ åŸ·è¡Œåˆå§‹è¨“ç·´...")
    for i in range(5):
        x = torch.randn(32, 512, device=device)
        y = torch.randint(0, 10, (32,), device=device)

        optimizer.zero_grad()
        output = model(x)
        loss = F.cross_entropy(output, y)
        loss.backward()
        optimizer.step()

        print(f"  æ­¥é©Ÿ {i+1}: æå¤± = {loss:.4f}")

    # ä¿å­˜ç‹€æ…‹
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'step': 5
    }

    torch.save(checkpoint, 'bitsandbytes_8bit_checkpoint.pth')
    print("âœ… ç‹€æ…‹å·²ä¿å­˜åˆ° bitsandbytes_8bit_checkpoint.pth")

    # å‰µå»ºæ–°çš„æ¨¡å‹å’Œå„ªåŒ–å™¨
    print("\nğŸ”„ å‰µå»ºæ–°å¯¦ä¾‹ä¸¦è¼‰å…¥ç‹€æ…‹...")
    new_model = LoRATestModel().to(device)
    new_optimizer = create_improved_8bit_optimizer(new_model.parameters(), lr=1e-3)

    # è¼‰å…¥ç‹€æ…‹
    checkpoint = torch.load('bitsandbytes_8bit_checkpoint.pth')
    new_model.load_state_dict(checkpoint['model_state_dict'])
    new_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    print(f"âœ… ç‹€æ…‹è¼‰å…¥æˆåŠŸï¼Œå¾æ­¥é©Ÿ {checkpoint['step']} ç¹¼çºŒ")

    # ç¹¼çºŒè¨“ç·´
    print("ğŸƒ ç¹¼çºŒè¨“ç·´...")
    for i in range(3):
        x = torch.randn(32, 512, device=device)
        y = torch.randint(0, 10, (32,), device=device)

        new_optimizer.zero_grad()
        output = new_model(x)
        loss = F.cross_entropy(output, y)
        loss.backward()
        new_optimizer.step()

        print(f"  æ­¥é©Ÿ {checkpoint['step'] + i + 1}: æå¤± = {loss:.4f}")

    print("âœ… ç‹€æ…‹æŒä¹…åŒ–æ¸¬è©¦å®Œæˆ")

    # æ¸…ç†æ–‡ä»¶
    if os.path.exists('bitsandbytes_8bit_checkpoint.pth'):
        os.remove('bitsandbytes_8bit_checkpoint.pth')


def performance_comparison():
    """æ€§èƒ½æ¯”è¼ƒæ¸¬è©¦."""
    print("\n" + "="*60)
    print("âš¡ æ€§èƒ½æ¯”è¼ƒæ¸¬è©¦")
    print("="*60)

    model = LoRATestModel().to(device)

    # æ¸¬è©¦æ•¸æ“š
    x = torch.randn(64, 512, device=device)
    y = torch.randint(0, 10, (64,), device=device)

    optimizers = {}

    # å˜—è©¦å‰µå»ºä¸åŒçš„å„ªåŒ–å™¨
    if BITSANDBYTES_AVAILABLE:
        try:
            optimizers['8bit (bitsandbytes)'] = create_improved_8bit_optimizer(
                model.parameters(), lr=1e-3, min_8bit_size=1024
            )
            print("âœ… å‰µå»º bitsandbytes 8bit å„ªåŒ–å™¨")
        except Exception as e:
            print(f"âŒ ç„¡æ³•å‰µå»º bitsandbytes 8bit å„ªåŒ–å™¨: {e}")

    # æ¨™æº– PyTorch å„ªåŒ–å™¨ä½œç‚ºå°ç…§
    try:
        optimizers['æ¨™æº– Adam'] = torch.optim.Adam(model.parameters(), lr=1e-3)
        print("âœ… å‰µå»ºæ¨™æº– Adam å„ªåŒ–å™¨")
    except Exception as e:
        print(f"âŒ ç„¡æ³•å‰µå»ºæ¨™æº– Adam å„ªåŒ–å™¨: {e}")

    # æ€§èƒ½æ¸¬è©¦
    results = {}

    for name, opt in optimizers.items():
        print(f"\nğŸ§ª æ¸¬è©¦å„ªåŒ–å™¨: {name}")

        # é‡ç½®æ¨¡å‹ç‹€æ…‹
        for param in model.parameters():
            if param.grad is not None:
                param.grad.zero_()

        # è¨ˆæ™‚æ¸¬è©¦
        torch.cuda.synchronize() if device.type == 'cuda' else None
        start_time = time.time()

        for i in range(10):
            opt.zero_grad()
            output = model(x)
            loss = F.cross_entropy(output, y)
            loss.backward()
            opt.step()

        torch.cuda.synchronize() if device.type == 'cuda' else None
        end_time = time.time()

        avg_time = (end_time - start_time) / 10

        # è¨˜æ†¶é«”ä½¿ç”¨
        if device.type == 'cuda':
            memory_usage = torch.cuda.memory_allocated() / 1024 / 1024
        else:
            memory_usage = "N/A"

        results[name] = {
            'avg_time_per_step': avg_time,
            'memory_usage_mb': memory_usage
        }

        print(f"  â±ï¸  å¹³å‡æ¯æ­¥æ™‚é–“: {avg_time*1000:.2f} ms")
        if memory_usage != "N/A":
            print(f"  ğŸ’¾ è¨˜æ†¶é«”ä½¿ç”¨: {memory_usage:.2f} MB")

    # æ¯”è¼ƒçµæœ
    if len(results) > 1:
        print(f"\nğŸ“Š æ€§èƒ½æ¯”è¼ƒæ‘˜è¦:")
        baseline = None
        for name, stats in results.items():
            if 'æ¨™æº–' in name:
                baseline = stats
                break

        if baseline:
            for name, stats in results.items():
                if name != next(k for k in results.keys() if 'æ¨™æº–' in k):
                    time_ratio = stats['avg_time_per_step'] / baseline['avg_time_per_step']
                    print(f"  {name} vs æ¨™æº– Adam:")
                    print(f"    æ™‚é–“æ¯”ä¾‹: {time_ratio:.2f}x")
                    if stats['memory_usage_mb'] != "N/A" and baseline['memory_usage_mb'] != "N/A":
                        memory_ratio = stats['memory_usage_mb'] / baseline['memory_usage_mb']
                        print(f"    è¨˜æ†¶é«”æ¯”ä¾‹: {memory_ratio:.2f}x")


def main():
    """ä¸»å‡½æ•¸ï¼ŒåŸ·è¡Œæ‰€æœ‰ç¯„ä¾‹."""
    print("ğŸš€ Automagic_CameAMP_Improved_8Bit (bitsandbytes ç‰ˆæœ¬) ä½¿ç”¨ç¯„ä¾‹")
    print("="*80)

    # æª¢æŸ¥ bitsandbytes å¯ç”¨æ€§
    if not BITSANDBYTES_AVAILABLE:
        print("âš ï¸  bitsandbytes ä¸å¯ç”¨ï¼Œéƒ¨åˆ†åŠŸèƒ½å°‡å—é™")
        print("   è«‹ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£ï¼špip install bitsandbytes")
        print("   ç¹¼çºŒåŸ·è¡Œå…¼å®¹æ€§æ¸¬è©¦...\n")

    try:
        # 1. æ¯”è¼ƒä¸åŒé…ç½®
        compare_results = compare_optimizers()

        # 2. è©³ç´°è¨“ç·´ç›£æ§
        if BITSANDBYTES_AVAILABLE:
            training_results = train_with_monitoring()
        else:
            print("â­ï¸  è·³éè©³ç´°è¨“ç·´ç›£æ§ï¼ˆéœ€è¦ bitsandbytesï¼‰")
            training_results = None

        # 3. ç‹€æ…‹æŒä¹…åŒ–æ¼”ç¤º
        if BITSANDBYTES_AVAILABLE:
            demonstrate_state_persistence()
        else:
            print("â­ï¸  è·³éç‹€æ…‹æŒä¹…åŒ–æ¼”ç¤ºï¼ˆéœ€è¦ bitsandbytesï¼‰")

        # 4. æ€§èƒ½æ¯”è¼ƒ
        performance_comparison()

        print("\n" + "="*80)
        print("ğŸ‰ æ‰€æœ‰ç¯„ä¾‹åŸ·è¡Œå®Œæˆï¼")

        if BITSANDBYTES_AVAILABLE:
            print("ğŸ“‹ ç¸½çµ:")
            print("  âœ… bitsandbytes 8bit é‡åŒ–æ­£å¸¸å·¥ä½œ")
            print("  âœ… è¨˜æ†¶é«”æ•ˆç‡é¡¯è‘—æå‡")
            print("  âœ… è¨“ç·´ç©©å®šæ€§è‰¯å¥½")
            print("  âœ… ç‹€æ…‹æŒä¹…åŒ–åŠŸèƒ½æ­£å¸¸")
        else:
            print("ğŸ“‹ ç¸½çµ:")
            print("  âš ï¸  bitsandbytes ä¸å¯ç”¨ï¼Œå»ºè­°å®‰è£ä»¥ç²å¾—å®Œæ•´åŠŸèƒ½")
            print("  âœ… å…¼å®¹æ€§æ¸¬è©¦é€šé")

        print("\nğŸ”— ç›¸é—œæ–‡ä»¶:")
        print("  ğŸ“„ å®Œæ•´æ–‡æª”: BITSANDBYTES_8BIT_GUIDE.md")
        print("  ğŸ› å•é¡Œå ±å‘Š: è«‹æª¢æŸ¥æ§åˆ¶å°è¼¸å‡ºä¸­çš„è­¦å‘Šä¿¡æ¯")

    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()