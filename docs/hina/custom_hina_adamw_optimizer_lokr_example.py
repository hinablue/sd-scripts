"""
HinaAdamWOptimizer LoKr æ”¯æ´ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨ LoKr è¨“ç·´ä¸­ä½¿ç”¨å¢å¼·å„ªåŒ–å™¨
"""

import torch
import torch.nn as nn
from custom_hina_adamw_optimizer import HinaAdamWOptimizer

class MockLoKrLayer(nn.Module):
    """
    æ¨¡æ“¬ LoKr å±¤ï¼Œç”¨æ–¼æ¸¬è©¦å„ªåŒ–å™¨çš„ LoKr æ”¯æ´

    LoKr ä½¿ç”¨ Kronecker ç©åˆ†è§£ï¼šW = W0 + (B1 âŠ— B2)(A1 âŠ— A2)
    """
    def __init__(self, in_features, out_features, rank=16):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.rank = rank

        # åŸå§‹æ¬Šé‡ï¼ˆå‡çµï¼‰
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.weight.requires_grad = False

        # LoKr åƒæ•¸ - ä½¿ç”¨ä¸åŒçš„å‘½åæ¨¡å¼ä¾†æ¸¬è©¦è­˜åˆ¥åŠŸèƒ½
        self.lokr_w1_a = nn.Parameter(torch.randn(rank, in_features))
        self.lokr_w1_b = nn.Parameter(torch.randn(out_features, rank))
        self.lokr_w2_a = nn.Parameter(torch.randn(rank, rank))
        self.lokr_w2_b = nn.Parameter(torch.randn(rank, rank))

        # ç‚ºåƒæ•¸è¨­ç½®åç¨±å±¬æ€§ï¼Œæ¨¡æ“¬å¯¦éš›è¨“ç·´æ¡†æ¶çš„è¡Œç‚º
        self.lokr_w1_a.param_name = "test_layer.lokr_w1_a.weight"
        self.lokr_w1_b.param_name = "test_layer.lokr_w1_b.weight"
        self.lokr_w2_a.param_name = "test_layer.lokr_w2_a.weight"
        self.lokr_w2_b.param_name = "test_layer.lokr_w2_b.weight"

        self.scaling = 0.1

    def forward(self, x):
        # è¨ˆç®— Kronecker ç©è¿‘ä¼¼
        # é€™æ˜¯ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš› LoKr å¯¦ç¾æœƒæ›´è¤‡é›œ
        w1_product = torch.matmul(self.lokr_w1_b, self.lokr_w1_a)
        w2_product = torch.matmul(self.lokr_w2_b, self.lokr_w2_a)

        # ç°¡åŒ–çš„ Kronecker ç©æ•ˆæœ
        delta_w = w1_product + w2_product * 0.5  # ç°¡åŒ–çµ„åˆ

        # åŸå§‹è¼¸å‡º + LoKr å¢é‡
        output = torch.nn.functional.linear(x, self.weight)
        delta_output = torch.nn.functional.linear(x, delta_w * self.scaling)

        return output + delta_output

class AlternativeLoKrLayer(nn.Module):
    """
    ä½¿ç”¨æ›¿ä»£å‘½åæ¨¡å¼çš„ LoKr å±¤ï¼Œæ¸¬è©¦ä¸åŒçš„åƒæ•¸å‘½åè­˜åˆ¥
    """
    def __init__(self, in_features, out_features, rank=8):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features

        # ä½¿ç”¨ä¸åŒçš„å‘½åæ¨¡å¼
        self.lokr_w1 = nn.Parameter(torch.randn(rank, in_features))
        self.lokr_w2 = nn.Parameter(torch.randn(out_features, rank))

        # è¨­ç½®åƒæ•¸åç¨±
        self.lokr_w1.param_name = "alt_layer.lokr.w1.weight"
        self.lokr_w2.param_name = "alt_layer.lokr.w2.weight"

        self.scaling = 0.05

    def forward(self, x):
        # ç°¡å–®çš„ä½ç§©è¿‘ä¼¼
        intermediate = torch.nn.functional.linear(x, self.lokr_w1.T)
        output = torch.nn.functional.linear(intermediate, self.lokr_w2) * self.scaling
        return output

def test_lokr_support():
    """æ¸¬è©¦ LoKr æ”¯æ´åŠŸèƒ½"""
    print("=== HinaAdamWOptimizer LoKr æ”¯æ´æ¸¬è©¦ ===\n")

    # å‰µå»ºæ¸¬è©¦æ¨¡å‹
    model = nn.Sequential(
        MockLoKrLayer(512, 256, rank=32),
        nn.ReLU(),
        AlternativeLoKrLayer(256, 128, rank=16),
        nn.ReLU(),
        nn.Linear(128, 10)  # æ™®é€šå±¤
    )

    # å‰µå»ºå„ªåŒ–å™¨
    optimizer = HinaAdamWOptimizer(
        model.parameters(),
        lr=1e-3,
        use_alora=True,  # å•Ÿç”¨ ALoRAï¼ˆç¾åœ¨æ”¯æ´ LoKrï¼‰
        alora_ratio=18.0,  # LoKr å»ºè­°ä½¿ç”¨ç•¥ä½çš„æ¯”ä¾‹
        dynamic_weight_decay=True,
        wd_transition_steps=500,  # LoKr å»ºè­°è¼ƒå¿«çš„éæ¸¡
        wd_decay_factor=0.75,     # è¼ƒæº«å’Œçš„è¡°æ¸›
        wd_min_ratio=0.15,        # ä¿æŒæ›´é«˜çš„æœ€å°æ¬Šé‡è¡°æ¸›
        use_spd=True,
        use_cautious=True,
        verbose=True
    )

    # ç²å–å„ªåŒ–å™¨è³‡è¨Š
    opt_info = optimizer.get_optimization_info()

    print("ğŸ“Š å„ªåŒ–å™¨è³‡è¨Š:")
    print(f"  ç¸½åƒæ•¸æ•¸: {opt_info['total_params']}")
    print(f"  å„ªåŒ–å™¨é¡å‹: {opt_info['optimizer_type']}")

    # LoRA çµ±è¨ˆ
    lora_stats = opt_info['lora_stats']
    print(f"\nğŸ“ˆ LoRA åƒæ•¸çµ±è¨ˆ:")
    print(f"  LoRA A åƒæ•¸: {lora_stats['lora_a_params']}")
    print(f"  LoRA B åƒæ•¸: {lora_stats['lora_b_params']}")
    print(f"  LoRA é…å°: {lora_stats['lora_pairs']}")

    # LoKr çµ±è¨ˆ
    lokr_stats = opt_info['lokr_stats']
    print(f"\nğŸ”· LoKr åƒæ•¸çµ±è¨ˆ:")
    print(f"  LoKr W1 åƒæ•¸: {lokr_stats['lokr_w1_params']}")
    print(f"  LoKr W2 åƒæ•¸: {lokr_stats['lokr_w2_params']}")
    print(f"  LoKr W1A åƒæ•¸: {lokr_stats['lokr_w1_a_params']}")
    print(f"  LoKr W1B åƒæ•¸: {lokr_stats['lokr_w1_b_params']}")
    print(f"  LoKr W2A åƒæ•¸: {lokr_stats['lokr_w2_a_params']}")
    print(f"  LoKr W2B åƒæ•¸: {lokr_stats['lokr_w2_b_params']}")
    print(f"  LoKr é…å°: {lokr_stats['lokr_pairs']}")
    print(f"  LoKr çµ„åˆ¥: {lokr_stats['lokr_groups']}")

    # åŠŸèƒ½ç‰¹æ€§
    features = opt_info['features']
    print(f"\nğŸš€ å•Ÿç”¨çš„åŠŸèƒ½:")
    for feature, enabled in features.items():
        status = "âœ…" if enabled else "âŒ"
        print(f"  {status} {feature}: {enabled}")

    # ç°¡å–®çš„è¨“ç·´æ¨¡æ“¬
    print(f"\nğŸ‹ï¸ é–‹å§‹è¨“ç·´æ¨¡æ“¬...")

    criterion = nn.CrossEntropyLoss()
    for step in range(10):
        # æ¨¡æ“¬æ‰¹æ¬¡æ•¸æ“š
        x = torch.randn(8, 512)
        y = torch.randint(0, 10, (8,))

        # å‰å‘å‚³æ’­
        optimizer.zero_grad()
        output = model(x)
        loss = criterion(output, y)

        # åå‘å‚³æ’­å’Œå„ªåŒ–
        loss.backward()
        optimizer.step()

        if step % 5 == 0:
            print(f"  Step {step}: Loss = {loss:.4f}")

    print(f"\nâœ… LoKr æ”¯æ´æ¸¬è©¦å®Œæˆï¼")
    print(f"ğŸ“‹ ç¸½çµ:")
    print(f"  - æˆåŠŸè­˜åˆ¥ {lokr_stats['lokr_groups']} å€‹ LoKr çµ„åˆ¥")
    print(f"  - æª¢æ¸¬åˆ° {lokr_stats['lokr_pairs']} å€‹ LoKr åƒæ•¸é…å°")
    print(f"  - æ‡‰ç”¨äº†å°ˆé–€é‡å° LoKr çš„å­¸ç¿’ç‡ç¸®æ”¾å’Œæ¬Šé‡è¡°æ¸›ç­–ç•¥")

    return True

def test_parameter_classification():
    """æ¸¬è©¦åƒæ•¸åˆ†é¡åŠŸèƒ½"""
    print("\n=== åƒæ•¸åˆ†é¡æ¸¬è©¦ ===")

    # å‰µå»ºä¸€å€‹è™›æ“¬å„ªåŒ–å™¨ä¾†æ¸¬è©¦åˆ†é¡åŠŸèƒ½
    dummy_param = torch.randn(10, 10, requires_grad=True)
    optimizer = HinaAdamWOptimizer([dummy_param])

    # æ¸¬è©¦å„ç¨®åƒæ•¸åç¨±
    test_names = [
        ("layer.lora_down.weight", "lora_a"),
        ("layer.lora_up.weight", "lora_b"),
        ("layer.lokr_w1_a.weight", "lokr_w1_a"),
        ("layer.lokr_w1_b.weight", "lokr_w1_b"),
        ("layer.lokr_w2_a.weight", "lokr_w2_a"),
        ("layer.lokr_w2_b.weight", "lokr_w2_b"),
        ("layer.lokr.w1.weight", "lokr_w1"),
        ("layer.lokr.w2.weight", "lokr_w2"),
        ("layer.weight", "regular"),
        ("layer.bias", "regular"),
    ]

    print("ğŸ” åƒæ•¸åç¨±åˆ†é¡çµæœ:")
    for name, expected in test_names:
        result = optimizer._classify_parameter(name)
        status = "âœ…" if result == expected else "âŒ"
        print(f"  {status} '{name}' -> '{result}' (æœŸæœ›: '{expected}')")

    return True

if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    try:
        test_parameter_classification()
        test_lokr_support()
        print(f"\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼HinaAdamWOptimizer ç¾åœ¨å®Œå…¨æ”¯æ´ LoKr è¨“ç·´ã€‚")
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
        raise