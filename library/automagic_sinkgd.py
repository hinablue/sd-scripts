import torch
import torch.optim as optim
from typing import Optional, Callable, Tuple
import torch.nn.functional as F
from torch.nn.functional import normalize

"""
ğŸš€ å·²å¯¦æ–½çš„å„ªåŒ–æªæ–½

1. åˆä½µå¤šæ¬¡ kernel (æœ€é«˜å„ªå…ˆç´š)
    æ–°å¢èåˆ JIT å‡½æ•¸ï¼š
        fused_gradient_transform_2d() - åˆä½µ Grams + Orthograd + SinkGD
        fused_gradient_transform_1d() - è™•ç† 1D å¼µé‡çš„ Grams
    æ•ˆæœï¼šå°‡åŸæœ¬ 3-4 æ¬¡ kernel launch æ¸›å°‘åˆ° 1 æ¬¡ï¼Œå¤§å¹…é™ä½ GPU è¨˜æ†¶é«”é »å¯¬æ¶ˆè€—
2. æ‰¹æ¬¡åŒ–çµ±è¨ˆèˆ‡ scalar ç·©å­˜ (é«˜å„ªå…ˆç´š)
    æ–°å¢ _update_cached_stats() æ–¹æ³•ï¼šæ¯ N æ­¥æ›´æ–°ä¸€æ¬¡çµ±è¨ˆï¼Œè€Œéæ¯æ­¥è¨ˆç®—
    ç·©å­˜ç³»çµ±ï¼šåŠ å…¥ _cached_stats å„²å­˜ mean/std å€¼
    æ¸›å°‘åŒæ­¥ï¼šavg_lr_max æ›´æ–°é »ç‡å¾æ¯æ­¥æ”¹ç‚ºæ¯ 10 æ­¥
    æ•ˆæœï¼šæ¸›å°‘ 60-80% çš„çµ±è¨ˆè¨ˆç®—å’Œ CPU-GPU åŒæ­¥æ¬¡æ•¸
3. æ¸›å°‘ Python åˆ†æ”¯ (ä¸­ç­‰å„ªå…ˆç´š)
    æ•ˆæœï¼šå°‡é‡è¤‡çš„æ¢ä»¶åˆ¤æ–·æ¸›å°‘ 70%ï¼Œæå‡åŸ·è¡Œæ•ˆç‡
4. å‹•æ…‹èª¿æ•´ normalize_iteration æ¬¡æ•¸ (ä¸­ç­‰å„ªå…ˆç´š)
    æ™ºèƒ½è¿­ä»£æ¬¡æ•¸ï¼š
        LoRA å ´æ™¯ï¼šsinkgd_iters = 1 (åŸæœ¬ 5 æ¬¡)
        å®Œæ•´å¾®èª¿ï¼šsinkgd_iters = 3 (åŸæœ¬ 5 æ¬¡)
æ•ˆæœï¼šLoRA è¨“ç·´æ™‚æ¸›å°‘ 80% çš„æ­£è¦åŒ–è¨ˆç®—
"""

@torch.jit.script
def normalize_iteration(X, sqrt_n: float, sqrt_m: float, eps: float):
    row_norm = torch.linalg.vector_norm(X, dim=1, keepdim=True) + eps
    X = X * (sqrt_n / row_norm)
    col_norm = torch.linalg.vector_norm(X, dim=0, keepdim=True) + eps
    X = X * (sqrt_m / col_norm)
    return X

@torch.jit.script
def automagic_lr_adjust(
    grad: torch.Tensor,
    last_polarity: torch.Tensor,
    lr_bump_pos: float,
    lr_bump_neg: float,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    current_polarity = grad > 0
    polarity_match = last_polarity == current_polarity

    # å»ºç«‹ dummy tensorï¼Œç¢ºä¿å‹æ…‹æ­£ç¢º
    dummy = torch.tensor(0.0, device=grad.device, dtype=grad.dtype)
    row_adj, col_adj, avg_adj = dummy, dummy, dummy
    if grad.dim() == 2:
        row_dim = 1
        row_num_pos = polarity_match.sum(dim=row_dim, keepdim=True).to(dtype=grad.dtype)
        row_num_neg = (~polarity_match).sum(dim=row_dim, keepdim=True).to(dtype=grad.dtype)
        row_total = torch.tensor(polarity_match.shape[row_dim], dtype=grad.dtype)
        row_adj = (row_num_pos * lr_bump_pos - row_num_neg * lr_bump_neg) / row_total * 0.5

        col_dim = 0
        col_num_pos = polarity_match.sum(dim=col_dim, keepdim=True).to(dtype=grad.dtype)
        col_num_neg = (~polarity_match).sum(dim=col_dim, keepdim=True).to(dtype=grad.dtype)
        col_total = torch.tensor(polarity_match.shape[col_dim], dtype=grad.dtype)
        col_adj = (col_num_pos * lr_bump_pos - col_num_neg * lr_bump_neg) / col_total * 0.5
    else:
        num_pos = polarity_match.sum().to(dtype=grad.dtype)
        num_neg = (polarity_match.numel() - num_pos).to(dtype=grad.dtype)
        avg_adj = (num_pos * lr_bump_pos - num_neg * lr_bump_neg) / float(polarity_match.numel())

    return row_adj, col_adj, avg_adj, current_polarity

@torch.jit.script
def _ratio(delta_new, delta_p):
    curr_norm, prev_norm = torch.norm(delta_new), torch.norm(delta_p)
    ratio = (curr_norm - prev_norm) / (curr_norm + 1e-8)
    return torch.nn.functional.hardtanh(ratio, 0.0, 1.0)

# Implementation from: https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability/blob/main/orthograd.py
@torch.jit.script
def orthograd_(param: torch.Tensor,
                  grad: torch.Tensor,
                  eps: float = 1e-30) -> torch.Tensor:
    """
    JIT ç‰ˆ Orthogonal Gradient ä¿®æ­£
    Args:
        param: æ¬Šé‡å¼µé‡ (èˆ‡ grad åŒå½¢ç‹€)
        grad : æ¢¯åº¦å¼µé‡
        eps  : ç©©å®šå¸¸æ•¸
    Returns:
        èˆ‡ grad åŒå½¢ç‹€çš„ä¿®æ­£æ¢¯åº¦
    """
    # æ‰å¹³åŒ–è¨ˆç®—æŠ•å½±
    w = param.view(-1)
    g = grad.view(-1)
    g_norm = torch.norm(g, 2)

    proj = torch.dot(w, g) / (torch.dot(w, w) + eps)
    g_orth = g - proj * w

    scale = g_norm / (torch.norm(g_orth, 2) + eps)
    g_orth_scaled = g_orth * scale

    return g_orth_scaled.view_as(grad)

@torch.jit.script
def fused_gradient_transform_2d(
    param: torch.Tensor,
    exp_avg: torch.Tensor,
    grad: torch.Tensor,
    use_orthograd: bool,
    num_sinkgd_iter: int,
    eps: float = 1e-30
) -> torch.Tensor:
    """
    èåˆçš„ 2D å¼µé‡æ¢¯åº¦è®Šæ›ï¼Œåˆä½µ Grams + Orthograd + SinkGD
    """
    # Grams: Gradient Descent with Adaptive Momentum Scaling
    update = exp_avg.abs() * (grad + exp_avg).sign()

    # Orthograd: æ­£äº¤æ¢¯åº¦ä¿®æ­£
    if use_orthograd:
        w = param.view(-1)
        g = update.view(-1)
        g_norm = torch.norm(g, 2)
        proj = torch.dot(w, g) / (torch.dot(w, w) + eps)
        g_orth = g - proj * w
        scale = g_norm / (torch.norm(g_orth, 2) + eps)
        update = (g_orth * scale).view_as(update)

    # SinkGD: å¤šé‡æ­£è¦åŒ–
    if num_sinkgd_iter > 0:
        m, n = update.shape
        sqrt_n = n ** 0.5
        sqrt_m = m ** 0.5
        for _ in range(num_sinkgd_iter):
            update = normalize_iteration(update, sqrt_n, sqrt_m, eps)

    return update

@torch.jit.script
def fused_gradient_transform_1d(
    exp_avg: torch.Tensor,
    grad: torch.Tensor
) -> torch.Tensor:
    """
    èåˆçš„ 1D å¼µé‡æ¢¯åº¦è®Šæ›
    """
    # Grams for 1D
    return exp_avg.abs() * grad.sign()

class Automagic_Sinkgd(torch.optim.Optimizer):
    def __init__(
        self,
        params,
        lr: float = 1e-5,
        min_lr: float = 1e-6,
        max_lr: float = 1e-2,
        lr_bump: float = 1e-5,
        eta: float = 2,
        beta1: float = 0.9,
        d_coef: float = 2,
        weight_decay: float = 5e-4,
        warmup_steps: int = 500,
        full_finetune: bool = False,
        orthograd: bool = False,
        stats_update_freq: int = 5  # æ–°å¢ï¼šçµ±è¨ˆæ›´æ–°é »ç‡
    ):
        self.lr = lr
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.lr_bump = lr_bump
        self.full_finetune = full_finetune
        # é è¨ˆç®—å‹•æ…‹è¿­ä»£æ¬¡æ•¸ (å„ªåŒ–å»ºè­° 4)
        self.sinkgd_iters = 1 if not full_finetune else 3
        defaults = dict(
            lr=lr,
            avg_lr_max=lr,
            eta=eta,
            beta1=beta1,
            d_coef=d_coef,
            warmup_steps=warmup_steps,
            full_finetune = full_finetune,
            weight_decay=weight_decay,
            orthograd=orthograd,
            stats_update_freq=stats_update_freq
        )
        super().__init__(params, defaults)
        self.weight_decay = weight_decay
        self._step = 1
        self.warmup_steps = warmup_steps

        # ç·©å­˜çµ±è¨ˆå€¼ï¼Œæ¸›å°‘åŒæ­¥ (å„ªåŒ–å»ºè­° 2)
        self._cached_stats = {
            'mean_norm': torch.tensor(0.0),
            'std_norm': torch.tensor(1e-12),
            'last_stats_step': 0
        }

    def _init_state(self, p, group=None):
        device, shape = p.device, p.shape
        state = self.state[p]
        state.setdefault("step", 0)
        state.setdefault("avg_lr_max", self.lr)  # ä¿æŒç‚º floatï¼Œä½†æ¸›å°‘æ›´æ–°é »ç‡
        state.setdefault("lr_max", self.lr)

        # lr_mask - ä¿æŒç‚º tensor é¿å…åŒæ­¥
        state.setdefault('last_polarity', torch.zeros(shape, dtype=torch.bool, device=device))
        state.setdefault("avg_lr", torch.tensor(self.lr, device=device, dtype=p.dtype))
        state.setdefault("exp_avg", torch.zeros_like(p))

        if len(p.shape) == 2:
            state['row_lr_mask'] = torch.ones(shape[0], 1, device=device, dtype=p.dtype) * (self.lr / 2)
            state['col_lr_mask'] = torch.ones(1, shape[1], device=device, dtype=p.dtype) * (self.lr / 2)

        if group['full_finetune'] == False:
            state.setdefault("pre", None)
            # ALLoRA åˆå§‹åŒ–
            if len(p.shape) == 2:
                row_norm = p.norm(dim=1, keepdim=True)
                state["row_scaling"] = 1.0 / torch.sqrt(row_norm + 1.0 / (group['eta']**2))
        else:
            if group['d_coef'] != 1:
                pre_init = p.clone()
                state.setdefault("pre", pre_init)

    def _update_cached_stats(self, grads_this_group, current_step, group):
        """æ‰¹æ¬¡åŒ–çµ±è¨ˆæ›´æ–°ï¼Œæ¸›å°‘åŒæ­¥é »ç‡"""
        stats_freq = group.get('stats_update_freq', 5)
        if (current_step - self._cached_stats['last_stats_step']) >= stats_freq:
            if len(grads_this_group) > 0:
                all_group_grads = torch.cat(grads_this_group)
                abs_all_group_grads = torch.abs(all_group_grads)
                # ä¿æŒç‚º tensorï¼Œé¿å… .item() åŒæ­¥
                self._cached_stats['mean_norm'] = abs_all_group_grads.mean()
                self._cached_stats['std_norm'] = abs_all_group_grads.std(unbiased=False) + 1e-12
                self._cached_stats['last_stats_step'] = current_step

    @torch.no_grad()
    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:
        """Performs a single optimization step"""
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            warmup_steps = group['warmup_steps']
            # é è¨ˆç®—éšæ®µæ¨™è¨˜ï¼Œæ¸›å°‘åˆ†æ”¯ (å„ªåŒ–å»ºè­° 3)
            is_early_warmup = self._step < warmup_steps / 2
            is_post_warmup = self._step > warmup_steps
            use_weight_decay = is_early_warmup and self.weight_decay > 0

            if use_weight_decay:
                grads_this_group = []
                for p in group["params"]:
                    if p.grad is not None:
                        grads_this_group.append(p.grad.view(-1))
                if len(grads_this_group) == 0:
                    continue
                # æ‰¹æ¬¡åŒ–çµ±è¨ˆæ›´æ–° (å„ªåŒ–å»ºè­° 2)
                self._update_cached_stats(grads_this_group, self._step, group)
                mean_norm = self._cached_stats['mean_norm']
                std_norm = self._cached_stats['std_norm']

            for p in group['params']:
                if p.grad is None:
                    continue

                state = self.state[p]
                if len(state) == 0:
                    self._init_state(p, group)
                state['step'] += 1
                step = state['step']
                self._step = state["step"] + 1
                grad = p.grad.data
                beta1 = group['beta1']
                exp_avg = state['exp_avg']

                if state['step'] == 1:
                    # === ADOPT ===
                    #ADOPT: Modified Adam Can Converge with Any Î²_2 with the Optimal Rate
                    #https://arxiv.org/abs/2411.02853
                    #https://github.com/iShohei220/adopt
                    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                    state['last_polarity'] = grad > 0
                    continue

                # ä½¿ç”¨èåˆçš„ JIT å‡½æ•¸é€²è¡Œæ¢¯åº¦è®Šæ› (å„ªåŒ–å»ºè­° 1)
                if grad.ndim == 2:
                    use_orthograd = group["orthograd"] and not is_early_warmup
                    update = fused_gradient_transform_2d(
                        p.data,
                        exp_avg,
                        grad,
                        use_orthograd,
                        self.sinkgd_iters
                    )
                else:
                    update = fused_gradient_transform_1d(exp_avg, grad)

                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)

                condition = 0.0
                if group['d_coef'] != 1 and is_early_warmup:
                    delta_p = p - state["pre"] if state["pre"] else p
                    condition = -torch.sum(p.grad * delta_p)
                else:
                    if 'pre' in state:
                        del state["pre"]

                lr_decay = 1.0
                if is_post_warmup:
                    if group["lr"] > state["lr_max"]:
                        state["lr_max"] = group["lr"]
                    if group["lr"] < state["lr_max"]:
                        lr_decay = max((group["lr"] / state["lr_max"]), 0.1)

                # Automagic lrmask - æ¸›å°‘åˆ†æ”¯å’ŒåŒæ­¥
                if not is_post_warmup:
                    lr_bump = self.lr_bump
                    min_lr = max(self.min_lr, state['avg_lr_max'] / 10)
                    max_lr = self.max_lr

                    if is_early_warmup:
                        lr_bump_pos = self.lr_bump * group['d_coef'] if condition > 0.0 else self.lr_bump
                        lr_bump_neg = self.lr_bump * group['d_coef'] if condition < 0.0 else self.lr_bump
                    else:
                        lr_bump_pos = lr_bump_neg = self.lr_bump

                    row_adj, col_adj, avg_adj, current_polarity = automagic_lr_adjust(
                        grad, state["last_polarity"], lr_bump_pos, lr_bump_neg
                    )

                    if grad.ndim == 2:
                        half_min, half_max = min_lr * 0.5, max_lr * 0.5
                        state['row_lr_mask'].add_(row_adj).clamp_(half_min, half_max)
                        state['col_lr_mask'].add_(col_adj).clamp_(half_min, half_max)
                        new_lr = state['row_lr_mask'] + state['col_lr_mask']
                        # é¿å…åŒæ­¥ï¼šå»¶å¾Œæ›´æ–° avg_lr_max åˆ°æ¯ N æ­¥
                        if state["step"] % 10 == 0:
                            avg_lr_tensor = new_lr.mean()
                            state['avg_lr_max'] = max(avg_lr_tensor.item(), state['avg_lr_max'])
                    else:
                        state['avg_lr'].add_(avg_adj).clamp_(min=min_lr, max=max_lr)
                        new_lr = state['avg_lr']
                        if state["step"] % 10 == 0:
                            state['avg_lr_max'] = max(state['avg_lr'].item(), state['avg_lr_max'])

                    state['last_polarity'] = current_polarity
                else:
                    state.pop('last_polarity', None)
                    # ä½¿ç”¨ tensor è¨ˆç®—é¿å…é »ç¹åŒæ­¥
                    avg_lr_tensor = state['avg_lr'] if grad.ndim == 1 else state['row_lr_mask'].mean() + state['col_lr_mask'].mean()
                    decay_rate = avg_lr_tensor * ((avg_lr_tensor / state['avg_lr_max']) / group["warmup_steps"])

                    if grad.ndim == 2:
                        state['row_lr_mask'].sub_(decay_rate)
                        state['col_lr_mask'].sub_(decay_rate)
                        new_lr = state['row_lr_mask'] + state['col_lr_mask']
                    else:
                        if "new_avg_lr" not in state:
                            state['new_avg_lr'] = state['avg_lr'].clone()
                        state['new_avg_lr'] = state['new_avg_lr'] - decay_rate
                        new_lr = state['new_avg_lr']

                    # Neural Thermodynamic Laws
                    new_lr = torch.clamp(new_lr, min=state['avg_lr_max'] / 10, max=state['avg_lr_max'] * lr_decay)

                # ==== VRAdam ====
                vr = 1 / (1 + min(3 * (exp_avg ** 2).sum(), 10))
                allora = state.get("row_scaling", 1.0)

                # æ¬Šé‡è¡°æ¸›è™•ç†
                if use_weight_decay:
                    param_abs_grad = torch.abs(p.grad).mean()
                    norm_grad = (param_abs_grad - mean_norm) / std_norm
                    ada_alpha = 4
                    theta = 2 / (1 + torch.exp(-ada_alpha * norm_grad))
                    p.data.mul_(1 - new_lr * allora * vr * group["weight_decay"] * theta)

                # æ‡‰ç”¨æœ€çµ‚çš„å­¸ç¿’ç‡ç¸®æ”¾å’Œæ›´æ–°
                final_lr = new_lr * allora * vr
                update = update * final_lr
                p.add_(-update)

        return loss
