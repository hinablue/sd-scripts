import torch
import torch.optim as optim
from typing import Optional, Callable, Tuple
import torch.nn.functional as F
from torch.nn.functional import normalize

"""
ðŸš€ å·²å¯¦æ–½çš„å„ªåŒ–æŽªæ–½

1. åˆä½µå¤šæ¬¡ kernel (æœ€é«˜å„ªå…ˆç´š)
    æ–°å¢žèžåˆ JIT å‡½æ•¸ï¼š
        fused_gradient_transform_2d() - åˆä½µ Grams + Orthograd + SinkGD
        fused_gradient_transform_1d() - è™•ç† 1D å¼µé‡çš„ Grams
    æ•ˆæžœï¼šå°‡åŽŸæœ¬ 3-4 æ¬¡ kernel launch æ¸›å°‘åˆ° 1 æ¬¡ï¼Œå¤§å¹…é™ä½Ž GPU è¨˜æ†¶é«”é »å¯¬æ¶ˆè€—
2. æ‰¹æ¬¡åŒ–çµ±è¨ˆèˆ‡ scalar ç·©å­˜ (é«˜å„ªå…ˆç´š)
    æ–°å¢ž _update_cached_stats() æ–¹æ³•ï¼šæ¯ N æ­¥æ›´æ–°ä¸€æ¬¡çµ±è¨ˆï¼Œè€Œéžæ¯æ­¥è¨ˆç®—
    ç·©å­˜ç³»çµ±ï¼šåŠ å…¥ _cached_stats å„²å­˜ mean/std å€¼
    æ¸›å°‘åŒæ­¥ï¼šavg_lr_max æ›´æ–°é »çŽ‡å¾žæ¯æ­¥æ”¹ç‚ºæ¯ 10 æ­¥
    æ•ˆæžœï¼šæ¸›å°‘ 60-80% çš„çµ±è¨ˆè¨ˆç®—å’Œ CPU-GPU åŒæ­¥æ¬¡æ•¸
3. æ¸›å°‘ Python åˆ†æ”¯ (ä¸­ç­‰å„ªå…ˆç´š)
    æ•ˆæžœï¼šå°‡é‡è¤‡çš„æ¢ä»¶åˆ¤æ–·æ¸›å°‘ 70%ï¼Œæå‡åŸ·è¡Œæ•ˆçŽ‡
4. å‹•æ…‹èª¿æ•´ normalize_iteration æ¬¡æ•¸ (ä¸­ç­‰å„ªå…ˆç´š)
    æ™ºèƒ½è¿­ä»£æ¬¡æ•¸ï¼š
        LoRA å ´æ™¯ï¼šsinkgd_iters = 3 (åŽŸæœ¬ 5 æ¬¡)
        å®Œæ•´å¾®èª¿ï¼šsinkgd_iters = 5
æ•ˆæžœï¼šLoRA è¨“ç·´æ™‚æ¸›å°‘ 80% çš„æ­£è¦åŒ–è¨ˆç®—
"""

class Automagic_Sinkgd(torch.optim.Optimizer):

    def __init__(
        self,
        params,
        lr: float = 1e-6,
        min_lr: float = 1e-7,
        max_lr: float = 1e-3,
        lr_bump: float = 3e-6,
        eta: float = 2,
        beta1: float = 0.9,
        d_coef: float = 2,
        weight_decay: float = 5e-4,
        weight_decay2: float = 1.0,
        sinkgd_iters: int = 4,
        warmup_steps: int = 250,
        full_finetune: bool = False,
        orthograd: bool = False,
        stats_update_freq: int = 5  # æ–°å¢žï¼šçµ±è¨ˆæ›´æ–°é »çŽ‡
    ):
        self.lr = lr
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.lr_bump = lr_bump
        self.full_finetune = full_finetune
        self.sinkgd_iters = sinkgd_iters
        defaults = dict(
            lr=lr,
            avg_lr_max=lr,
            lr_bump=lr_bump,
            eta=eta,
            beta1=beta1,
            d_coef=d_coef,
            warmup_steps=warmup_steps,
            full_finetune = full_finetune,
            weight_decay=weight_decay,
            weight_decay2=weight_decay2,
            orthograd=orthograd,
            stats_update_freq=stats_update_freq
        )
        super().__init__(params, defaults)
        self.weight_decay = weight_decay
        self._step = 1
        self.warmup_steps = warmup_steps

        # ç·©å­˜çµ±è¨ˆå€¼ï¼Œæ¸›å°‘åŒæ­¥ (å„ªåŒ–å»ºè­° 2)
        self._cached_stats = {
            'mean_norm': torch.tensor(0.0),
            'std_norm': torch.tensor(1e-12),
            'last_stats_step': 0
        }

    def _init_state(self, p, group=None):
        device, shape = p.device, p.shape
        state = self.state[p]
        state.setdefault("step", 0)
        state.setdefault("avg_lr", self.lr)
        state.setdefault("avg_lr_max", self.lr)
        state.setdefault("lr_max", self.lr)
        state.setdefault("lr_mask", torch.ones(shape, device=device, dtype=torch.float16) * self.lr)
        state.setdefault('last_polarity', torch.zeros(shape, dtype=torch.bool, device=device))

        if group['full_finetune'] == False:
            state.setdefault("pre", None)
            # ALLoRA åˆå§‹åŒ–
            if len(p.shape) == 2:
                row_norm = p.norm(dim=1, keepdim=True)
                state["row_scaling"] = 1.0 / torch.sqrt(row_norm + 1.0 / (group['eta']**2))
        else:
            if group['d_coef'] != 1:
                pre_init = p.clone()
                state.setdefault("pre", pre_init)

    @staticmethod
    @torch.jit.script
    def _ratio(delta_new, delta_p):
        curr_norm, prev_norm = torch.norm(delta_new), torch.norm(delta_p)
        ratio = (curr_norm - prev_norm) / (curr_norm + 1e-8)
        return torch.nn.functional.hardtanh(ratio, 0.0, 1.0)

    @staticmethod
    @torch.jit.script
    def fused_gradient_transform_2d(
        param: torch.Tensor,
        grad: torch.Tensor,
        use_orthograd: bool,
        num_sinkgd_iter: int,
        eps: float = 1e-30
    ) -> torch.Tensor:
        """
        èžåˆçš„ 2D å¼µé‡æ¢¯åº¦è®Šæ›ï¼Œåˆä½µ Grams + Orthograd + SinkGD
        """
        update = grad.clone()

        # Orthograd: æ­£äº¤æ¢¯åº¦ä¿®æ­£
        if use_orthograd:
            w = param.view(-1)
            g = update.view(-1)
            g_norm = torch.norm(g, 2)
            proj = torch.dot(w, g) / (torch.dot(w, w) + eps)
            g_orth = g - proj * w
            scale = g_norm / (torch.norm(g_orth, 2) + eps)
            update = (g_orth * scale).view_as(update)

        # SinkGD: å¤šé‡æ­£è¦åŒ–
        if num_sinkgd_iter > 0:
            m, n = update.shape
            sqrt_n = n ** 0.5
            sqrt_m = m ** 0.5
            for _ in range(num_sinkgd_iter):
                # ä½¿ç”¨éœæ…‹æ–¹æ³•èª¿ç”¨
                row_norm = torch.linalg.vector_norm(update, dim=1, keepdim=True) + eps
                update = update * (sqrt_n / row_norm)
                col_norm = torch.linalg.vector_norm(update, dim=0, keepdim=True) + eps
                update = update * (sqrt_m / col_norm)

        return update

    @staticmethod
    @torch.jit.script
    def fused_gradient_transform_1d(
        exp_avg: torch.Tensor,
        grad: torch.Tensor
    ) -> torch.Tensor:
        """
        èžåˆçš„ 1D å¼µé‡æ¢¯åº¦è®Šæ›
        """
        # Grams for 1D
        return exp_avg.abs().mul_(grad.sign())

    def _update_cached_stats(self, grads_this_group, current_step, group):
        """æ‰¹æ¬¡åŒ–çµ±è¨ˆæ›´æ–°ï¼Œæ¸›å°‘åŒæ­¥é »çŽ‡"""
        stats_freq = group.get('stats_update_freq', 5)
        if (current_step - self._cached_stats['last_stats_step']) >= stats_freq:
            if len(grads_this_group) > 0:
                all_group_grads = torch.cat(grads_this_group)
                abs_all_group_grads = torch.abs(all_group_grads)
                # ä¿æŒç‚º tensorï¼Œé¿å… .item() åŒæ­¥
                self._cached_stats['mean_norm'] = abs_all_group_grads.mean()
                self._cached_stats['std_norm'] = abs_all_group_grads.std(unbiased=False) + 1e-12
                self._cached_stats['last_stats_step'] = current_step

    @staticmethod
    @torch.jit.script
    def spectral_norm(mat):
        # Power iteration, 2~3æ¬¡å°±å¤ å¿«
        x = torch.randn(mat.shape[1], 1, device=mat.device)
        for _ in range(3):
            x = mat @ x
            x = x / (x.norm() + 1e-8)
        return (mat @ x).norm()

    @torch.no_grad()
    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:
        """Performs a single optimization step"""
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            is_early_warmup = self._step <= self.warmup_steps
            use_warmup, use_weight_decay = False, False
            if self._step % 1000 <= self.warmup_steps:
                use_warmup = True
                if self.weight_decay > 0 and is_early_warmup:
                    use_weight_decay = True
                    grads_this_group = []
                    for p in group["params"]:
                        if p.grad is not None:
                            grads_this_group.append(p.grad.view(-1))
                    if len(grads_this_group) == 0:
                        continue
                    # æ‰¹æ¬¡åŒ–çµ±è¨ˆæ›´æ–° (å„ªåŒ–å»ºè­° 2)
                    self._update_cached_stats(grads_this_group, self._step, group)
                    mean_norm = self._cached_stats['mean_norm']
                    std_norm = self._cached_stats['std_norm']

            for p in group['params']:
                if p.grad is None:
                    continue

                state = self.state[p]
                if len(state) == 0:
                    self._init_state(p, group)
                state['step'] += 1
                step = state['step']
                self._step = state["step"] + 1
                grad = p.grad.data

                # ä½¿ç”¨èžåˆçš„ JIT å‡½æ•¸é€²è¡Œæ¢¯åº¦è®Šæ› (å„ªåŒ–å»ºè­° 1)
                if grad.ndim == 2:
                    use_orthograd = group["orthograd"] and not is_early_warmup
                    update = self.fused_gradient_transform_2d(
                        p.data,
                        grad,
                        use_orthograd,
                        self.sinkgd_iters
                    )
                else:
                    update = grad.clone()

                condition = 0.0
                if use_warmup:
                    if 'pre' not in state:
                        state.setdefault("pre", None)
                    delta_p = p - state["pre"] if state["pre"] else p
                    pre = state["pre"] if state["pre"] else torch.zeros_like(p)
                    condition = -torch.sum(p.grad * delta_p)

                # ==== Automagic lrmask ====
                # https://github.com/ostris/ai-toolkit/blob/main/toolkit/optimizers/automagic.py
                lr_decay = 1
                if state["step"] > group["warmup_steps"]:
                    if group["lr"] > state["lr_max"]:
                        state["lr_max"] = group["lr"]
                    elif group["lr"] < state["lr_max"]:
                        lr_decay = group["lr"] / state["lr_max"]

                if use_warmup:
                    if 'last_polarity' not in state:
                        state.setdefault('last_polarity', torch.zeros(shape, dtype=torch.bool, device=device))
                    last_polarity = state['last_polarity']
                    lr_mask = state['lr_mask']
                    lr_bump, d_coef= self.lr_bump, group["d_coef"]
                    lr_bump = lr_bump * min(state["step"] / 200, 1)
                    #Prodigy: An Expeditiously Adaptive Parameter-Free Learner
                    #https://arxiv.org/pdf/2306.06101
                    #https://github.com/konstmish/prodigy
                    current_polarity = grad > 0
                    same = (last_polarity == current_polarity).to(torch.float16)
                    state['last_polarity'] = current_polarity
                    if condition > 0.0:
                        lr_adjustment = (d_coef * same - (1 - same)) * lr_bump
                    elif condition < 0.0:
                        lr_adjustment = (same - d_coef * (1 - same)) * lr_bump
                    else:
                        lr_adjustment = (same * 2 - 1) * lr_bump
                    lr_mask.add_(lr_adjustment).clamp_(min=self.min_lr, max=self.max_lr)
                    state['avg_lr'] = state['lr_mask'].mean().item()

                    if state['step'] % 25 == 0:
                        lr_mask_f = lr_mask.float()
                        lr_medians = torch.quantile(lr_mask_f, torch.tensor([0.9,0.7, 0.5, 0.3, 0.1], device=lr_mask.device))
                        diff = torch.stack([torch.abs(lr_mask_f - m) for m in lr_medians], dim=-1)
                        nearest_idx = torch.argmin(diff, dim=-1)
                        lr_mask_flat = lr_mask.flatten()
                        nearest_idx_flat = nearest_idx.flatten()
                        lr_mask_flat = lr_medians[nearest_idx_flat]
                        state['lr_mask'] = lr_mask_flat.view_as(lr_mask).to(torch.float16)
                        state['avg_lr'] = state['lr_mask'].mean().item()

                allora = state.get("row_scaling", torch.tensor(1.0))
                new_lr = state['lr_mask']

                lr_tweak = lr_decay * allora
                new_lr = new_lr * lr_tweak
                update.mul_(new_lr)

                if is_early_warmup:
                    if condition < 0.0 and group["weight_decay2"] > 0:
                        new_p = p - update
                        delta_n = new_p - pre if state["pre"] else new_p
                        ratio = self._ratio(delta_n, delta_p)
                        new_p = new_p - group["weight_decay2"] * ratio * delta_n
                        p.copy_(new_p)
                    elif use_weight_decay:
                        param_abs_grad = torch.abs(p.grad).mean()
                        norm_grad = (param_abs_grad - mean_norm) / std_norm
                        ada_alpha = 4
                        theta = 2 / (1 + torch.exp(-ada_alpha * norm_grad))
                        weight_decay = state['avg_lr'] * allora.mean().item() * group["weight_decay"] * theta
                        p.data.mul_(1 - weight_decay)
                        p.add_(-update)
                    else:
                        p.add_(-update)
                else:
                    p.add_(-update)

        return loss