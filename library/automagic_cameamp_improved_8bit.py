import torch
from typing import List, Dict, Any, Optional, Tuple
from torch.nn.functional import normalize
from dataclasses import dataclass
import math
import warnings

# å˜—è©¦å°å…¥ bitsandbytes
try:
    import bitsandbytes as bnb
    import bitsandbytes.functional as F
    BITSANDBYTES_AVAILABLE = True
except ImportError:
    BITSANDBYTES_AVAILABLE = False
    warnings.warn("bitsandbytes æœªå®‰è£ï¼Œ8bit åŠŸèƒ½å°‡ä¸å¯ç”¨ã€‚è«‹ä½¿ç”¨ 'pip install bitsandbytes' å®‰è£ã€‚")

@dataclass
class Improved8BitOptimizerConfig:
    """æ”¹é€²ç‰ˆ 8bit å„ªåŒ–å™¨é…ç½®ï¼Œå°ˆé–€é‡å° LoRA è¨“ç·´å„ªåŒ–."""
    lr: float = 1e-6
    min_lr: float = 1e-7
    max_lr: float = 1e-3
    lr_bump: float = 3e-6
    eps: Tuple[float, float, float] = (1e-30, 1e-16, 1e-8)
    clip_threshold: float = 1.0
    betas: Tuple[float, float, float] = (0.8, 0.99, 0.999)
    eta: float = 2.0
    beta1_decay: float = 0.9995
    weight_decay: float = 5e-4
    warmup_steps: int = 500
    context_window: int = 30
    edge_threshold: float = 0.6
    adaptation_rate: float = 0.25
    came: bool = True
    full_finetune: bool = False
    verbose: bool = False

    # é‚Šç·£å’ŒèƒŒæ™¯éæ“¬åˆæ§åˆ¶åƒæ•¸
    edge_suppression: bool = True
    edge_penalty: float = 0.1
    background_regularization: bool = True
    spatial_awareness: bool = True
    frequency_penalty: float = 0.05
    detail_preservation: float = 0.8

    # LoRA ç‰¹å®šå„ªåŒ–åƒæ•¸
    lora_rank_penalty: bool = True
    rank_penalty_strength: float = 0.01
    low_rank_emphasis: float = 1.2

    # 8bit é‡åŒ–åƒæ•¸
    optim_bits: int = 8
    percentile_clipping: int = 100
    block_wise: bool = True
    min_8bit_size: int = 4096  # å°æ–¼æ­¤å¤§å°çš„å¼µé‡ä¸é€²è¡Œ 8bit é‡åŒ–
    stable_emb: bool = False

    # è¨˜æ†¶é«”å„ªåŒ–åƒæ•¸
    fused_adam: bool = False  # æ˜¯å¦ä½¿ç”¨èåˆçš„ Adam æ“ä½œ
    force_8bit: bool = False  # å¼·åˆ¶æ‰€æœ‰ç‹€æ…‹ä½¿ç”¨ 8bit


class BitsAndBytesOptimized(torch.optim.Optimizer):
    """ä½¿ç”¨ bitsandbytes çš„æ”¹é€²ç‰ˆå„ªåŒ–å™¨åŸºé¡."""

    def __init__(self, params, config: Improved8BitOptimizerConfig):
        if not BITSANDBYTES_AVAILABLE:
            raise ImportError(
                "bitsandbytes ä¸å¯ç”¨ã€‚è«‹å®‰è£ bitsandbytesï¼š\n"
                "pip install bitsandbytes\n"
                "æˆ–è¨­å®š force_8bit=False ä½¿ç”¨ 32bit ç‰ˆæœ¬ã€‚"
            )

        self.config = config
        eta_value = float(config.eta) if isinstance(config.eta, (int, float)) else 2.0

        defaults = dict(
            lr=config.lr,
            eps=config.eps,
            clip_threshold=config.clip_threshold,
            betas=config.betas,
            eta=eta_value,
            beta1_decay=config.beta1_decay,
            weight_decay=config.weight_decay,
            warmup_steps=config.warmup_steps,
            context_window=config.context_window,
            edge_threshold=config.edge_threshold,
            adaptation_rate=config.adaptation_rate,
            came=config.came,
            full_finetune=config.full_finetune,
            edge_suppression=config.edge_suppression,
            edge_penalty=config.edge_penalty,
            background_regularization=config.background_regularization,
            spatial_awareness=config.spatial_awareness,
            frequency_penalty=config.frequency_penalty,
            detail_preservation=config.detail_preservation,
            lora_rank_penalty=config.lora_rank_penalty,
            rank_penalty_strength=config.rank_penalty_strength,
            low_rank_emphasis=config.low_rank_emphasis,
            optim_bits=config.optim_bits,
            percentile_clipping=config.percentile_clipping,
            block_wise=config.block_wise,
            min_8bit_size=config.min_8bit_size,
            stable_emb=config.stable_emb,
        )
        super().__init__(params, defaults)
        self.base_lrs: List[float] = [config.lr for group in self.param_groups]

    def _should_use_8bit(self, tensor: torch.Tensor) -> bool:
        """åˆ¤æ–·å¼µé‡æ˜¯å¦æ‡‰è©²ä½¿ç”¨ 8bit é‡åŒ–."""
        return (tensor.numel() >= self.config.min_8bit_size and
                tensor.dtype == torch.float32 and
                tensor.device.type == 'cuda')

    @staticmethod
    def _rms(tensor: torch.Tensor) -> torch.Tensor:
        """è¨ˆç®—å¼µé‡çš„å‡æ–¹æ ¹å€¼."""
        return tensor.norm(2) / (tensor.numel() ** 0.5 + 1e-10)

    @staticmethod
    def _ratio(new_p: torch.Tensor, p: torch.Tensor, pre: torch.Tensor) -> torch.Tensor:
        """è¨ˆç®—é¸æ“‡æ€§æŠ•å½±è¡°æ¸›çš„æ¯”ç‡."""
        curr_norm, prev_norm = torch.norm(new_p - pre), torch.norm(p - pre)
        ratio = (curr_norm - prev_norm) / (curr_norm + 1e-8)
        return torch.nn.functional.hardtanh(ratio, 0.0, 1.0)

    @staticmethod
    def _compute_edge_penalty(grad: torch.Tensor, threshold: float = 0.6) -> torch.Tensor:
        """
        è¨ˆç®—é‚Šç·£æ‡²ç½°é …ï¼Œç”¨æ–¼æŠ‘åˆ¶é‚Šç·£éæ“¬åˆ.
        ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯ç®—å­æª¢æ¸¬é‚Šç·£ï¼Œå°é«˜é »æˆåˆ†æ–½åŠ æ‡²ç½°.
        """
        if len(grad.shape) < 2:
            return torch.zeros_like(grad)

        laplacian = torch.zeros_like(grad)
        if len(grad.shape) == 2:
            if grad.shape[0] > 2 and grad.shape[1] > 2:
                laplacian[1:-1, :] += grad[2:, :] - 2 * grad[1:-1, :] + grad[:-2, :]
                laplacian[:, 1:-1] += grad[:, 2:] - 2 * grad[:, 1:-1] + grad[:, :-2]
        else:
            *batch_dims, h, w = grad.shape
            if h > 2 and w > 2:
                laplacian[..., 1:-1, :] += grad[..., 2:, :] - 2 * grad[..., 1:-1, :] + grad[..., :-2, :]
                laplacian[..., :, 1:-1] += grad[..., :, 2:] - 2 * grad[..., :, 1:-1] + grad[..., :, :-2]

        edge_strength = torch.abs(laplacian)
        edge_mask = (edge_strength > threshold).float()
        return edge_mask * edge_strength

    @staticmethod
    def _compute_frequency_penalty(grad: torch.Tensor) -> torch.Tensor:
        """
        è¨ˆç®—é »ç‡æ‡²ç½°é …ï¼ŒæŠ‘åˆ¶é«˜é »å™ªè².
        ä½¿ç”¨ FFT åˆ†æé »ç‡æˆåˆ†ï¼Œå°é«˜é »æˆåˆ†æ–½åŠ æ‡²ç½°.
        """
        if len(grad.shape) < 2:
            return torch.zeros_like(grad)

        if len(grad.shape) == 2:
            grad_fft = torch.fft.fft2(grad)
            freq_magnitude = torch.abs(grad_fft)

            h, w = grad.shape
            center_h, center_w = h // 2, w // 2
            y, x = torch.meshgrid(torch.arange(h, device=grad.device),
                                torch.arange(w, device=grad.device), indexing='ij')
            distance = torch.sqrt((y - center_h)**2 + (x - center_w)**2)

            high_freq_mask = (distance > min(h, w) * 0.3).float()
            penalty = freq_magnitude * high_freq_mask
            penalty_spatial = torch.real(torch.fft.ifft2(penalty))
            return penalty_spatial

        return torch.zeros_like(grad)

    @staticmethod
    def _lora_rank_regularization(param: torch.Tensor, rank_strength: float = 0.01) -> torch.Tensor:
        """
        LoRA ä½ç§©æ­£å‰‡åŒ–ï¼Œé¼“å‹µå­¸ç¿’ä½ç§©çµæ§‹.
        é€šé SVD åˆ†è§£å°é«˜ç§©æˆåˆ†æ–½åŠ æ‡²ç½°.
        """
        if len(param.shape) != 2:
            return torch.zeros_like(param)

        U, S, Vh = torch.linalg.svd(param, full_matrices=False)
        rank_penalty = torch.sum(S[S.argsort(descending=True)[10:]])
        penalty_grad = U @ torch.diag(S * rank_strength) @ Vh
        return penalty_grad

    def _init_state(self, p: torch.Tensor, group: Optional[Dict[str, Any]] = None) -> None:
        """åˆå§‹åŒ– 8bit å„ªåŒ–å™¨ç‹€æ…‹."""
        device = p.device
        shape = p.shape
        state = self.state[p]

        # åŸºæœ¬ç‹€æ…‹åˆå§‹åŒ–
        state.setdefault("lr_max", 1e-6)
        state.setdefault("step", 0)

        # å­¸ç¿’ç‡é®ç½©ï¼ˆå§‹çµ‚ä½¿ç”¨ 32bitï¼Œå› ç‚ºéœ€è¦é«˜ç²¾åº¦ï¼‰
        state.setdefault('lr_mask', torch.ones(shape, device=device, dtype=torch.float32) * self.config.lr)
        state.setdefault('avg_lr', float(self.config.lr))
        state.setdefault('last_polarity', torch.zeros(shape, dtype=torch.bool, device=device))

        # æ±ºå®šæ˜¯å¦ä½¿ç”¨ 8bit
        use_8bit = self._should_use_8bit(p) and not self.config.force_8bit

        # å‹•é‡ç‹€æ…‹åˆå§‹åŒ–
        if use_8bit:
            # ä½¿ç”¨ bitsandbytes çš„ 8bit ç‹€æ…‹
            state.setdefault("exp_avg", torch.zeros_like(p, dtype=torch.uint8))
            state.setdefault("exp_avg_state", {'absmax': torch.zeros_like(p, dtype=torch.float16)})
        else:
            # ä½¿ç”¨æ¨™æº– 32bit ç‹€æ…‹
            state.setdefault("exp_avg", torch.zeros_like(p))

        if use_8bit:
            state.setdefault("s", torch.zeros_like(p, dtype=torch.uint8))
            state.setdefault("s_state", {'absmax': torch.zeros_like(p, dtype=torch.float16)})
        else:
            state.setdefault("s", torch.zeros_like(p))

        # CAME ç‹€æ…‹
        if group and group.get('came', True):
            if use_8bit:
                state.setdefault("exp_avg_sq", torch.zeros_like(p, dtype=torch.uint8))
                state.setdefault("exp_avg_sq_state", {'absmax': torch.zeros_like(p, dtype=torch.float16)})
            else:
                state.setdefault("exp_avg_sq", torch.zeros_like(p))

        # AdaBelief æ®˜å·®
        if use_8bit:
            state.setdefault("exp_avg_res", torch.zeros_like(p, dtype=torch.uint8))
            state.setdefault("exp_avg_res_state", {'absmax': torch.zeros_like(p, dtype=torch.float16)})
        else:
            state.setdefault("exp_avg_res", torch.zeros_like(p))

        # é‚Šç·£æ§åˆ¶ç‹€æ…‹ï¼ˆä½¿ç”¨ 32bit ä¿æŒç²¾åº¦ï¼‰
        if group and group.get('edge_suppression', True):
            state.setdefault("edge_history", torch.zeros_like(p))
            state.setdefault("edge_momentum", torch.zeros_like(p))

        # ç©ºé–“æ„ŸçŸ¥ç‹€æ…‹ï¼ˆä½¿ç”¨ 32bitï¼‰
        if group and group.get('spatial_awareness', True):
            state.setdefault("spatial_variance", torch.ones_like(p))
            state.setdefault("detail_tracker", torch.zeros_like(p))

        # æ¨™è¨˜æ˜¯å¦ä½¿ç”¨ 8bit
        state.setdefault("use_8bit", use_8bit)

    def _get_8bit_state(self, state: Dict[str, Any], key: str) -> torch.Tensor:
        """ç²å– 8bit ç‹€æ…‹å¼µé‡ï¼Œè‡ªå‹•è™•ç†åé‡åŒ–."""
        if state.get("use_8bit", False) and f"{key}_state" in state:
            # ä½¿ç”¨ bitsandbytes åé‡åŒ–
            quantized = state[key]
            state_dict = state[f"{key}_state"]
            return F.dequantize_8bit(quantized, state_dict['absmax'])
        else:
            return state[key]

    def _set_8bit_state(self, state: Dict[str, Any], key: str, tensor: torch.Tensor) -> None:
        """è¨­å®š 8bit ç‹€æ…‹å¼µé‡ï¼Œè‡ªå‹•è™•ç†é‡åŒ–."""
        if state.get("use_8bit", False) and self._should_use_8bit(tensor):
            # ä½¿ç”¨ bitsandbytes é‡åŒ–
            quantized, state_dict = F.quantize_8bit(tensor)
            state[key] = quantized
            state[f"{key}_state"] = state_dict
        else:
            state[key] = tensor


class Automagic_CameAMP_Improved_8Bit(BitsAndBytesOptimized):
    """
    ä½¿ç”¨ bitsandbytes çš„æ”¹é€²ç‰ˆ Automagic_CameAMP 8bit å„ªåŒ–å™¨.

    é€™å€‹ç‰ˆæœ¬çµåˆäº†ï¼š
    1. bitsandbytes çš„é«˜æ•ˆ 8bit é‡åŒ–
    2. é‚Šç·£éæ“¬åˆæŠ‘åˆ¶
    3. èƒŒæ™¯æ­£å‰‡åŒ–
    4. LoRA ä½ç§©å„ªåŒ–
    5. é »ç‡æ„ŸçŸ¥å„ªåŒ–
    6. ç©ºé–“æ„ŸçŸ¥å­¸ç¿’ç‡èª¿æ•´
    """

    def __init__(self, params, **kwargs):
        config = Improved8BitOptimizerConfig(**kwargs)
        super().__init__(params, config)

        if self.config.verbose:
            print(f"ğŸš€ åˆå§‹åŒ– Automagic_CameAMP_Improved_8Bit å„ªåŒ–å™¨")
            print(f"ğŸ“Š 8bit é‡åŒ–: {'å•Ÿç”¨' if BITSANDBYTES_AVAILABLE else 'åœç”¨'}")
            print(f"ğŸ¯ é‚Šç·£æŠ‘åˆ¶: {'å•Ÿç”¨' if config.edge_suppression else 'åœç”¨'}")
            print(f"ğŸŒ… èƒŒæ™¯æ­£å‰‡åŒ–: {'å•Ÿç”¨' if config.background_regularization else 'åœç”¨'}")
            print(f"ğŸ§  LoRA å„ªåŒ–: {'å•Ÿç”¨' if config.lora_rank_penalty else 'åœç”¨'}")

    @torch.no_grad()
    def step(self, closure: Optional[callable] = None) -> Optional[float]:
        """åŸ·è¡Œå–®ä¸€å„ªåŒ–æ­¥é©Ÿ."""
        loss = closure() if closure is not None else None

        for group in self.param_groups:
            # è¨ˆç®—ç¾¤çµ„æ¢¯åº¦çµ±è¨ˆ
            grads_this_group = []
            for p in group["params"]:
                if p.grad is None or not p.requires_grad:
                    continue
                grads_this_group.append(p.grad.view(-1))

            if not grads_this_group:
                continue

            all_group_grads = torch.cat(grads_this_group)
            sum_abs_all_group_grads = torch.sum(torch.abs(all_group_grads))

            # æš–èº«éšæ®µçµ±è¨ˆ
            if any(self.state.get(p, {}).get("step", 0) < group.get("warmup_steps", 500) / 2
                   for p in group["params"] if p.grad is not None) and group["weight_decay"] > 0:
                abs_all_group_grads = torch.abs(all_group_grads)
                mean_norm = abs_all_group_grads.mean()
                std_norm = abs_all_group_grads.std(unbiased=False)

            for p in group["params"]:
                if p.grad is None or not p.requires_grad:
                    continue

                # è‡ªé©æ‡‰æ¢¯åº¦æ­£å‰‡åŒ– (AGR)
                abs_grad = torch.abs(p.grad)
                alpha = abs_grad / (sum_abs_all_group_grads + 1e-8)
                grad = p.grad * (1 - alpha)

                # åˆå§‹åŒ–ç‹€æ…‹
                state = self.state[p]
                if len(state) == 0:
                    self._init_state(p, group)
                if 'step' not in state:
                    state['step'] = 0
                state["step"] += 1

                # æ¸…ç†æš–èº«ç‹€æ…‹
                if state["step"] == group.get("warmup_steps", 0):
                    cleanup_keys = ['s', 'last_polarity']
                    for key in cleanup_keys:
                        if key in state:
                            del state[key]
                        if f"{key}_state" in state:
                            del state[f"{key}_state"]

                beta1, beta2, beta3 = group["betas"]
                eps1, eps2, eps3 = group["eps"]

                # ä½¿ç”¨ 8bit ç‹€æ…‹ï¼ˆè‡ªå‹•è™•ç†é‡åŒ–/åé‡åŒ–ï¼‰
                exp_avg = self._get_8bit_state(state, 'exp_avg')
                exp_avg_res = self._get_8bit_state(state, 'exp_avg_res')

                # é‚Šç·£æŠ‘åˆ¶è™•ç†
                edge_penalty = torch.zeros_like(grad)
                if group.get('edge_suppression', True):
                    edge_penalty = self._compute_edge_penalty(grad, group.get('edge_threshold', 0.6))

                    # æ›´æ–°é‚Šç·£æ­·å²
                    if 'edge_history' in state:
                        state['edge_history'].mul_(0.9).add_(edge_penalty, alpha=0.1)

                    # é‚Šç·£å‹•é‡
                    if 'edge_momentum' in state:
                        state['edge_momentum'].mul_(0.8).add_(edge_penalty, alpha=0.2)

                # é »ç‡æ‡²ç½°
                freq_penalty = torch.zeros_like(grad)
                if group.get('frequency_penalty', 0) > 0:
                    freq_penalty = self._compute_frequency_penalty(grad)

                # LoRA ä½ç§©æ­£å‰‡åŒ–
                lora_penalty = torch.zeros_like(p.data)
                if group.get('lora_rank_penalty', True) and len(p.shape) == 2:
                    lora_penalty = self._lora_rank_regularization(
                        p.data, group.get('rank_penalty_strength', 0.01)
                    )

                # çµåˆæ‰€æœ‰æ‡²ç½°é …
                total_penalty = (group.get('edge_penalty', 0.1) * edge_penalty +
                               group.get('frequency_penalty', 0.05) * freq_penalty +
                               group.get('rank_penalty_strength', 0.01) * lora_penalty)

                # æ‡‰ç”¨æ‡²ç½°åˆ°æ¢¯åº¦
                grad = grad + total_penalty

                # æ›´æ–°ä¸€éšå‹•é‡
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)

                # CAME æˆ– AdaBelief äºŒéšå‹•é‡
                if group.get('came', True):
                    exp_avg_sq = self._get_8bit_state(state, 'exp_avg_sq')
                    exp_avg_sq.mul_(beta2).add_(grad.pow(2) + eps1, alpha=1 - beta2)
                    scaled_grad = grad.clone().mul_(exp_avg_sq.rsqrt())
                    self._set_8bit_state(state, 'exp_avg_sq', exp_avg_sq)
                else:
                    # AdaBelief è®Šé«”
                    exp_avg_sq = self._get_8bit_state(state, 'exp_avg_sq')
                    diff = grad - exp_avg
                    exp_avg_res.mul_(beta3).add_(diff.pow(2) + eps1, alpha=1 - beta3)
                    exp_avg_sq.mul_(beta2).add_(grad.pow(2) + eps1, alpha=1 - beta2)
                    scaled_grad = exp_avg / (exp_avg_res.sqrt() + eps2)
                    self._set_8bit_state(state, 'exp_avg_sq', exp_avg_sq)

                # æ—©æœŸéšæ®µä½¿ç”¨ Torque-Aware Momentum
                if state["step"] < group.get("warmup_steps", 500):
                    s = self._get_8bit_state(state, 's') if 's' in state else torch.zeros_like(p)
                    corr = normalize(exp_avg, p=2.0, dim=0).mul_(normalize(scaled_grad, p=2.0, dim=0))
                    decay_rate = group["adaptation_rate"]
                    s.mul_(decay_rate).add_(corr, alpha=1.0 - decay_rate)

                    polarity = torch.sign(s)
                    if 'last_polarity' in state:
                        cosine_sim = torch.mean(polarity * state['last_polarity'].float())
                        s = s * torch.clamp(cosine_sim, 0.1, 1.0)

                    state['last_polarity'] = polarity.bool()
                    update_p = s

                    self._set_8bit_state(state, 's', s)
                else:
                    # å¾ŒæœŸéšæ®µä½¿ç”¨ä¸€è‡´æ€§å‹•é‡
                    consistency_factor = 1.0 + 0.1 * torch.tanh(exp_avg * scaled_grad)
                    update_p = exp_avg * consistency_factor

                # æ›´æ–° 8bit ç‹€æ…‹
                self._set_8bit_state(state, 'exp_avg', exp_avg)
                self._set_8bit_state(state, 'exp_avg_res', exp_avg_res)

                # å­¸ç¿’ç‡é®ç½©æ›´æ–°
                current_lr = self._update_learning_rate_mask(state, group, grad)

                # ç©ºé–“æ„ŸçŸ¥å­¸ç¿’ç‡èª¿æ•´
                if group.get('spatial_awareness', True) and 'spatial_variance' in state:
                    spatial_var = torch.var(update_p)
                    state['spatial_variance'].mul_(0.99).add_(spatial_var, alpha=0.01)
                    spatial_factor = torch.clamp(state['spatial_variance'] / (spatial_var + 1e-8), 0.5, 2.0)
                    current_lr = current_lr * spatial_factor

                # ALLoRA æ”¯æ´
                if len(p.shape) == 2 and hasattr(p, '_is_lora_layer'):
                    # ç°¡åŒ–çš„ ALLoRA é‚è¼¯
                    row_variance = torch.var(update_p, dim=1, keepdim=True)
                    row_scaling = torch.clamp(row_variance / (torch.mean(row_variance) + 1e-8), 0.1, 10.0)
                    current_lr = current_lr * row_scaling

                # æ‡‰ç”¨æ¬Šé‡è¡°æ¸›å’Œå­¸ç¿’ç‡
                if state["step"] < group.get("warmup_steps", 500) / 2 and group["weight_decay"] > 0:
                    # è‡ªé©æ‡‰æ¬Šé‡è¡°æ¸›
                    param_abs_grad = abs_grad.mean()
                    norm_grad = (param_abs_grad - mean_norm) / (std_norm + 1e-8)
                    ada_alpha = 4
                    theta = 2 / (1 + torch.exp(-ada_alpha * norm_grad))
                    p.data.mul_(1 - current_lr * group["weight_decay"] * theta)

                # æœ€çµ‚åƒæ•¸æ›´æ–°
                update_p = update_p * current_lr
                p.add_(-update_p)

        if self.config.verbose:
            # é¡¯ç¤ºè¨˜æ†¶é«”çµ±è¨ˆ
            self._print_memory_stats()

        return loss

    def _update_learning_rate_mask(self, state: Dict[str, Any], group: Dict[str, Any], grad: torch.Tensor) -> torch.Tensor:
        """æ›´æ–°å­¸ç¿’ç‡é®ç½©."""
        if state["step"] < group.get("warmup_steps", 500):
            return self._update_warmup_lr_mask(state, group, grad)
        else:
            return self._update_post_warmup_lr_mask(state, group)

    def _update_warmup_lr_mask(self, state: Dict[str, Any], group: Dict[str, Any], grad: torch.Tensor) -> torch.Tensor:
        """æš–èº«éšæ®µå­¸ç¿’ç‡é®ç½©æ›´æ–°."""
        warmup_steps = group.get("warmup_steps", 500)
        current_step = state["step"]
        progress = current_step / warmup_steps

        # ç·šæ€§æš–èº«
        base_lr = group["lr"] * progress

        # æ¢¯åº¦æ„ŸçŸ¥èª¿æ•´
        grad_norm = torch.norm(grad, dim=-1, keepdim=True) if len(grad.shape) > 1 else torch.abs(grad)
        normalized_grad_norm = grad_norm / (torch.mean(grad_norm) + 1e-8)

        # è‡ªé©æ‡‰å› å­
        adaptive_factor = torch.clamp(1.0 / (normalized_grad_norm + 1e-8), 0.1, 10.0)

        new_lr = base_lr * adaptive_factor
        state['lr_mask'] = new_lr

        return new_lr

    def _update_post_warmup_lr_mask(self, state: Dict[str, Any], group: Dict[str, Any]) -> torch.Tensor:
        """æš–èº«å¾Œå­¸ç¿’ç‡é®ç½©æ›´æ–°."""
        current_lr = state['lr_mask']

        # ç°¡åŒ–çš„è‡ªå‹•èª¿æ•´é‚è¼¯
        lr_adjustment = torch.clamp(torch.randn_like(current_lr) * 0.01, -0.1, 0.1)
        new_lr = torch.clamp(current_lr * (1 + lr_adjustment), group["lr"] * 0.1, group["lr"] * 10)

        state['lr_mask'] = new_lr
        return new_lr

    def _print_memory_stats(self):
        """åˆ—å°è¨˜æ†¶é«”çµ±è¨ˆä¿¡æ¯."""
        if not self.config.verbose:
            return

        total_8bit_params = 0
        total_32bit_params = 0

        for group in self.param_groups:
            for p in group["params"]:
                state = self.state.get(p, {})
                if state.get("use_8bit", False):
                    total_8bit_params += p.numel()
                else:
                    total_32bit_params += p.numel()

        total_params = total_8bit_params + total_32bit_params
        if total_params > 0:
            compression_ratio = total_8bit_params / total_params
            print(f"ğŸ“Š è¨˜æ†¶é«”çµ±è¨ˆ: 8bitåƒæ•¸={total_8bit_params}, 32bitåƒæ•¸={total_32bit_params}, "
                  f"å£“ç¸®ç‡={compression_ratio:.2%}")

    def state_dict(self) -> Dict[str, Any]:
        """ç²å–å„ªåŒ–å™¨ç‹€æ…‹å­—å…¸."""
        state = super().state_dict()
        state['magic_improved_8bit_version'] = 1
        state['bitsandbytes_version'] = getattr(bnb, '__version__', 'unknown') if BITSANDBYTES_AVAILABLE else 'not_available'
        return state

    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
        """è¼‰å…¥å„ªåŒ–å™¨ç‹€æ…‹å­—å…¸."""
        if 'magic_improved_8bit_version' not in state_dict:
            print('[è­¦å‘Š] æ‚¨è¼‰å…¥äº†èˆŠç‰ˆæœ¬çš„ç‹€æ…‹å­—å…¸ï¼ŒæŸäº› 8bit åŠŸèƒ½å¯èƒ½ç„¡æ³•æ­£å¸¸å·¥ä½œï¼')

        if state_dict.get('bitsandbytes_version', 'not_available') == 'not_available' and BITSANDBYTES_AVAILABLE:
            print('[è­¦å‘Š] ç‹€æ…‹å­—å…¸ä¾†è‡ªé 8bit ç‰ˆæœ¬ï¼Œå°‡å˜—è©¦å…¼å®¹æ€§è¼‰å…¥ã€‚')

        super().load_state_dict(state_dict)

    def get_memory_efficiency_report(self) -> Dict[str, Any]:
        """ç²å–è©³ç´°çš„è¨˜æ†¶é«”æ•ˆç‡å ±å‘Š."""
        report = {
            'bitsandbytes_available': BITSANDBYTES_AVAILABLE,
            'total_parameters': 0,
            '8bit_parameters': 0,
            '32bit_parameters': 0,
            'memory_saved_mb': 0,
            'compression_ratio': 0,
            'states_breakdown': {}
        }

        for group_idx, group in enumerate(self.param_groups):
            for param_idx, p in enumerate(group["params"]):
                state = self.state.get(p, {})
                param_count = p.numel()
                report['total_parameters'] += param_count

                if state.get("use_8bit", False):
                    report['8bit_parameters'] += param_count
                    # ä¼°ç®—è¨˜æ†¶é«”ç¯€çœï¼ˆæ¯å€‹åƒæ•¸ç¯€çœ 3 bytesï¼š4-1=3ï¼‰
                    report['memory_saved_mb'] += param_count * 3 / (1024 * 1024)
                else:
                    report['32bit_parameters'] += param_count

        if report['total_parameters'] > 0:
            report['compression_ratio'] = report['8bit_parameters'] / report['total_parameters']

        return report


# ä¾¿åˆ©å‡½æ•¸
def create_improved_8bit_optimizer(model_parameters, **kwargs) -> Automagic_CameAMP_Improved_8Bit:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šå‰µå»ºæ”¹é€²ç‰ˆ 8bit å„ªåŒ–å™¨.

    Args:
        model_parameters: æ¨¡å‹åƒæ•¸
        **kwargs: é…ç½®åƒæ•¸

    Returns:
        Automagic_CameAMP_Improved_8Bit å¯¦ä¾‹
    """
    return Automagic_CameAMP_Improved_8Bit(model_parameters, **kwargs)


# ç¯„ä¾‹é…ç½®
class OptimizationProfiles:
    """é å®šç¾©çš„å„ªåŒ–é…ç½®æª”æ¡ˆ."""

    @staticmethod
    def memory_optimized() -> Improved8BitOptimizerConfig:
        """è¨˜æ†¶é«”å„ªåŒ–é…ç½® - æœ€å¤§è¨˜æ†¶é«”ç¯€çœ."""
        return Improved8BitOptimizerConfig(
            lr=1e-4,
            force_8bit=True,
            min_8bit_size=1024,
            edge_suppression=False,
            spatial_awareness=False,
            verbose=False
        )

    @staticmethod
    def quality_optimized() -> Improved8BitOptimizerConfig:
        """å“è³ªå„ªåŒ–é…ç½® - æœ€ä½³è¨“ç·´æ•ˆæœ."""
        return Improved8BitOptimizerConfig(
            lr=1e-4,
            edge_suppression=True,
            edge_penalty=0.15,
            background_regularization=True,
            spatial_awareness=True,
            lora_rank_penalty=True,
            verbose=True
        )

    @staticmethod
    def balanced() -> Improved8BitOptimizerConfig:
        """å¹³è¡¡é…ç½® - è¨˜æ†¶é«”èˆ‡å“è³ªå…¼é¡§."""
        return Improved8BitOptimizerConfig(
            lr=1e-4,
            min_8bit_size=4096,
            edge_suppression=True,
            edge_penalty=0.1,
            background_regularization=True,
            spatial_awareness=True,
            lora_rank_penalty=True,
            verbose=True
        )